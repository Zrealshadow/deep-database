# From Schema to Signal: Retrieval-Augmented Modeling for Relational Data Analytics

![Python](https://img.shields.io/badge/Python-3.9.10-blue.svg)
[![Linting - flake8](https://img.shields.io/badge/code%20style-flake8-blue)](https://flake8.pycqa.org/)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Conference](https://img.shields.io/badge/Conference-SIGMOD%202026-blue)
![Status](https://img.shields.io/badge/Status-Submitted-orange)

>***Retrieval-Augmented Modeling*** (RAM), a new relational data modeling paradigm that learns from both explicit structure and implicit semantics by integrating information retrieval (IR) with graph-based modeling.



## Overview

![overview](figs/readme/overview.jpg)




## Setup

This project uses **Conda** for environment and dependency management. See the root-level [`environment.yml`](../environment.yml) for complete setup.

### Quick Setup

1. **Install Conda**

   Ensure you have [Miniconda](https://docs.conda.io/en/latest/miniconda.html) or [Anaconda](https://www.anaconda.com/) installed.

2. **Create the Conda environment**

   ```bash
   cd <project-root>
   conda env create -f environment.yml
   conda activate deepdb
   ```

### Main Dependencies

- **relbench** - RelBench dataset framework
- **torch** - PyTorch deep learning framework
- **torch_geometric (PyG)** - Graph neural networks
- **torch_frame** - Tabular data handling
- **bm25s** - BM25 retrieval with Numba backend
- **pandas, numpy** - Data processing


## Structure

This directory contains the RAM (Retrieval-Augmented Modeling) implementation and baseline methods:

### Core RAM Scripts

- `ram_run.py` - Main script to run RAM, including training and inference
- `pretrain_ram.py` - Pretraining process for RAM using retrieval-augmented objectives

### Baseline Implementations

- `graph_baseline.py` - Graph-based baseline methods including `R-GCN`, `R-GAT`, `HGT`
- `node2vec_baseline.py` - Node2Vec baseline methods including `Node2Vec`, `GraphSAGE`, `GAT`
- `dnn_baseline_table_data.py` - Execute tabular learning baseline methods including `MLP`, `ResNet`, `FT-Transformer`
- `ml_baseline.py` - Machine learning baseline methods including `CatBoost`, `LightGBM`
- `pretrain_baseline.py` - Graph pretraining baseline methods including `DGI`, `GraphCL`, `BGRL`

### Preprocessing

- `preprocess/` - Database-specific preprocessing scripts for building retrieval indices
  - `avito.py` - Preprocessing and retrieval index building for Avito dataset
  - Additional preprocessing scripts for other RelBench databases

Each script has its own arguments. Run `python <script_name>.py --help` to see argument details.

## Data Directories

During preprocessing and training, RAM generates several data directories:

- **`/edges`** - Cross-table edge files generated by retrieval
  - Stores semantic connections between entity tables as `.npz` files
  - Format: `{src_table}-{dst_table}` edge arrays
  - See [edges/README.md](../edges/README.md) for details

- **`/samples`** - Positive sample pools for contrastive learning
  - Contains similar entity pairs discovered via retrieval
  - Used for self-supervised pretraining objectives
  - See [samples/README.md](../samples/README.md) for details

- **`/tmp`** - Retrieval indices (BM25)
  - Stores BM25 retriever models per entity table
  - Generated during preprocessing, loaded during training
  - See [tmp/README.md](../tmp/README.md) for details

- **`/tmp_docs`** - Tokenized documents
  - Text representations of relational data
  - Intermediate format for retrieval index building
  - See [tmp_docs/README.md](../tmp_docs/README.md) for details

## Usage

### Preprocessing Pipeline

1. **Tokenize Database**
   - Convert relational data to text documents
   - Infer column types and apply appropriate tokenization

2. **Build Retrieval Indices**
   - Create BM25 indices for each entity table
   - Generate documents via random walks on the homogeneous graph

3. **Discover Cross-table Edges**
   - Use retrieval to find semantically related entities across tables
   - Filter edges by score threshold (mean + 2*std)

4. **Generate Positive Samples**
   - Find similar entities within each table via retrieval
   - Create positive pairs for contrastive learning

Example preprocessing for a dataset:

```python
# See preprocess/avito.py for complete example
from relbench.datasets import get_dataset
from utils.builder import make_homograph_from_db
from utils.tokenize import tokenize_database

# Load dataset and build graph
dataset = get_dataset(name="rel-avito", download=True)
db = dataset.get_db()
homoGraph = make_homograph_from_db(db, verbose=True)

# Tokenize and build retrieval indices
tk_db = tokenize_database(db, col_type_dict, './tmp_docs/rel-avito', True)
# ... (see preprocess scripts for full pipeline)
```

### Training

```bash
# Run RAM training
python ram_run.py --dataset <dataset_name> --task <task_name> [options]

# Run baseline methods
python graph_baseline.py --help
python node2vec_baseline.py --help
python pretrain_baseline.py --help
```

## Experiments Results
![overview](figs/readme/results.png)



## License
This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.