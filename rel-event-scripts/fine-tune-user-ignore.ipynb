{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from tqdm import tqdm\n",
    "from utils.data import preprocess_event_database\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from relbench.datasets import get_dataset\n",
    "\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-event/db...\n",
      "Done in 3.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion/utils/data.py:231: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_df[\"event_id\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  event_df[\"event_id\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:233: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_interest_df[\"event\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:234: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_attendees_flattened_df[\"event\"].replace(event_id2index, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset('rel-event')\n",
    "db = dataset.get_db()\n",
    "preprocess_event_database(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"./data/rel-event-tensor-frame/\"\n",
    "# [NOTE]: the dataset has been materialized\n",
    "\n",
    "# get infer_type in cache\n",
    "type_path = os.path.join(cache_path,\"col_type_dict.pkl\")\n",
    "col_type_dict = pickle.load(open(type_path, \"rb\"))\n",
    "len(col_type_dict)\n",
    "\n",
    "# add \"compress_text\" in each table in case \n",
    "for table_name, table in db.table_dict.items():\n",
    "    table.df[\"text_compress\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from utils.resource import get_text_embedder_cfg\n",
    "text_embedder_cfg = get_text_embedder_cfg(\n",
    "    model_name = \"sentence-transformers/average_word_embeddings_glove.6B.300d\", \n",
    "    device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Materialize event_attendees Tensor Frame\n",
      "-----> Build edge between users and users\n",
      "-----> Materialize events Tensor Frame\n",
      "-----> Materialize event_interest Tensor Frame\n",
      "-----> Materialize users Tensor Frame\n"
     ]
    }
   ],
   "source": [
    "from utils.builder import build_pyg_hetero_graph\n",
    "data, col_stats_dict = build_pyg_hetero_graph(\n",
    "    db,\n",
    "    col_type_dict,\n",
    "    text_embedder_cfg,\n",
    "    cache_path,\n",
    "    True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add new edges:\n",
    "from utils.util import load_np_dict\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "edge_dict = load_np_dict(\"./edges/rel-event-edges.npz\")\n",
    "\n",
    "for edge_name, edge_np in edge_dict.items():\n",
    "    src_table, dst_table = edge_name.split('-')[0], edge_name.split('-')[1]\n",
    "    edge_index = torch.from_numpy(edge_np.astype(int)).t()\n",
    "    # [2, edge_num]\n",
    "    edge_type = (src_table, f\"appendix\", dst_table)\n",
    "    data[edge_type].edge_index = sort_edge_index(edge_index)\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relbench tasks\n",
    "from relbench.tasks import get_task\n",
    "from relbench.modeling.graph import get_node_train_table_input\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from relbench.base import BaseTask\n",
    "from model.base import CompositeModel, FeatureEncodingPart, NodeRepresentationPart\n",
    "from relbench.modeling.nn import HeteroTemporalEncoder\n",
    "# start to fine-train on the task a\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_a = get_task(\"rel-event\", \"user-ignore\", download = True)\n",
    "entity_table = task_a.entity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loader_dict(task: BaseTask, data:HeteroData) -> dict:\n",
    "    loader_dict = {}\n",
    "    for split, table in [\n",
    "        (\"train\", task.get_table(\"train\")),\n",
    "        (\"val\",task.get_table(\"val\")),\n",
    "        (\"test\", task.get_table(\"test\")),\n",
    "    ]:\n",
    "        table_input = get_node_train_table_input(\n",
    "            table=table,\n",
    "            task=task,\n",
    "        )\n",
    "        loader_dict[split] = NeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=[\n",
    "                128, 64\n",
    "            ],  # we sample subgraphs of depth 2, 128 neighbors per node.\n",
    "            time_attr=\"time\",\n",
    "            input_nodes=table_input.nodes,\n",
    "            input_time=table_input.time,\n",
    "            transform=table_input.transform,\n",
    "            batch_size=256,\n",
    "            temporal_strategy=\"uniform\",\n",
    "            shuffle=split == \"train\",\n",
    "            num_workers=0,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "    return loader_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, model: torch.nn.Module, task: BaseTask)-> np.ndarray:\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(\n",
    "            batch,\n",
    "            task.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "    return torch.cat(pred_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct bottom model\n",
    "channels = 128\n",
    "\n",
    "temporal_encoder = HeteroTemporalEncoder(\n",
    "    node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "    channels=channels,\n",
    ")\n",
    "\n",
    "feat_encoder = FeatureEncodingPart(\n",
    "    data=data,\n",
    "    node_to_col_stats=col_stats_dict,\n",
    "    channels=channels,\n",
    ")\n",
    "\n",
    "node_encoder = NodeRepresentationPart(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    num_layers=1,\n",
    "    normalization=\"layer_norm\",\n",
    "    dropout_prob=0.2\n",
    ")\n",
    "\n",
    "net = CompositeModel(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    out_channels=1,\n",
    "    dropout=0.2,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    "    num_layer=2,\n",
    "    feature_encoder=feat_encoder,\n",
    "    node_encoder=node_encoder,\n",
    "    temporal_encoder=temporal_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the pre-trained model\n",
    "pre_trained_model_param_path = './static/rel-event-pre-trained-channel128-ep100-best-state.pth'\n",
    "pre_trained_state_dict = torch.load(pre_trained_model_param_path)\n",
    "net.load_state_dict(pre_trained_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "task_loader_dict = generate_loader_dict(task_a,data)\n",
    "lr = 0.002\n",
    "epoches = 50\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "tune_metric = \"auroc\"\n",
    "higher_is_better = True\n",
    "early_stop = 3\n",
    "max_round_epoch = 20\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr = lr)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 01>******************************\n",
      ", Train loss: 0.35093651711940765, Val metrics: {'auroc': 0.8794194672170863}\n",
      "Test metrics: {'auroc': 0.8242920199422058}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 02>******************************\n",
      ", Train loss: 0.28410197198390963, Val metrics: {'auroc': 0.8975515776111015}\n",
      "Test metrics: {'auroc': 0.8461512813184718}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 03>******************************\n",
      ", Train loss: 0.27950604185461997, Val metrics: {'auroc': 0.9050792681745061}\n",
      "Test metrics: {'auroc': 0.860586834333619}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 04>******************************\n",
      ", Train loss: 0.25375309586524963, Val metrics: {'auroc': 0.9152536726941489}\n",
      "Test metrics: {'auroc': 0.8539430313422883}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 05>******************************\n",
      ", Train loss: 0.2611527815461159, Val metrics: {'auroc': 0.896202783107545}\n",
      "Test metrics: {'auroc': 0.8359994919183259}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 06>******************************\n",
      ", Train loss: 0.24755770340561867, Val metrics: {'auroc': 0.916963437796771}\n",
      "Test metrics: {'auroc': 0.8546518052776984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 07>******************************\n",
      ", Train loss: 0.25396684631705285, Val metrics: {'auroc': 0.9251482644339788}\n",
      "Test metrics: {'auroc': 0.8512413070401067}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 08>******************************\n",
      ", Train loss: 0.2513189874589443, Val metrics: {'auroc': 0.9201476708917184}\n",
      "Test metrics: {'auroc': 0.849637039154044}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 09>******************************\n",
      ", Train loss: 0.24894133061170579, Val metrics: {'auroc': 0.9292830736283118}\n",
      "Test metrics: {'auroc': 0.8489473182814137}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 10>******************************\n",
      ", Train loss: 0.25251981541514396, Val metrics: {'auroc': 0.9192213210070354}\n",
      "Test metrics: {'auroc': 0.8368257597408784}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 11>******************************\n",
      ", Train loss: 0.22222886011004447, Val metrics: {'auroc': 0.9231129595712928}\n",
      "Test metrics: {'auroc': 0.8228414467625671}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 12>******************************\n",
      ", Train loss: 0.23191254884004592, Val metrics: {'auroc': 0.919944776343586}\n",
      "Test metrics: {'auroc': 0.8253659775808961}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 13>******************************\n",
      ", Train loss: 0.21814620718359948, Val metrics: {'auroc': 0.9245292846483322}\n",
      "Test metrics: {'auroc': 0.8324886475500938}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_metric = -math.inf if higher_is_better else math.inf\n",
    "net.to(device)\n",
    "best_epoch = 0\n",
    "patience = 0\n",
    "for epoch in range(1, epoches + 1):\n",
    "    net.train()\n",
    "    cnt = 0\n",
    "    loss_accum = count_accum = 0\n",
    "    for batch in tqdm(task_loader_dict[\"train\"], leave=False):\n",
    "        cnt += 1\n",
    "        if cnt > max_round_epoch:\n",
    "            break\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(\n",
    "            batch,\n",
    "            entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        loss = loss_fn(pred, batch[entity_table].y.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_accum += loss.detach().item() * pred.size(0)\n",
    "        count_accum += pred.size(0)\n",
    "    \n",
    "    train_loss = loss_accum / count_accum\n",
    "    val_logits = test(task_loader_dict[\"val\"], net, task_a)\n",
    "    val_logits = torch.sigmoid(val_logits).numpy()\n",
    "    \n",
    "    val_pred = (val_logits > 0.5).astype(int)\n",
    "    val_pred_hat = task_a.get_table(\"val\").df[task_a.target_col].to_numpy()\n",
    "    val_metrics = {\n",
    "            \"auroc\": roc_auc_score(val_pred_hat, val_logits),\n",
    "        # \"accuracy\": accuracy_score(val_pred_hat, val_pred),\n",
    "        # \"precision\": precision_score(val_pred_hat, val_pred),\n",
    "        # \"recall\": recall_score(val_pred_hat, val_pred),\n",
    "        # \"f1\": f1_score(val_pred_hat, val_pred),\n",
    "    }\n",
    "    \n",
    "    test_logits = test(task_loader_dict[\"test\"], net, task_a)\n",
    "    test_logits =  torch.sigmoid(test_logits).numpy()\n",
    "    print(\"*\"*30 + f\"<Epoch: {epoch:02d}>\" + \"*\"*30)\n",
    "    print(f\", Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "    \n",
    "    test_pred = (test_logits > 0.5).astype(int)\n",
    "    test_pred_hat = task_a.get_table(\"test\", mask_input_cols = False).df[task_a.target_col].to_numpy()\n",
    "    test_metrics = {\n",
    "        \"auroc\": roc_auc_score(test_pred_hat, test_logits),\n",
    "        # \"accuracy\": accuracy_score(test_pred_hat, test_pred),\n",
    "        # \"precision\": precision_score(test_pred_hat, test_pred),\n",
    "        # \"recall\": recall_score(test_pred_hat, test_pred),\n",
    "        # \"f1score\": f1_score(test_pred_hat, test_pred),\n",
    "    }\n",
    "\n",
    "    print(f\"Test metrics: {test_metrics}\")\n",
    "\n",
    "    \n",
    "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
    "        not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
    "    ):\n",
    "        patience = 0\n",
    "        best_epoch = epoch\n",
    "        best_val_metric = val_metrics[tune_metric]\n",
    "        state_dict = copy.deepcopy(net.state_dict())\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience > early_stop:\n",
    "        break\n",
    "\n",
    "# print the best epoch\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': 0.8490159093074212,\n",
       " 'accuracy': 0.8908584451759939,\n",
       " 'precision': 0.5263157894736842,\n",
       " 'recall': 0.4222222222222222,\n",
       " 'f1score': 0.468557336621455}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(state_dict)\n",
    "test_logits = test(task_loader_dict[\"test\"], net, task_a)\n",
    "test_logits =  torch.sigmoid(test_logits).numpy()\n",
    "\n",
    "test_pred = (test_logits > 0.5).astype(int)\n",
    "test_pred_hat = task_a.get_table(\"test\", mask_input_cols = False).df[task_a.target_col].to_numpy()\n",
    "test_metrics = {\n",
    "    \"auroc\": roc_auc_score(test_pred_hat, test_logits),\n",
    "    \"accuracy\": accuracy_score(test_pred_hat, test_pred),\n",
    "    \"precision\": precision_score(test_pred_hat, test_pred),\n",
    "    \"recall\": recall_score(test_pred_hat, test_pred),\n",
    "    \"f1score\": f1_score(test_pred_hat, test_pred),\n",
    "}\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
