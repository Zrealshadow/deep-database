{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from relbench.datasets import get_dataset\n",
    "\n",
    "from utils.data import preprocess_event_database\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-event/db...\n",
      "Done in 3.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion/utils/data.py:231: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_df[\"event_id\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  event_df[\"event_id\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:233: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_interest_df[\"event\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:234: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_attendees_flattened_df[\"event\"].replace(event_id2index, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset('rel-event')\n",
    "db = dataset.get_db()\n",
    "preprocess_event_database(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['events', 'users']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.document import generate_document_given_table\n",
    "from utils.builder import identify_entity_table\n",
    "from utils.builder import generate_hop_matrix\n",
    "entity_tables = identify_entity_table(db)\n",
    "entity_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidates_pairs = []\n",
    "edge_candidates_pairs.extend([(\"events\", \"users\"), (\"users\", \"events\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table event_attendees -> table events has 49822 edges\n",
      "table event_attendees -> table users has 49822 edges\n",
      "table user_friends -> table users has 213703 edges\n",
      "table user_friends -> table users has 213703 edges\n",
      "table events -> table users has 86 edges\n",
      "table event_interest -> table events has 14135 edges\n",
      "table event_interest -> table users has 14135 edges\n"
     ]
    }
   ],
   "source": [
    "# homoGraph\n",
    "from utils.builder import HomoGraph, make_homograph_from_db\n",
    "homoGraph = make_homograph_from_db(db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rule 0]: event_attendees Inferred id from numerical as categorical\n",
      "[rule 0]: event_attendees Inferred user_id from numerical as categorical\n",
      "[rule 0]: user_friends Inferred id from numerical as categorical\n",
      "[rule 0]: events Inferred event_id from numerical as categorical\n",
      "[rule 0]: events Inferred user_id from numerical as categorical\n",
      "[rule 1]: events Inferred c_1 from numerical as categorical\n",
      "[rule 1]: events Inferred c_2 from numerical as categorical\n",
      "[rule 1]: events Inferred c_3 from numerical as categorical\n",
      "[rule 1]: events Inferred c_4 from numerical as categorical\n",
      "[rule 1]: events Inferred c_5 from numerical as categorical\n",
      "[rule 1]: events Inferred c_6 from numerical as categorical\n",
      "[rule 1]: events Inferred c_7 from numerical as categorical\n",
      "[rule 1]: events Inferred c_8 from numerical as categorical\n",
      "[rule 1]: events Inferred c_9 from numerical as categorical\n",
      "[rule 1]: events Inferred c_10 from numerical as categorical\n",
      "[rule 1]: events Inferred c_11 from numerical as categorical\n",
      "[rule 1]: events Inferred c_12 from numerical as categorical\n",
      "[rule 1]: events Inferred c_13 from numerical as categorical\n",
      "[rule 1]: events Inferred c_14 from numerical as categorical\n",
      "[rule 1]: events Inferred c_15 from numerical as categorical\n",
      "[rule 1]: events Inferred c_16 from numerical as categorical\n",
      "[rule 1]: events Inferred c_17 from numerical as categorical\n",
      "[rule 1]: events Inferred c_18 from numerical as categorical\n",
      "[rule 1]: events Inferred c_19 from numerical as categorical\n",
      "[rule 1]: events Inferred c_20 from numerical as categorical\n",
      "[rule 1]: events Inferred c_21 from numerical as categorical\n",
      "[rule 1]: events Inferred c_22 from numerical as categorical\n",
      "[rule 1]: events Inferred c_23 from numerical as categorical\n",
      "[rule 1]: events Inferred c_24 from numerical as categorical\n",
      "[rule 1]: events Inferred c_25 from numerical as categorical\n",
      "[rule 1]: events Inferred c_26 from numerical as categorical\n",
      "[rule 1]: events Inferred c_27 from numerical as categorical\n",
      "[rule 1]: events Inferred c_28 from numerical as categorical\n",
      "[rule 1]: events Inferred c_29 from numerical as categorical\n",
      "[rule 1]: events Inferred c_30 from numerical as categorical\n",
      "[rule 1]: events Inferred c_31 from numerical as categorical\n",
      "[rule 1]: events Inferred c_32 from numerical as categorical\n",
      "[rule 1]: events Inferred c_33 from numerical as categorical\n",
      "[rule 1]: events Inferred c_34 from numerical as categorical\n",
      "[rule 1]: events Inferred c_35 from numerical as categorical\n",
      "[rule 1]: events Inferred c_36 from numerical as categorical\n",
      "[rule 1]: events Inferred c_37 from numerical as categorical\n",
      "[rule 1]: events Inferred c_38 from numerical as categorical\n",
      "[rule 1]: events Inferred c_39 from numerical as categorical\n",
      "[rule 1]: events Inferred c_40 from numerical as categorical\n",
      "[rule 1]: events Inferred c_41 from numerical as categorical\n",
      "[rule 1]: events Inferred c_42 from numerical as categorical\n",
      "[rule 1]: events Inferred c_43 from numerical as categorical\n",
      "[rule 1]: events Inferred c_44 from numerical as categorical\n",
      "[rule 1]: events Inferred c_45 from numerical as categorical\n",
      "[rule 1]: events Inferred c_46 from numerical as categorical\n",
      "[rule 1]: events Inferred c_47 from numerical as categorical\n",
      "[rule 1]: events Inferred c_48 from numerical as categorical\n",
      "[rule 1]: events Inferred c_49 from numerical as categorical\n",
      "[rule 1]: events Inferred c_50 from numerical as categorical\n",
      "[rule 1]: events Inferred c_51 from numerical as categorical\n",
      "[rule 1]: events Inferred c_52 from numerical as categorical\n",
      "[rule 1]: events Inferred c_53 from numerical as categorical\n",
      "[rule 1]: events Inferred c_54 from numerical as categorical\n",
      "[rule 1]: events Inferred c_55 from numerical as categorical\n",
      "[rule 1]: events Inferred c_56 from numerical as categorical\n",
      "[rule 1]: events Inferred c_57 from numerical as categorical\n",
      "[rule 1]: events Inferred c_58 from numerical as categorical\n",
      "[rule 1]: events Inferred c_59 from numerical as categorical\n",
      "[rule 1]: events Inferred c_60 from numerical as categorical\n",
      "[rule 1]: events Inferred c_61 from numerical as categorical\n",
      "[rule 1]: events Inferred c_62 from numerical as categorical\n",
      "[rule 1]: events Inferred c_63 from numerical as categorical\n",
      "[rule 1]: events Inferred c_64 from numerical as categorical\n",
      "[rule 1]: events Inferred c_65 from numerical as categorical\n",
      "[rule 1]: events Inferred c_66 from numerical as categorical\n",
      "[rule 1]: events Inferred c_67 from numerical as categorical\n",
      "[rule 1]: events Inferred c_68 from numerical as categorical\n",
      "[rule 1]: events Inferred c_69 from numerical as categorical\n",
      "[rule 1]: events Inferred c_70 from numerical as categorical\n",
      "[rule 1]: events Inferred c_71 from numerical as categorical\n",
      "[rule 1]: events Inferred c_72 from numerical as categorical\n",
      "[rule 1]: events Inferred c_73 from numerical as categorical\n",
      "[rule 1]: events Inferred c_74 from numerical as categorical\n",
      "[rule 1]: events Inferred c_75 from numerical as categorical\n",
      "[rule 1]: events Inferred c_76 from numerical as categorical\n",
      "[rule 1]: events Inferred c_77 from numerical as categorical\n",
      "[rule 1]: events Inferred c_78 from numerical as categorical\n",
      "[rule 1]: events Inferred c_79 from numerical as categorical\n",
      "[rule 1]: events Inferred c_80 from numerical as categorical\n",
      "[rule 1]: events Inferred c_81 from numerical as categorical\n",
      "[rule 1]: events Inferred c_82 from numerical as categorical\n",
      "[rule 1]: events Inferred c_83 from numerical as categorical\n",
      "[rule 1]: events Inferred c_84 from numerical as categorical\n",
      "[rule 1]: events Inferred c_85 from numerical as categorical\n",
      "[rule 1]: events Inferred c_86 from numerical as categorical\n",
      "[rule 1]: events Inferred c_87 from numerical as categorical\n",
      "[rule 1]: events Inferred c_88 from numerical as categorical\n",
      "[rule 1]: events Inferred c_89 from numerical as categorical\n",
      "[rule 1]: events Inferred c_90 from numerical as categorical\n",
      "[rule 1]: events Inferred c_91 from numerical as categorical\n",
      "[rule 1]: events Inferred c_92 from numerical as categorical\n",
      "[rule 1]: events Inferred c_93 from numerical as categorical\n",
      "[rule 1]: events Inferred c_94 from numerical as categorical\n",
      "[rule 1]: events Inferred c_95 from numerical as categorical\n",
      "[rule 1]: events Inferred c_96 from numerical as categorical\n",
      "[rule 1]: events Inferred c_97 from numerical as categorical\n",
      "[rule 1]: events Inferred c_98 from numerical as categorical\n",
      "[rule 1]: events Inferred c_99 from numerical as categorical\n",
      "[rule 1]: events Inferred c_100 from numerical as categorical\n",
      "[rule 0]: event_interest Inferred id from numerical as categorical\n",
      "[rule 0]: users Inferred user_id from numerical as categorical\n",
      "[rule 1]: users Inferred locale from text_embedded as categorical\n",
      "[rule 1]: users Inferred birthyear from numerical as categorical\n",
      "[rule 1]: users Inferred timezone from numerical as categorical\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import infer_type_in_db\n",
    "from utils.tokenize import tokenize_database\n",
    "col_type_dict = infer_type_in_db(db, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table event_attendees\n",
      "id: categorical\n",
      "event: categorical\n",
      "status: categorical\n",
      "user_id: categorical\n",
      "start_time: timestamp\n",
      "****************************************\n",
      "Table user_friends\n",
      "id: categorical\n",
      "user: categorical\n",
      "friend: categorical\n",
      "****************************************\n",
      "Table events\n",
      "event_id: categorical\n",
      "user_id: categorical\n",
      "start_time: timestamp\n",
      "city: text_embedded\n",
      "state: text_embedded\n",
      "zip: text_embedded\n",
      "country: text_embedded\n",
      "lat: numerical\n",
      "lng: numerical\n",
      "c_1: categorical\n",
      "c_2: categorical\n",
      "c_3: categorical\n",
      "c_4: categorical\n",
      "c_5: categorical\n",
      "c_6: categorical\n",
      "c_7: categorical\n",
      "c_8: categorical\n",
      "c_9: categorical\n",
      "c_10: categorical\n",
      "c_11: categorical\n",
      "c_12: categorical\n",
      "c_13: categorical\n",
      "c_14: categorical\n",
      "c_15: categorical\n",
      "c_16: categorical\n",
      "c_17: categorical\n",
      "c_18: categorical\n",
      "c_19: categorical\n",
      "c_20: categorical\n",
      "c_21: categorical\n",
      "c_22: categorical\n",
      "c_23: categorical\n",
      "c_24: categorical\n",
      "c_25: categorical\n",
      "c_26: categorical\n",
      "c_27: categorical\n",
      "c_28: categorical\n",
      "c_29: categorical\n",
      "c_30: categorical\n",
      "c_31: categorical\n",
      "c_32: categorical\n",
      "c_33: categorical\n",
      "c_34: categorical\n",
      "c_35: categorical\n",
      "c_36: categorical\n",
      "c_37: categorical\n",
      "c_38: categorical\n",
      "c_39: categorical\n",
      "c_40: categorical\n",
      "c_41: categorical\n",
      "c_42: categorical\n",
      "c_43: categorical\n",
      "c_44: categorical\n",
      "c_45: categorical\n",
      "c_46: categorical\n",
      "c_47: categorical\n",
      "c_48: categorical\n",
      "c_49: categorical\n",
      "c_50: categorical\n",
      "c_51: categorical\n",
      "c_52: categorical\n",
      "c_53: categorical\n",
      "c_54: categorical\n",
      "c_55: categorical\n",
      "c_56: categorical\n",
      "c_57: categorical\n",
      "c_58: categorical\n",
      "c_59: categorical\n",
      "c_60: categorical\n",
      "c_61: categorical\n",
      "c_62: categorical\n",
      "c_63: categorical\n",
      "c_64: categorical\n",
      "c_65: categorical\n",
      "c_66: categorical\n",
      "c_67: categorical\n",
      "c_68: categorical\n",
      "c_69: categorical\n",
      "c_70: categorical\n",
      "c_71: categorical\n",
      "c_72: categorical\n",
      "c_73: categorical\n",
      "c_74: categorical\n",
      "c_75: categorical\n",
      "c_76: categorical\n",
      "c_77: categorical\n",
      "c_78: categorical\n",
      "c_79: categorical\n",
      "c_80: categorical\n",
      "c_81: categorical\n",
      "c_82: categorical\n",
      "c_83: categorical\n",
      "c_84: categorical\n",
      "c_85: categorical\n",
      "c_86: categorical\n",
      "c_87: categorical\n",
      "c_88: categorical\n",
      "c_89: categorical\n",
      "c_90: categorical\n",
      "c_91: categorical\n",
      "c_92: categorical\n",
      "c_93: categorical\n",
      "c_94: categorical\n",
      "c_95: categorical\n",
      "c_96: categorical\n",
      "c_97: categorical\n",
      "c_98: categorical\n",
      "c_99: categorical\n",
      "c_100: categorical\n",
      "c_other: numerical\n",
      "****************************************\n",
      "Table event_interest\n",
      "user: categorical\n",
      "event: categorical\n",
      "invited: categorical\n",
      "timestamp: timestamp\n",
      "interested: categorical\n",
      "not_interested: categorical\n",
      "id: categorical\n",
      "****************************************\n",
      "Table users\n",
      "user_id: categorical\n",
      "locale: categorical\n",
      "birthyear: categorical\n",
      "gender: categorical\n",
      "joinedAt: timestamp\n",
      "location: text_embedded\n",
      "timezone: categorical\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# check all col types\n",
    "for table_name, col_types in col_type_dict.items():\n",
    "    print(f\"Table {table_name}\")\n",
    "    for col, type_ in col_types.items():\n",
    "        print(f\"{col}: {type_}\")\n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------> Tokenizing event_attendees each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-events/event_attendees.npy\n",
      "----------------> Tokenizing user_friends each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-events/user_friends.npy\n",
      "----------------> Tokenizing events each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-events/events.npy\n",
      "----------------> Tokenizing event_interest each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-events/event_interest.npy\n",
      "----------------> Tokenizing users each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-events/users.npy\n"
     ]
    }
   ],
   "source": [
    "tk_db = tokenize_database(db, col_type_dict, './tmp_docs/rel-events', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (710976225.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    generated the documents and build the retrieval index\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "generated the documents and build the retrieval index\n",
    "# entity_to_docs = {}\n",
    "# walk_length = 10\n",
    "# round = 10\n",
    "# for entity in entity_tables:\n",
    "#    _, entity_to_docs[entity] = generate_document_given_table(\n",
    "#         homoGraph, \n",
    "#         tk_db, \n",
    "#         entity, \n",
    "#         walk_length=walk_length, \n",
    "#         round = round, \n",
    "#         verbose=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "# # temporarily save the index\n",
    "# import bm25s\n",
    "# entity_to_retriver = {}\n",
    "# for entity, docs in entity_to_docs.items():\n",
    "#     retriever = bm25s.BM25(backend=\"numba\")\n",
    "#     retriever.index(docs)\n",
    "#     retriever.activate_numba_scorer()\n",
    "#     entity_to_retriver[entity] = retriever\n",
    "\n",
    "# # save the retriever\n",
    "# for entity, retriever in entity_to_retriver.items():\n",
    "#     retriever.save(f\"./tmp/event/{entity}_retriever_bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ./tmp/event/events_retriever_bm25\n",
      "load ./tmp/event/users_retriever_bm25\n"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "entity_to_retriver = {}\n",
    "\n",
    "# load the retriever\n",
    "entity_to_retriver = {}\n",
    "for entity in entity_tables:\n",
    "    path = f\"./tmp/event/{entity}_retriever_bm25\"\n",
    "    retriever = bm25s.BM25.load(path)\n",
    "    retriever.activate_numba_scorer()\n",
    "    entity_to_retriver[entity] = retriever\n",
    "    print(f\"load {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table events - shape torch.Size([11465, 10, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table users - shape torch.Size([37143, 10, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "# resample the candidate docs, and retrieve the related docs in the bm25 retrievers\n",
    "# generated the documents and build the retrieval index\n",
    "walk_length = 8\n",
    "round = 10\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n\n",
    "    entity_candidate_pkys[entity], entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> events ---- users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:19<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add cross table edges #8174 between events and users\n",
      "--------> users ---- events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:07<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add cross table edges #30902 between users and events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the cross-table edges,\n",
    "import numpy as np\n",
    "topn = 20\n",
    "edge_dict = {}\n",
    "batch_size = 2048\n",
    "# (src_table, des_table) -> edge 2-D array\n",
    "for entity, retrieve_entity in edge_candidates_pairs:\n",
    "\n",
    "    # retrieve the related docs\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    retriever = entity_to_retriver[retrieve_entity]\n",
    "    \n",
    "    print(f\"--------> {entity} ---- {retrieve_entity}\")\n",
    "    score_np = []\n",
    "    related_pkys_np = []\n",
    "    for batch_idx in tqdm(range(0, len(entity_query_docs), batch_size)):\n",
    "        batch_query_docs = entity_query_docs[batch_idx:batch_idx+batch_size]\n",
    "        related_pkys, scores = retriever.retrieve(batch_query_docs, k = topn, n_threads = -1)\n",
    "        score_np.append(np.array(scores))\n",
    "        related_pkys_np.append(np.array(related_pkys))\n",
    "    \n",
    "    score_np = np.concatenate(score_np, axis = 0)\n",
    "    related_pkys_np = np.concatenate(related_pkys_np, axis = 0)\n",
    "    \n",
    "    threshold = score_np.mean() + 2*scores.std()\n",
    "    # Get indices where the score is above the threshold\n",
    "    mask = score_np > threshold\n",
    "\n",
    "    # Apply the mask\n",
    "    filtered_cols = related_pkys_np[mask]\n",
    "\n",
    "    # Generate the corresponding query entities\n",
    "    entity_query_pkys = np.array(entity_query_pkys)  # shape [n]\n",
    "\n",
    "    # Repeat each query item the number of True values per row in the mask\n",
    "    row_repeats = mask.sum(axis=1)  # how many times to repeat each query\n",
    "    filtered_rows = np.repeat(entity_query_pkys, row_repeats)\n",
    "    \n",
    "    \n",
    "    filtered_edge = np.stack([filtered_rows, filtered_cols], axis=1)\n",
    "    # added edge\n",
    "    num_edges = filtered_rows.shape[0]\n",
    "    edge_dict[(entity, retrieve_entity)] = filtered_edge\n",
    "    print(f\"Add cross table edges #{num_edges} between {entity} and {retrieve_entity}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (src_table, des_table) -> edge 2-D array\n",
    "npz_data = {\n",
    "    f\"{src}-{dst}\": edge_array\n",
    "    for (src, dst), edge_array in edge_dict.items()\n",
    "}\n",
    "\n",
    "path = f\"./edges/rel-event-edges.npz\"\n",
    "np.savez(path, **npz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table events - shape torch.Size([11465, 10, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table users - shape torch.Size([37143, 10, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "# resample the candidate docs, and retrieve the related docs in the bm25 retrievers\n",
    "# generated the documents and build the retrieval index\n",
    "walk_length = 10\n",
    "round = 10\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n\n",
    "    entity_candidate_pkys[entity], entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:14<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #3554, original candidate 11465 in events table\n",
      "--------> users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:09<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #24091, original candidate 37143 in users table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# self-entity correlation\n",
    "# which can generate the positive pairs in the contrastive learning\n",
    "entity_topn = {\n",
    "    \"events\": 21,\n",
    "    \"users\": 21,\n",
    "}\n",
    "# the most related doc should be itself, so we need to retrieve topn + 1\n",
    "positive_pool_dict = {}\n",
    "# entity -> positive candidate, padding the non-value\n",
    "threshold = 0.7\n",
    "batch_size = 2048\n",
    "for entity, retriever in entity_to_retriver.items():\n",
    "    # retrieve the related docs\n",
    "    topn = entity_topn[entity]\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    score_np = []\n",
    "    related_pkys_np = []\n",
    "    print(f\"--------> {entity}\")\n",
    "    for batch_idx in tqdm(range(0, len(entity_query_docs), batch_size)):\n",
    "        batch_query_docs = entity_query_docs[batch_idx:batch_idx + batch_size]\n",
    "        related_pkys, scores = retriever.retrieve(batch_query_docs, k = topn, n_threads=-1)\n",
    "        score_np.append(np.array(scores))\n",
    "        related_pkys_np.append(np.array(related_pkys))\n",
    "    \n",
    "    score_np = np.concatenate(score_np, axis = 0)\n",
    "    related_pkys_np = np.concatenate(related_pkys_np, axis = 0)\n",
    "    # Get indices where the score is above the threshold\n",
    "    # the first one is the most related one, should be itself\n",
    "    mask = score_np > (score_np[:,[0]] * threshold)\n",
    "    # add padding for those non-related docs which is filtered out.\n",
    "    related_pkys_np[~mask] = -1\n",
    "    rows_num = np.sum(mask, axis = 1)\n",
    "    # except itself, still has similar docs\n",
    "    rows_mask = rows_num > 1\n",
    "    positive_pool = related_pkys_np[rows_mask]\n",
    "    \n",
    "    positive_pool_dict[entity] = positive_pool\n",
    "    print(f\"Generate positive pools #{len(positive_pool)}, original candidate {len(entity_query_docs)} in {entity} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./samples/rel-event-samples.npz\"\n",
    "np.savez(path, **positive_pool_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
