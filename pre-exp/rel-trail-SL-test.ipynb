{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from model.base import CompositeModel, FeatureEncodingPart, NodeRepresentationPart\n",
    "from relbench.modeling.nn import HeteroTemporalEncoder\n",
    "\n",
    "import os\n",
    "\n",
    "# Disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "from relbench.base import BaseTask\n",
    "from torch_geometric.seed import seed_everything\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-trial/db...\n",
      "Done in 7.80 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(name=\"rel-trial\", download=True)\n",
    "db = dataset.get_db()\n",
    "task_a = get_task(\"rel-trial\", \"study-outcome\", download = True)\n",
    "task_b = get_task(\"rel-trial\", \"site-success\", download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from relbench.base import Database\n",
    "from torch_frame.config import TextEmbedderConfig\n",
    "from torch_frame import stype\n",
    "\n",
    "from typing import Dict, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gid_to_entity, entity_to_gid = {}, {}\n",
    "# node_id -> (table_name, pkey_index)\n",
    "# table_name -> { pkey_index -> node_id}\n",
    "\n",
    "# initialize node\n",
    "for table_name, table in db.table_dict.items():\n",
    "    df = table.df\n",
    "    if table.pkey_col is not None:\n",
    "        assert (df[table.pkey_col].values == np.arange(len(df))).all()\n",
    "\n",
    "    gid_init = len(gid_to_entity)\n",
    "    gid_end = gid_init + len(df)\n",
    "    gids = np.arange(gid_init, gid_end)\n",
    "    pkey_idxs = np.arange(len(df))\n",
    "\n",
    "    tmp_id = [(table_name, i) for i  in range(len(df))]\n",
    "    gid_to_entity.update(dict(zip(gids, tmp_id)))\n",
    "    entity_to_gid[table_name] = dict(zip(pkey_idxs, gids))\n",
    "\n",
    "# initialize edge\n",
    "row, col = [], []\n",
    "# source node, target node\n",
    "for table_name, table in db.table_dict.items():\n",
    "    df = table.df\n",
    "    for fkey_name, pkey_table_name in table.fkey_col_to_pkey_table.items():\n",
    "        pkey_index = df[fkey_name]\n",
    "        # Filter out dangling foreign keys (missing value)\n",
    "        mask = ~pkey_index.isna()\n",
    "        fkey_index = pd.Series(np.arange(len(pkey_index)))\n",
    "\n",
    "        # Filter missing value\n",
    "        pkey_index = pkey_index[mask].astype(int)\n",
    "        fkey_index = fkey_index[mask]\n",
    "        \n",
    "        # convert to node id\n",
    "        pkey_gid = pkey_index.map(entity_to_gid[pkey_table_name]).values\n",
    "        fkey_gid = fkey_index.map(entity_to_gid[table_name]).values\n",
    "\n",
    "        pkey_gid = torch.LongTensor(pkey_gid)\n",
    "        fkey_gid = torch.LongTensor(fkey_gid)\n",
    "\n",
    "        # fkey -> pkey edges\n",
    "        row.append(fkey_gid)\n",
    "        col.append(pkey_gid)\n",
    "\n",
    "        # pkey -> fkey edges\n",
    "        row.append(pkey_gid)\n",
    "        col.append(fkey_gid)\n",
    "\n",
    "row = torch.cat(row, dim=0)\n",
    "col = torch.cat(col, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_cluster import random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the target node\n",
    "node_gid = [ v for k,v in entity_to_gid[task_a.entity_table].items()]\n",
    "node_gid = sorted(node_gid)\n",
    "target_node = torch.LongTensor(node_gid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length = len(db.table_dict)\n",
    "round = 10\n",
    "walks = []\n",
    "for _ in range(round):\n",
    "    walk = random_walk(row, col, target_node, walk_length, p=10, q=0.5)\n",
    "    # walk -> [num_target_node, walk_length]\n",
    "    walk = walk.unsqueeze(1)\n",
    "    # walk -> [num_target_node, 1, walk_length]\n",
    "    walks.append(walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249730, 10, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_bags = torch.concatenate(walks, dim = 1)\n",
    "# node_bags -> [num_target_node, round, walk_length]\n",
    "node_bags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for BM25 relevance score\n",
    "\n",
    "# remove the foreignKey columns in db.df table, since the graph is already constructed.\n",
    "# the foreignKey column is duplicated data\n",
    "for table_name in db.table_dict.keys():\n",
    "    for fk_col,  _ in db.table_dict[table_name].fkey_col_to_pkey_table.items():\n",
    "        db.table_dict[table_name].df.drop(fk_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rule 0] Convert interventions.intervention_id from numerical to categorical data\n",
      "Unique value: 3462, Count value: 3462\n",
      "[Rule 0] Convert interventions_studies.id from numerical to categorical data\n",
      "Unique value: 171771, Count value: 171771\n",
      "[Rule 0] Convert facilities_studies.id from numerical to categorical data\n",
      "Unique value: 1798765, Count value: 1798765\n",
      "[Rule 0] Convert sponsors.sponsor_id from numerical to categorical data\n",
      "Unique value: 53241, Count value: 53241\n",
      "[Rule 0] Convert sponsors.agency_class from text_embedded to categorical data\n",
      "Unique value: 9, Count value: 53241\n",
      "[Rule 0] Convert eligibilities.id from numerical to categorical data\n",
      "Unique value: 249730, Count value: 249730\n",
      "[Rule 1] Convert eligibilities.minimum_age from text_embedded to categorical data\n",
      "Unique value: 289, Count value: 234048\n",
      "[Rule 1] Convert eligibilities.maximum_age from text_embedded to categorical data\n",
      "Unique value: 435, Count value: 132548\n",
      "[Rule 0] Convert reported_event_totals.id from numerical to categorical data\n",
      "Unique value: 383064, Count value: 383064\n",
      "[Rule 0] Convert designs.id from numerical to categorical data\n",
      "Unique value: 249093, Count value: 249093\n",
      "[Rule 0] Convert designs.observational_model from text_embedded to categorical data\n",
      "Unique value: 10, Count value: 47428\n",
      "[Rule 1] Convert designs.primary_purpose from text_embedded to categorical data\n",
      "Unique value: 11, Count value: 194280\n",
      "[Rule 0] Convert conditions_studies.id from numerical to categorical data\n",
      "Unique value: 408422, Count value: 408422\n",
      "[Rule 0] Convert drop_withdrawals.id from numerical to categorical data\n",
      "Unique value: 381199, Count value: 381199\n",
      "[Rule 1] Convert drop_withdrawals.period from text_embedded to categorical data\n",
      "Unique value: 7039, Count value: 381199\n",
      "[Rule 0] Convert studies.nct_id from numerical to categorical data\n",
      "Unique value: 249730, Count value: 249730\n",
      "[Rule 1] Convert studies.biospec_retention from text_embedded to categorical data\n",
      "Unique value: 4, Count value: 10095\n",
      "[Rule 0] Convert studies.source_class from text_embedded to categorical data\n",
      "Unique value: 8, Count value: 249730\n",
      "[Rule 0] Convert outcome_analyses.id from numerical to categorical data\n",
      "Unique value: 225846, Count value: 225846\n",
      "[Rule 0] Convert outcome_analyses.non_inferiority_type from text_embedded to categorical data\n",
      "Unique value: 8, Count value: 225846\n",
      "[Rule 0] Convert outcome_analyses.param_type from text_embedded to categorical data\n",
      "Unique value: 4751, Count value: 152430\n",
      "[Rule 1] Convert outcome_analyses.method from text_embedded to categorical data\n",
      "Unique value: 3340, Count value: 194999\n",
      "[Rule 0] Convert sponsors_studies.id from numerical to categorical data\n",
      "Unique value: 391462, Count value: 391462\n",
      "[Rule 0] Convert outcomes.id from numerical to categorical data\n",
      "Unique value: 411933, Count value: 411933\n",
      "[Rule 0] Convert outcomes.dispersion_type from text_embedded to categorical data\n",
      "Unique value: 36, Count value: 250950\n",
      "[Rule 0] Convert conditions.condition_id from numerical to categorical data\n",
      "Unique value: 3973, Count value: 3973\n",
      "[Rule 0] Convert facilities.facility_id from numerical to categorical data\n",
      "Unique value: 453233, Count value: 453233\n",
      "[Rule 0] Convert facilities.city from text_embedded to categorical data\n",
      "Unique value: 28166, Count value: 453221\n",
      "[Rule 0] Convert facilities.state from text_embedded to categorical data\n",
      "Unique value: 7723, Count value: 225929\n",
      "[Rule 0] Convert facilities.zip from text_embedded to categorical data\n",
      "Unique value: 48348, Count value: 353213\n",
      "[Rule 1] Convert facilities.country from text_embedded to categorical data\n",
      "Unique value: 211, Count value: 453221\n"
     ]
    }
   ],
   "source": [
    "# convert the text and numerical data to categroical data through cluster or bin\n",
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "\n",
    "\n",
    "# rule 0\n",
    "# based on the column name\n",
    "# we predefined some\n",
    "numerical_keywords = [\n",
    "    'count', 'num', 'amount', 'total', 'length', 'height', 'value', 'rate',  'number',\n",
    "    'score', 'level', 'size', 'price', 'percent', 'ratio', 'volume', 'index', 'avg', 'max', 'min'\n",
    "]\n",
    "categorical_keywords = [\n",
    "    'type', 'category', 'class', 'label', 'status', 'code', 'id',\n",
    "    'region', 'zone', 'flag', 'is_', 'has_', 'mode', 'city', 'state', 'zip'\n",
    "]\n",
    "\n",
    "text_keywords = [\n",
    "    'description', 'comments', 'content', 'name', 'review', 'message', 'note', 'query', 'summary'\n",
    "]\n",
    "\n",
    "\n",
    "# rule 1\n",
    "# unique_value < 0.02 * total_value -> categorical data\n",
    "# rule 1, general rule for text and numerical data\n",
    "\n",
    "for table_name, table in db.table_dict.items():\n",
    "    df = table.df\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        if col_name not in col_to_stype_dict[table_name]:\n",
    "            continue\n",
    "        guess_type = col_to_stype_dict[table_name][col_name]\n",
    "\n",
    "        # rule 0\n",
    "        if any([kw in col_name.lower() for kw in text_keywords]):\n",
    "            if guess_type == stype.text_embedded:\n",
    "                continue\n",
    "\n",
    "        if any([kw in col_name.lower() for kw in numerical_keywords]):\n",
    "            # check the data can be converted to numerical data\n",
    "            is_convertible = (\n",
    "                pd.to_numeric(df[col_name], errors='coerce').notna()\n",
    "                + df[col_name].isna()).all()\n",
    "            \n",
    "            if is_convertible:\n",
    "                if guess_type != stype.numerical:\n",
    "                    print(\n",
    "                        f\"[Rule 0] Convert {table_name}.{col_name} from {guess_type} to numerical data\")\n",
    "                col_to_stype_dict[table_name][col_name] = stype.numerical\n",
    "                continue\n",
    "\n",
    "        unique_value = len(df[col_name].unique())\n",
    "        count_value = (~df[col_name].isna()).sum()\n",
    "\n",
    "        if any([kw in col_name.lower() for kw in categorical_keywords]):\n",
    "            if guess_type != stype.categorical:\n",
    "                # print the unique value and count value for check\n",
    "                print(\n",
    "                    f\"[Rule 0] Convert {table_name}.{col_name} from {guess_type} to categorical data\")\n",
    "                print(\n",
    "                    f\"Unique value: {unique_value}, Count value: {count_value}\")\n",
    "\n",
    "            col_to_stype_dict[table_name][col_name] = stype.categorical\n",
    "            continue\n",
    "\n",
    "        # rule 1\n",
    "        if guess_type == stype.categorical or guess_type == stype.timestamp:\n",
    "            continue\n",
    "        # check whether can convert to numerical\n",
    "        is_convertible = (\n",
    "            pd.to_numeric(df[col_name], errors='coerce').notna()\n",
    "            + df[col_name].isna()).all()\n",
    "        \n",
    "        if is_convertible and guess_type == stype.numerical:\n",
    "            continue\n",
    "\n",
    "        # for  type  numerical or text_embedding check Rule 1\n",
    "        if unique_value*1.0 / count_value < 0.02:\n",
    "            # minimum average frequency is 50.\n",
    "            col_to_stype_dict[table_name][col_name] = stype.categorical\n",
    "            print(\n",
    "                f\"[Rule 1] Convert {table_name}.{col_name} from {guess_type} to categorical data\")\n",
    "            print(f\"Unique value: {unique_value}, Count value: {count_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin reported_event_totals.subjects_affected to 137 bins, convert numerical data to categorical data\n",
      "Bin reported_event_totals.subjects_at_risk to 136 bins, convert numerical data to categorical data\n",
      "Bin drop_withdrawals.count to 146 bins, convert numerical data to categorical data\n",
      "Bin studies.enrollment to 126 bins, convert numerical data to categorical data\n",
      "Bin studies.number_of_arms to 115 bins, convert numerical data to categorical data\n",
      "Bin studies.number_of_groups to 65 bins, convert numerical data to categorical data\n",
      "Bin outcome_analyses.param_value to 107 bins, convert numerical data to categorical data\n",
      "Bin outcome_analyses.dispersion_value to 70 bins, convert numerical data to categorical data\n",
      "Bin outcome_analyses.p_value to 115 bins, convert numerical data to categorical data\n",
      "Bin outcome_analyses.ci_percent to 109 bins, convert numerical data to categorical data\n",
      "Bin outcome_analyses.ci_lower_limit to 105 bins, convert numerical data to categorical data\n",
      "Bin outcome_analyses.ci_upper_limit to 105 bins, convert numerical data to categorical data\n"
     ]
    }
   ],
   "source": [
    "# for numerical data, we try to cluster and bin it convert it to categorical data\n",
    "# for text data, we encoded it using pre-trained model and cluster them to categorical data\n",
    "\n",
    "table_data = {}\n",
    "# table_name -> pd.df\n",
    "for table_name, table in db.table_dict.items():\n",
    "    df = table.df.copy()\n",
    "    table_data[table_name] = df\n",
    "    for col_name in df.columns:\n",
    "        if col_name not in col_to_stype_dict[table_name]:\n",
    "            continue\n",
    "\n",
    "        dtype = col_to_stype_dict[table_name][col_name]\n",
    "        if dtype == stype.numerical:\n",
    "            # bin\n",
    "            # we assign equal-width bin, and dynamically adjust the bin size\n",
    "\n",
    "            # step 1. dynamically adjust the bin number\n",
    "            n = (~df[col_name].isna()).sum()\n",
    "            binned = pd.Series(index=df.index, dtype='object')\n",
    "\n",
    "            if n > 1_000:\n",
    "                # Rice rule\n",
    "                bin_num = math.ceil(2 * n**(1/3))\n",
    "            else:\n",
    "                # n < 1000,\n",
    "                # Sturges' formula\n",
    "                bin_num = math.ceil(math.log2(n) + 1)\n",
    "\n",
    "            # Step 2. bin the outlier first\n",
    "            q1 = df[col_name].quantile(0.1)\n",
    "            q2 = df[col_name].quantile(0.9)\n",
    "            upper_bound = q2 + 1.5 * (q2 - q1)\n",
    "            lower_bound = q1 - 1.5 * (q2 - q1)\n",
    "\n",
    "            upper_outlier_mask = df[col_name] > upper_bound\n",
    "            lower_outlier_mask = df[col_name] < lower_bound\n",
    "            normal_mask = ~upper_outlier_mask & ~lower_outlier_mask\n",
    "            normal_mask = normal_mask & df[col_name].notna()\n",
    "\n",
    "            upper_outlier_label = f\"larger than {upper_bound}\"\n",
    "            lower_outlier_label = f\"smaller than {lower_bound}\"\n",
    "\n",
    "            binned[upper_outlier_mask] = upper_outlier_label\n",
    "            binned[lower_outlier_mask] = lower_outlier_label\n",
    "\n",
    "            # Step 3. bin the normal data\n",
    "\n",
    "            # first get the bin labeled\n",
    "            _, bin_edges = pd.cut(df[col_name][normal_mask], bins=bin_num,\n",
    "                                  labels=False, retbins=True, include_lowest=True)\n",
    "\n",
    "            bin_labels = [\n",
    "                f\"{bin_edges[i]:.0f}-{bin_edges[i+1]:.0f}\" if bin_edges[i].is_integer() and bin_edges[i+1].is_integer()\n",
    "                else f\"{bin_edges[i]:.2f}-{bin_edges[i+1]:.2f}\"\n",
    "                for i in range(len(bin_edges) - 1)\n",
    "            ]\n",
    "            \n",
    "            binned[normal_mask] = pd.cut(df[col_name][normal_mask], bins=bin_edges, labels=bin_labels)\n",
    "            \n",
    "            print(f\"Bin {table_name}.{col_name} to {bin_num} bins, convert numerical data to categorical data\")\n",
    "            df[col_name] = binned\n",
    "            \n",
    "        if dtype == stype.text_embedded:\n",
    "            # encode the text in next step\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text type data, basically, text type data is more diverse than numerical data and categorical data\n",
    "# there are several steps to process:\n",
    "# --- 1. using pre-trained model to encode the text data.\n",
    "# --- 2. generally the text embedding is high-dimensional, we reduce the dimensionality in each attribute.\n",
    "# --- 3. we cluster these low-dimensional vector to bins\n",
    "# --- 4. we convert the cluster result to categorical data\n",
    "\n",
    "# <Fail>\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import umap\n",
    "# import hdbscan\n",
    "\n",
    "# encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# for table_name, df in table_data.items():\n",
    "#     for col_name in df.columns:\n",
    "#         if col_name not in col_to_stype_dict[table_name]:\n",
    "#             continue\n",
    "#         dtype = col_to_stype_dict[table_name][col_name]\n",
    "#         if dtype != stype.text_embedded:\n",
    "#             continue\n",
    "        \n",
    "#         # preprocess text embedding\n",
    "#         series = df[col_name]\n",
    "#         sentences = series[series.notna()].tolist()\n",
    "        \n",
    "#         sentence_set = list(set(sentences))\n",
    "#         embedding_set = encoder.encode(sentence_set)\n",
    "#         # reduce the dimension from 384 to 10 using UMAP\n",
    "#         std_embedding_set = umap.UMAP(n_components=10, random_state = 2025, n_jobs = 1).fit_transform(embedding_set)\n",
    "        \n",
    "#         # using HDBSCAN to cluster the data\n",
    "#         labels = hdbscan.HDBSCAN(\n",
    "#             min_samples = 10,\n",
    "#             min_cluster_size = 30,\n",
    "#         ).fit_predict(std_embedding_set)\n",
    "        \n",
    "#         unique_elements, counts = np.unique(labels, return_counts = True)\n",
    "       \n",
    "#         # convert outlier to outlier label\n",
    "#         outlier_mask = labels == -1\n",
    "#         outlier_num = outlier_mask.sum()\n",
    "#         print(outlier_num)\n",
    "\n",
    "#         labels[outlier_mask] = list(range(len(unique_elements) - 1, len(unique_elements) - 1 + outlier_num))\n",
    "#         unique_num = len(unique_elements) - 1 + outlier_num\n",
    "        \n",
    "#         print(f\"Cluster {table_name}.{col_name} to {unique_num} bins, contains {outlier_num} outlier, Text Embedding to Categorical data\")\n",
    "        \n",
    "#         # convert the label to categorical data\n",
    "#         sentence_to_label = {st: int(y) for (st, y) in zip(sentence_set, labels)}\n",
    "#         # series[series.notna()] = series[series.notna()].map(sentence_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interventions.mesh_term is text_embedded\n",
      "sponsors.name is text_embedded\n",
      "eligibilities.population is text_embedded\n",
      "eligibilities.criteria is text_embedded\n",
      "eligibilities.gender_description is text_embedded\n",
      "designs.masking_description is text_embedded\n",
      "designs.intervention_model_description is text_embedded\n",
      "drop_withdrawals.reason is text_embedded\n",
      "studies.target_duration is text_embedded\n",
      "studies.acronym is text_embedded\n",
      "studies.baseline_population is text_embedded\n",
      "studies.brief_title is text_embedded\n",
      "studies.official_title is text_embedded\n",
      "studies.source is text_embedded\n",
      "studies.biospec_description is text_embedded\n",
      "studies.detailed_descriptions is text_embedded\n",
      "studies.brief_summaries is text_embedded\n",
      "outcome_analyses.non_inferiority_description is text_embedded\n",
      "outcome_analyses.p_value_description is text_embedded\n",
      "outcome_analyses.method_description is text_embedded\n",
      "outcome_analyses.estimate_description is text_embedded\n",
      "outcome_analyses.groups_description is text_embedded\n",
      "outcome_analyses.other_analysis_description is text_embedded\n",
      "outcomes.title is text_embedded\n",
      "outcomes.description is text_embedded\n",
      "outcomes.time_frame is text_embedded\n",
      "outcomes.population is text_embedded\n",
      "outcomes.units is text_embedded\n",
      "outcomes.units_analyzed is text_embedded\n",
      "conditions.mesh_term is text_embedded\n",
      "facilities.name is text_embedded\n"
     ]
    }
   ],
   "source": [
    "for table_name, col_types in col_to_stype_dict.items():\n",
    "    for col_name, col_type in col_types.items():\n",
    "        if col_type == stype.text_embedded:\n",
    "            print(f\"{table_name}.{col_name} is text_embedded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text type data, basically, text type data is more diverse than numerical data and categorical data\n",
    "# there are several steps to process:\n",
    "\n",
    "# --- 1. using pre-trained model to extract the keywords from text.\n",
    "# --- 2. these keywords are concatenated with the column name to form the token in \"doc\"\n",
    "\n",
    "# in KeyBERT model, we set the setting to default, only extract one-gram keywors for simplicity.\n",
    "\n",
    "# why just extract keywords from this keywords\n",
    "# -- reduces noise by focusing only on meaningful and frequent concepts.\n",
    "# -- creates a more compact document with relevant terms.\n",
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "text_dict_in_col_table = {}\n",
    "for table_name, df in table_data.items():\n",
    "    for col_name in df.columns:\n",
    "        if col_name not in col_to_stype_dict[table_name]:\n",
    "            continue\n",
    "\n",
    "        dtype = col_to_stype_dict[table_name][col_name]\n",
    "        if dtype != stype.text_embedded:\n",
    "            continue\n",
    "\n",
    "        # only text embedding data\n",
    "        series = df[col_name]\n",
    "        sentences =  series[series.notna()].tolist()\n",
    "        sentence_set = list(set(sentences))\n",
    "        print(f\"=> Table @{table_name} and col @{col_name} has {len(series)} records, has {len(sentence_set)} unique sentences\")\n",
    "        \n",
    "        # convert the sentences to keyword set\n",
    "        # construct a dict {sentence : keyword set}\n",
    "        batch_size = 2048\n",
    "        sentence_to_kws = {}\n",
    "        for i in range(0, len(sentence_set), batch_size):\n",
    "            batch = sentence_set[i:i+batch_size]\n",
    "            keywords = kw_model.extract_keywords(batch)\n",
    "            sentence_to_kws.update(dict(zip(batch, keywords)))\n",
    "\n",
    "        sentence_to_kws = {st: [kw for kw, _ in kws] for st, kws in sentence_to_kws.items()}\n",
    "        # remove relevance score\n",
    "        \n",
    "        print(f\"Convert the [{table_name}.{col_name}] from text to keywords set\")\n",
    "        # assign keywords_sets to the dataframe\n",
    "        # df[mask][col_name] = keywords_series\n",
    "        text_dict_in_col_table[(table_name, col_name)] = sentence_to_kws\n",
    "        # each keyword is like this [('keyword1', 0.9), ('keyword2', 0.8), ('keyword3', 0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IO READ]\n",
    "# read the text_dict\n",
    "import pickle\n",
    "tmp_text_dict_file_path = './tmp/text_dict.pkl'\n",
    "with open(tmp_text_dict_file_path, \"rb\") as f:\n",
    "    text_dict_in_col_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IO WRIRTE]\n",
    "# temporarily save the text_dict_in_col_table\n",
    "import pickle\n",
    "tmp_text_dict_file_path = './tmp/text_dict.pkl'\n",
    "with open(tmp_text_dict_file_path, 'wb') as f:\n",
    "    pickle.dump(text_dict_in_col_table, f)\n",
    "file_size = os.path.getsize(tmp_text_dict_file_path)\n",
    "print(f\"Save the text_dict_in_col_table to {tmp_text_dict_file_path}, size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the walk related node and convert it to \"Doc\"\n",
    "# the \"word\" should be table_name + col_name + value\n",
    "# if the column is type, the \"word\" should be table_name + col_name + keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:36<00:00,  6.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# first preprocess\n",
    "# directly process a gid -> doc Dict\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "table_idx_to_doc = {}\n",
    "\n",
    "for table_name, df in tqdm(table_data.items()):\n",
    "    pkey_to_gid = entity_to_gid[table_name]\n",
    "    \n",
    "    tmp_df = pd.DataFrame()\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        if col_name not in col_to_stype_dict[table_name]:\n",
    "            continue\n",
    "        \n",
    "        col_type = col_to_stype_dict[table_name][col_name]\n",
    "        \n",
    "        if col_type is stype.numerical or col_type is stype.categorical or col_type is stype.timestamp:\n",
    "            series = df[col_name].apply(lambda x: [f\"{table_name}.{col_name}.{x}\"] if x is not None else [])\n",
    "            \n",
    "        elif col_type is stype.text_embedded:\n",
    "            sentence_to_kws = text_dict_in_col_table[(table_name, col_name)]\n",
    "            sentence_to_words = {st: [f\"{table_name}.{col_name}.{kw}\" for kw in kws] for st, kws in sentence_to_kws.items()}\n",
    "            series = df[col_name].apply(lambda x: sentence_to_words[x] if x is not None else [])\n",
    "\n",
    "        tmp_df[col_name] = series\n",
    "        \n",
    "    # reduce each row\n",
    "    idx_to_keywords = tmp_df.apply(lambda x: reduce(lambda a, b: a + b, x.tolist()), axis=1).tolist()\n",
    "    table_idx_to_doc[table_name] = idx_to_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [IO]\n",
    "# temporarily save the gid_to_doc and entity_to_gid\n",
    "\n",
    "\n",
    "tmp_gid_to_doc_file_path = './tmp/table_idx_to_doc.pkl'\n",
    "with open(tmp_gid_to_doc_file_path, 'wb') as f:\n",
    "    pickle.dump(table_idx_to_doc, f)\n",
    "# table-> List[List[kw]], the idx is pkey \n",
    "file_size = os.path.getsize(tmp_gid_to_doc_file_path)\n",
    "print(f\"Save the gid_to_doc to {tmp_gid_to_doc_file_path}, size: {file_size} bytes\")\n",
    "\n",
    "tmp_entity_to_gid_file_path = './tmp/entity_to_gid.pkl'\n",
    "with open(tmp_entity_to_gid_file_path, 'wb') as f:\n",
    "    pickle.dump(entity_to_gid, f)\n",
    "file_size = os.path.getsize(tmp_entity_to_gid_file_path)\n",
    "print(f\"Save the entity_to_gid to {tmp_entity_to_gid_file_path}, size: {file_size} bytes\")\n",
    "\n",
    "\n",
    "tmp_git_to_entity_file_path = './tmp/gid_to_entity.pkl'\n",
    "with open(tmp_git_to_entity_file_path, 'wb') as f:\n",
    "    pickle.dump(gid_to_entity, f)\n",
    "# gid -> (table_name, pkey_index)\n",
    "file_size = os.path.getsize(tmp_git_to_entity_file_path)\n",
    "print(f\"Save the gid_to_entity to {tmp_git_to_entity_file_path}, size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249730/249730 [01:06<00:00, 3764.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# based on walk and gid_to_doc, to generate these docs\n",
    "from functools import reduce\n",
    "docs = []\n",
    "for idx in tqdm(range(node_bags.shape[0])):\n",
    "    walks = node_bags[idx].tolist() # [round, walk_length]\n",
    "    # for each walks, we remove repeated node to construct subgraph.\n",
    "    # make sure in each round of sample (RandomWalk), the node is unique\n",
    "    doc = reduce(lambda a, b: list(set(a)) + list(set(b)), walks)\n",
    "    doc = map(lambda x: table_idx_to_doc[gid_to_entity[x][0]][gid_to_entity[x][1]], doc)\n",
    "    doc = reduce(lambda a, b: a + b, doc)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the studies docs to ./tmp/studies_docs.pkl, size: 1666987496 bytes\n"
     ]
    }
   ],
   "source": [
    "# [IO WRITE]\n",
    "# tmporarily save these docs\n",
    "# the target node is table in task_a\n",
    "entity = task_a.entity_table\n",
    "file_name = f\"{entity}_docs.pkl\"\n",
    "file_path = f\"./tmp/{file_name}\"\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(docs, f)\n",
    "file_size = os.path.getsize(file_path)\n",
    "print(f\"Save the {entity} docs to {file_path}, size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
