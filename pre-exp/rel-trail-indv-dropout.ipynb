{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.base import CompositeModel, FeatureEncodingPart, NodeRepresentationPart\n",
    "from relbench.modeling.nn import HeteroTemporalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import relbench\n",
    "from relbench.base import Table, Database, Dataset, EntityTask\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "from relbench.base import BaseTask\n",
    "from torch_geometric.seed import seed_everything\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "from relbench.modeling.graph import get_node_train_table_input\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch_geometric\n",
    "import torch_frame\n",
    "\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from typing import List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch.nn import L1Loss, BCEWithLogitsLoss\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-trial/db...\n",
      "Done in 9.62 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(name=\"rel-trial\", download=True)\n",
    "db = dataset.get_db()\n",
    "task_a = get_task(\"rel-trial\", \"study-outcome\", download = True)\n",
    "task_b = get_task(\"rel-trial\", \"site-success\", download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "\n",
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device\n",
    "                                       ] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))\n",
    "\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=device), batch_size=256\n",
    ")\n",
    "\n",
    "\n",
    "root_dir = \"/home/lingze/embedding_fusion/data\"\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n",
    "    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n",
    "    cache_dir=os.path.join(\n",
    "        root_dir, f\"rel-trial_materialized_cache\"\n",
    "    ),  # store materialized graph for convenience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loader_dict(task: BaseTask) -> dict:\n",
    "    loader_dict = {}\n",
    "    for split, table in [\n",
    "        (\"train\", task.get_table(\"train\")),\n",
    "        (\"val\",task.get_table(\"val\")),\n",
    "        (\"test\", task.get_table(\"test\")),\n",
    "    ]:\n",
    "        table_input = get_node_train_table_input(\n",
    "            table=table,\n",
    "            task=task,\n",
    "        )\n",
    "        loader_dict[split] = NeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=[\n",
    "                128 for i in range(2)\n",
    "            ],  # we sample subgraphs of depth 2, 128 neighbors per node.\n",
    "            time_attr=\"time\",\n",
    "            input_nodes=table_input.nodes,\n",
    "            input_time=table_input.time,\n",
    "            transform=table_input.transform,\n",
    "            batch_size=512,\n",
    "            temporal_strategy=\"uniform\",\n",
    "            shuffle=split == \"train\",\n",
    "            num_workers=0,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "    return loader_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taska_loader_dict = generate_loader_dict(task_a)\n",
    "taskb_loader_dict = generate_loader_dict(task_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid(loader: NeighborLoader, model: torch.nn.Module, task: BaseTask)-> np.ndarray:\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    pred_hat_list = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(\n",
    "            batch,\n",
    "            task.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "        pred_hat_list.append(batch[task.entity_table].y.detach().cpu())\n",
    "    return torch.cat(pred_list, dim=0), torch.cat(pred_hat_list, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, model: torch.nn.Module, task: BaseTask)-> np.ndarray:\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(\n",
    "            batch,\n",
    "            task.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "    return torch.cat(pred_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 128\n",
    "\n",
    "task_a_temporal_encoder = HeteroTemporalEncoder(\n",
    "    node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "            channels=channels,\n",
    ")\n",
    "\n",
    "task_a_feat_encoder = FeatureEncodingPart(\n",
    "    data=data,\n",
    "    node_to_col_stats=col_stats_dict,\n",
    "    channels=channels\n",
    ")\n",
    "\n",
    "\n",
    "task_a_node_encoder = NodeRepresentationPart(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    num_layers=1,\n",
    "    normalization=\"layer_norm\",\n",
    "    dropout_prob=0.2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "task_a_model =  CompositeModel(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    out_channels=1,\n",
    "    dropout=0.2,\n",
    "    aggr=\"mean\",\n",
    "    norm=\"layer_norm\",\n",
    "    num_layer=2,\n",
    "    feature_encoder=task_a_feat_encoder,\n",
    "    node_encoder=task_a_node_encoder,\n",
    "    temporal_encoder=task_a_temporal_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Task A\n",
    "optimizer = torch.optim.Adam(task_a_model.parameters(), lr=0.005)\n",
    "epochs = 15\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "tune_metric = \"auroc\"\n",
    "higher_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 0.6894868534068893, Val metrics: {'auroc': 0.4877367214828515, 'accuracy': 0.584375, 'precision': 0.584375, 'recall': 1.0, 'f1': 0.73767258382643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 0.6564565871046925, Val metrics: {'auroc': 0.5614973262032086, 'accuracy': 0.584375, 'precision': 0.584375, 'recall': 1.0, 'f1': 0.73767258382643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 0.6493301573586062, Val metrics: {'auroc': 0.5979386970099044, 'accuracy': 0.584375, 'precision': 0.584375, 'recall': 1.0, 'f1': 0.73767258382643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 0.634345735145128, Val metrics: {'auroc': 0.6277905101434513, 'accuracy': 0.584375, 'precision': 0.584375, 'recall': 1.0, 'f1': 0.73767258382643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:06<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 0.6210746712757783, Val metrics: {'auroc': 0.6499805663892351, 'accuracy': 0.5854166666666667, 'precision': 0.5849843587069864, 'recall': 1.0, 'f1': 0.7381578947368421}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 0.6072499076982568, Val metrics: {'auroc': 0.6390039269296235, 'accuracy': 0.5927083333333333, 'precision': 0.6693227091633466, 'recall': 0.5989304812834224, 'f1': 0.632173095014111}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 0.593892188286491, Val metrics: {'auroc': 0.6393702616612833, 'accuracy': 0.6114583333333333, 'precision': 0.6127098321342925, 'recall': 0.910873440285205, 'f1': 0.7326164874551971}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 0.5772828233029339, Val metrics: {'auroc': 0.6417156974432516, 'accuracy': 0.5989583333333334, 'precision': 0.6554770318021201, 'recall': 0.661319073083779, 'f1': 0.6583850931677019}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 0.5672043125709335, Val metrics: {'auroc': 0.6388341620539764, 'accuracy': 0.6270833333333333, 'precision': 0.6415620641562064, 'recall': 0.8199643493761141, 'f1': 0.7198748043818466}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.5455926545902314, Val metrics: {'auroc': 0.6450663199889206, 'accuracy': 0.6260416666666667, 'precision': 0.660828025477707, 'recall': 0.7397504456327986, 'f1': 0.6980656013456686}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 0.5298433202851985, Val metrics: {'auroc': 0.6338886431765689, 'accuracy': 0.6208333333333333, 'precision': 0.6347469220246238, 'recall': 0.8270944741532977, 'f1': 0.718266253869969}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 0.5176240227034395, Val metrics: {'auroc': 0.6308462779051014, 'accuracy': 0.5927083333333333, 'precision': 0.6585820895522388, 'recall': 0.6292335115864528, 'f1': 0.6435733819507748}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 0.4970846722150656, Val metrics: {'auroc': 0.6132577432887031, 'accuracy': 0.596875, 'precision': 0.6570397111913358, 'recall': 0.6488413547237076, 'f1': 0.6529147982062781}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 0.48224579991251904, Val metrics: {'auroc': 0.6213126398884913, 'accuracy': 0.621875, 'precision': 0.651840490797546, 'recall': 0.7575757575757576, 'f1': 0.7007419620774938}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 0.46138886097313586, Val metrics: {'auroc': 0.6300197910105031, 'accuracy': 0.5770833333333333, 'precision': 0.6565656565656566, 'recall': 0.5793226381461676, 'f1': 0.615530303030303}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = None\n",
    "best_val_metric = -math.inf if higher_is_better else math.inf\n",
    "task_a_model.to(device)\n",
    "best_epoch = 0\n",
    "task_a_model.reset_parameters()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    task_a_model.train()\n",
    "    loss_accum = count_accum = 0\n",
    "    for batch in tqdm(taska_loader_dict[\"train\"]):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = task_a_model(\n",
    "            batch,\n",
    "            task_a.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        loss = loss_fn(pred, batch[task_a.entity_table].y.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_accum += loss.detach().item() * pred.size(0)\n",
    "        count_accum += pred.size(0)\n",
    "    \n",
    "    train_loss = loss_accum / count_accum\n",
    "    val_logits = test(taska_loader_dict[\"val\"], task_a_model, task_a)\n",
    "    val_logits = torch.sigmoid(val_logits).numpy()\n",
    "    \n",
    "    val_pred = (val_logits > 0.5).astype(int)\n",
    "    val_pred_hat = task_a.get_table(\"val\").df[task_a.target_col].to_numpy()\n",
    "    val_metrics = {\n",
    "            \"auroc\": roc_auc_score(val_pred_hat, val_logits),\n",
    "        \"accuracy\": accuracy_score(val_pred_hat, val_pred),\n",
    "        \"precision\": precision_score(val_pred_hat, val_pred),\n",
    "        \"recall\": recall_score(val_pred_hat, val_pred),\n",
    "        \"f1\": f1_score(val_pred_hat, val_pred),\n",
    "    }\n",
    "    \n",
    "    print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
    "        not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
    "    ):\n",
    "        best_epoch = epoch\n",
    "        best_val_metric = val_metrics[tune_metric]\n",
    "        state_dict = copy.deepcopy(task_a_model.state_dict())\n",
    "\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': 0.6789013596793917,\n",
       " 'accuracy': 0.5878787878787879,\n",
       " 'precision': 0.5868772782503038,\n",
       " 'recall': 1.0,\n",
       " 'f1score': 0.7396630934150077}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test task A\n",
    "task_a_model.load_state_dict(state_dict)\n",
    "test_logits = test(taska_loader_dict[\"test\"], task_a_model, task_a)\n",
    "test_logits =  torch.sigmoid(test_logits).numpy()\n",
    "\n",
    "test_pred = (test_logits > 0.5).astype(int)\n",
    "test_pred_hat = task_a.get_table(\"test\", mask_input_cols = False).df[task_a.target_col].to_numpy()\n",
    "test_metrics = {\n",
    "    \"auroc\": roc_auc_score(test_pred_hat, test_logits),\n",
    "    \"accuracy\": accuracy_score(test_pred_hat, test_pred),\n",
    "    \"precision\": precision_score(test_pred_hat, test_pred),\n",
    "    \"recall\": recall_score(test_pred_hat, test_pred),\n",
    "    \"f1score\": f1_score(test_pred_hat, test_pred),\n",
    "}\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the individual model state\n",
    "torch.save(task_a_model.state_dict(), \"task_a_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Task B\n",
    "task_b_temporal_encoder = HeteroTemporalEncoder(\n",
    "    node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "            channels=channels,\n",
    ")\n",
    "\n",
    "task_b_feat_encoder = FeatureEncodingPart(\n",
    "    data=data,\n",
    "    node_to_col_stats=col_stats_dict,\n",
    "    channels=channels\n",
    ")\n",
    "\n",
    "\n",
    "task_b_node_encoder = NodeRepresentationPart(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    num_layers=1,\n",
    "    normalization=\"layer_norm\",\n",
    "    dropout_prob=0.4\n",
    ")\n",
    "\n",
    "task_b_model =  CompositeModel(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    out_channels=1,\n",
    "    dropout=0.3,\n",
    "    aggr=\"mean\",\n",
    "    norm=\"layer_norm\",\n",
    "    num_layer=2,\n",
    "    feature_encoder=task_b_feat_encoder,\n",
    "    node_encoder=task_b_node_encoder,\n",
    "    temporal_encoder=task_b_temporal_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(task_b_model.parameters(), lr=0.005)\n",
    "epochs = 20\n",
    "loss_fn = L1Loss()\n",
    "tune_metric = \"mae\"\n",
    "higher_is_better = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/296 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:41,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 0.4110280603170395, Val metrics: {'mae': 0.3936864982174104, 'r2': -0.30282323912695497, 'rmse': 0.5452460380940995}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:42,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 0.24289779141545295, Val metrics: {'mae': 0.384304029025163, 'r2': -0.2736703055791698, 'rmse': 0.5391111102901006}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:42,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 0.2302646704018116, Val metrics: {'mae': 0.3877429890546543, 'r2': -0.2695288294198446, 'rmse': 0.5382339077140139}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:41,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 0.22985964342951776, Val metrics: {'mae': 0.3908682909753259, 'r2': -0.33945272075606603, 'rmse': 0.5528578297369027}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:38,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 0.21786684431135656, Val metrics: {'mae': 0.39870456898865636, 'r2': -0.38730088352122327, 'rmse': 0.5626458263218033}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:41,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 0.2138837032020092, Val metrics: {'mae': 0.3794568005223825, 'r2': -0.3743733489618122, 'rmse': 0.5600181888565388}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:40,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 0.2111013986170292, Val metrics: {'mae': 0.4065624115894496, 'r2': -0.40869757010953034, 'rmse': 0.5669681373353582}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:39,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 0.20857503078877926, Val metrics: {'mae': 0.3900789748812933, 'r2': -0.38278624839372455, 'rmse': 0.5617295829243674}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:05<00:35,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 0.2060912225395441, Val metrics: {'mae': 0.4063927779075654, 'r2': -0.5193313758323912, 'rmse': 0.5888111474175233}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:05<00:35,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.20251142829656602, Val metrics: {'mae': 0.4020019833621722, 'r2': -0.4002314430996885, 'rmse': 0.5652618598386858}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:05<00:34,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 0.19478647522628306, Val metrics: {'mae': 0.3873272033693128, 'r2': -0.38843926023356556, 'rmse': 0.562876623971106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:39,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 0.1929924976080656, Val metrics: {'mae': 0.38693799886738184, 'r2': -0.3918509976059299, 'rmse': 0.5635677629355763}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:39,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 0.18783764094114302, Val metrics: {'mae': 0.39364775330928603, 'r2': -0.4563227805582635, 'rmse': 0.5764724962189511}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:38,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 0.18445986583828927, Val metrics: {'mae': 0.3983604497899226, 'r2': -0.4403371393483424, 'rmse': 0.5732998785991873}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:42,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 0.1852744035422802, Val metrics: {'mae': 0.40969337951042534, 'r2': -0.5093403171949973, 'rmse': 0.5868719555921155}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:05<00:38,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train loss: 0.18679688014090062, Val metrics: {'mae': 0.4025752679474265, 'r2': -0.4999514545499366, 'rmse': 0.5850437874459633}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:40,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 0.1825265321880579, Val metrics: {'mae': 0.4052837893479933, 'r2': -0.48206966643968485, 'rmse': 0.5815460088863766}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:38,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 0.1769079878926277, Val metrics: {'mae': 0.3983733381981597, 'r2': -0.4803401211491005, 'rmse': 0.5812065836234902}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:39,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 0.17650280594825746, Val metrics: {'mae': 0.4072830869679515, 'r2': -0.48362593122006525, 'rmse': 0.5818512584067168}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 40/296 [00:06<00:42,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train loss: 0.18459568172693253, Val metrics: {'mae': 0.3996612915887673, 'r2': -0.5034183352771859, 'rmse': 0.585719511441545}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = None\n",
    "best_val_metric = -math.inf if higher_is_better else math.inf\n",
    "task_b_model.to(device)\n",
    "best_epoch = 0\n",
    "early_stop = 40\n",
    "# train\n",
    "task_b_model.reset_parameters()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    task_b_model.train()\n",
    "    \n",
    "    cnt = 0\n",
    "    loss_accum = count_accum = 0\n",
    "    for batch in tqdm(taskb_loader_dict[\"train\"]):\n",
    "        cnt += 1\n",
    "        if cnt > early_stop:\n",
    "            break\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = task_b_model(\n",
    "            batch,\n",
    "            task_b.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        loss = loss_fn(pred, batch[task_b.entity_table].y.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_accum += loss.detach().item() * pred.size(0)\n",
    "        count_accum += pred.size(0)\n",
    "\n",
    "    train_loss = loss_accum / count_accum\n",
    "    val_pred_hat = task_b.get_table(\"val\").df[task_b.target_col].to_numpy()\n",
    "    val_logits = test(taskb_loader_dict[\"val\"], task_b_model, task_b)\n",
    "    val_logits = val_logits.numpy()\n",
    "    \n",
    "    val_metrics = {\n",
    "        \"mae\": mean_absolute_error(val_pred_hat, val_logits),\n",
    "        \"r2\": r2_score(val_pred_hat, val_logits),\n",
    "        \"rmse\": root_mean_squared_error(val_pred_hat, val_logits),\n",
    "    }\n",
    "    \n",
    "    print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "    \n",
    "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
    "        not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
    "    ):\n",
    "        best_epoch = epoch\n",
    "        best_val_metric = val_metrics[tune_metric]\n",
    "        state_dict = copy.deepcopy(task_b_model.state_dict())\n",
    "\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 0.40999596248485687,\n",
       " 'r2': -0.5214476645866393,\n",
       " 'rmse': 0.5935707603008625}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "task_b_model.load_state_dict(state_dict)\n",
    "logits = test(taskb_loader_dict[\"test\"], task_b_model, task_b)\n",
    "logits = logits.numpy()\n",
    "pred_hat = task_b.get_table(\"test\", mask_input_cols=False).df[task_b.target_col].to_numpy()\n",
    "test_metrics = {\n",
    "        \"mae\": mean_absolute_error(pred_hat, logits),\n",
    "        \"r2\": r2_score(pred_hat, logits),\n",
    "        \"rmse\": root_mean_squared_error(pred_hat, logits),\n",
    "}\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# save the individual model state\n",
    "torch.save(task_b_model.state_dict(), \"task_b_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
