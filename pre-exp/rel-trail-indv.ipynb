{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import relbench\n",
    "from relbench.base import Table, Database, Dataset, EntityTask\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "from relbench.base import BaseTask\n",
    "from torch_geometric.seed import seed_everything\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "from relbench.modeling.graph import get_node_train_table_input\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch_geometric\n",
    "import torch_frame\n",
    "\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from typing import List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch.nn import L1Loss\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-trial/db...\n",
      "Done in 8.15 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(name=\"rel-trial\", download=True)\n",
    "db = dataset.get_db()\n",
    "task_a = get_task(\"rel-trial\", \"study-outcome\", download = True)\n",
    "task_b = get_task(\"rel-trial\", \"site-success\", download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "\n",
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device\n",
    "                                       ] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))\n",
    "\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=device), batch_size=256\n",
    ")\n",
    "\n",
    "\n",
    "root_dir = \"/home/lingze/embedding_fusion/data\"\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n",
    "    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n",
    "    cache_dir=os.path.join(\n",
    "        root_dir, f\"rel-trial_materialized_cache\"\n",
    "    ),  # store materialized graph for convenience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loader_dict(task: BaseTask) -> dict:\n",
    "    loader_dict = {}\n",
    "    for split, table in [\n",
    "        (\"train\", task.get_table(\"train\")),\n",
    "        (\"val\",task.get_table(\"val\")),\n",
    "        (\"test\", task.get_table(\"test\")),\n",
    "    ]:\n",
    "        table_input = get_node_train_table_input(\n",
    "            table=table,\n",
    "            task=task,\n",
    "        )\n",
    "        loader_dict[split] = NeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=[\n",
    "                128 for i in range(2)\n",
    "            ],  # we sample subgraphs of depth 2, 128 neighbors per node.\n",
    "            time_attr=\"time\",\n",
    "            input_nodes=table_input.nodes,\n",
    "            input_time=table_input.time,\n",
    "            transform=table_input.transform,\n",
    "            batch_size=512,\n",
    "            temporal_strategy=\"uniform\",\n",
    "            shuffle=split == \"train\",\n",
    "            num_workers=0,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "    return loader_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taska_loader_dict = generate_loader_dict(task_a)\n",
    "taskb_loader_dict = generate_loader_dict(task_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the model\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import copy\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch_frame\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleDict\n",
    "from torch_frame.data.stats import StatType\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MLP\n",
    "from torch_frame.nn.models.resnet import FCResidualBlock\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_frame.nn.encoder import StypeWiseFeatureEncoder\n",
    "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncodingPart(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data:HeteroData,\n",
    "        node_to_col_stats: Dict[str, Dict[str, Dict[StatType, Tensor]]],\n",
    "        channels: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoders = torch.nn.ModuleDict()\n",
    "        # node_type : StypeWiseFeatureEncoder\n",
    "        \n",
    "        node_to_col_names_dict = {\n",
    "            node_type: data[node_type].tf.col_names_dict\n",
    "            for node_type in data.node_types\n",
    "        }\n",
    "        # node_type:  {stype: [col_name]}\n",
    "        \n",
    "        default_stype_encoder_cls_kwargs: Dict[torch_frame.stype, Any] = {\n",
    "            torch_frame.categorical: (torch_frame.nn.EmbeddingEncoder, {}),\n",
    "            torch_frame.numerical: (torch_frame.nn.LinearEncoder, {}),\n",
    "            torch_frame.multicategorical: (\n",
    "                torch_frame.nn.MultiCategoricalEmbeddingEncoder,\n",
    "                {},\n",
    "            ),\n",
    "            torch_frame.embedding: (torch_frame.nn.LinearEmbeddingEncoder, {}),\n",
    "            torch_frame.timestamp: (torch_frame.nn.TimestampEncoder, {}),\n",
    "        }\n",
    "                \n",
    "        for node_type in node_to_col_names_dict.keys():\n",
    "            stype_encoder_dict = {\n",
    "                stype: default_stype_encoder_cls_kwargs[stype][0](\n",
    "                    **default_stype_encoder_cls_kwargs[stype][1]\n",
    "                )\n",
    "                for stype in node_to_col_names_dict[node_type].keys()\n",
    "            }\n",
    "            self.encoders.update({node_type: StypeWiseFeatureEncoder(\n",
    "                out_channels=channels,\n",
    "                col_stats = node_to_col_stats[node_type],\n",
    "                col_names_dict=node_to_col_names_dict[node_type],\n",
    "                stype_encoder_dict=stype_encoder_dict\n",
    "            )})\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for encoder in self.encoders.values():\n",
    "            encoder.reset_parameters()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        tf_dict: Dict[NodeType, torch_frame.TensorFrame],\n",
    "    )-> Dict[NodeType, Tensor]:\n",
    "        x_dict = {}\n",
    "        for node_type, tf in tf_dict.items():\n",
    "            x, _ = self.encoders[node_type](tf)\n",
    "            x_dict[node_type] = x\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeRepresentationPart(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: HeteroData,\n",
    "        channels:int,\n",
    "        num_layers: int,\n",
    "        normalization: Optional[str] = \"layer_norm\",\n",
    "        dropout_prob: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mappers = torch.nn.ModuleDict()\n",
    "        \n",
    "        node_to_col_names_dict = {\n",
    "            node_type: data[node_type].tf.col_names_dict\n",
    "            for node_type in data.node_types\n",
    "        } \n",
    "        # node_type:  {stype: [col_name]}\n",
    "        \n",
    "        for node_type, type_to_col_names in node_to_col_names_dict.items():\n",
    "            col_cnt = 0\n",
    "            for cols in type_to_col_names.values():\n",
    "                col_cnt += len(cols)\n",
    "            in_channels = col_cnt * channels\n",
    "            backbone = torch.nn.Sequential(*[\n",
    "                FCResidualBlock(\n",
    "                    in_channels if i == 0 else channels,\n",
    "                    channels,\n",
    "                    normalization=normalization,\n",
    "                    dropout_prob=dropout_prob\n",
    "                )\n",
    "            for i in range(num_layers)], \n",
    "            torch.nn.LayerNorm(channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(channels, channels)\n",
    "            )\n",
    "            self.mappers.update(\n",
    "                {\n",
    "                    node_type: backbone\n",
    "                }\n",
    "            )\n",
    "    def reset_parameters(self):\n",
    "        for mapper in self.mappers.values():\n",
    "            for layer in mapper:\n",
    "                if hasattr(layer, \"reset_parameters\"):\n",
    "                    layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, \n",
    "                x_dict: Dict[NodeType, Tensor]\n",
    "        ) -> Dict[NodeType, Tensor]:\n",
    "        out_dict = {}\n",
    "        for node_type, x in x_dict.items():\n",
    "            # Flattening the encoder output\n",
    "            x = x.view(x.size(0), math.prod(x.shape[1:]))\n",
    "            out_dict[node_type] = self.mappers[node_type](x)\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: HeteroData,\n",
    "        channels: int,\n",
    "        out_channels:int,\n",
    "        aggr:str,\n",
    "        norm:str,\n",
    "        num_layer:int,\n",
    "        feature_encoder: torch.nn.Module,\n",
    "        node_encoder: torch.nn.Module,\n",
    "        temporal_encoder: torch.nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gnn = HeteroGraphSAGE(\n",
    "            node_types = data.node_types,\n",
    "            edge_types= data.edge_types,\n",
    "            channels=channels,\n",
    "            aggr = aggr,\n",
    "            num_layers=num_layer\n",
    "        )\n",
    "        \n",
    "        self.head = MLP(\n",
    "            channels,\n",
    "            out_channels=out_channels,\n",
    "            norm=norm,\n",
    "            num_layers=1\n",
    "        )\n",
    "        \n",
    "        self.feature_encoder = feature_encoder\n",
    "        self.node_encoder = node_encoder\n",
    "        self.temporal_encoder = temporal_encoder\n",
    "    \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.gnn.reset_parameters()\n",
    "        self.head.reset_parameters()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        batch: HeteroData,\n",
    "        entity_table: NodeType\n",
    "    ) -> Tensor:\n",
    "        seed_time = batch[entity_table].seed_time\n",
    "        x_dict = self.feature_encoder(batch.tf_dict)\n",
    "        x_dict = self.node_encoder(x_dict)\n",
    "        rel_time_dict = self.temporal_encoder(\n",
    "            seed_time, batch.time_dict, batch.batch_dict\n",
    "        )\n",
    "        \n",
    "        for node_type, rel_time in rel_time_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
    "        \n",
    "        x_dict = self.gnn(\n",
    "            x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch.num_sampled_nodes_dict,\n",
    "            batch.num_sampled_edges_dict,\n",
    "        )\n",
    "        return self.head(x_dict[entity_table][: seed_time.size(0)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid(loader: NeighborLoader, model: torch.nn.Module, task: BaseTask)-> np.ndarray:\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    pred_hat_list = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(\n",
    "            batch,\n",
    "            task.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "        pred_hat_list.append(batch[task.entity_table].y.detach().cpu())\n",
    "    return torch.cat(pred_list, dim=0), torch.cat(pred_hat_list, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, model: torch.nn.Module, task: BaseTask)-> np.ndarray:\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(\n",
    "            batch,\n",
    "            task.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "    return torch.cat(pred_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 128\n",
    "\n",
    "task_a_temporal_encoder = HeteroTemporalEncoder(\n",
    "    node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "            channels=channels,\n",
    ")\n",
    "\n",
    "task_a_feat_encoder = FeatureEncodingPart(\n",
    "    data=data,\n",
    "    node_to_col_stats=col_stats_dict,\n",
    "    channels=channels\n",
    ")\n",
    "\n",
    "\n",
    "task_a_node_encoder = NodeRepresentationPart(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    num_layers=1,\n",
    "    normalization=\"layer_norm\",\n",
    "    dropout_prob=0.2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "task_a_model =  CompositeModel(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    out_channels=1,\n",
    "    aggr=\"mean\",\n",
    "    norm=\"layer_norm\",\n",
    "    num_layer=2,\n",
    "    feature_encoder=task_a_feat_encoder,\n",
    "    node_encoder=task_a_node_encoder,\n",
    "    temporal_encoder=task_a_temporal_encoder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Task A\n",
    "optimizer = torch.optim.Adam(task_a_model.parameters(), lr=0.005)\n",
    "epochs = 15\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "tune_metric = \"auroc\"\n",
    "higher_is_better = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 0.6837852612244639, Val metrics: {'auroc': 0.5895911793744611, 'accuracy': 0.584375, 'precision': 0.584375, 'recall': 1.0, 'f1': 0.73767258382643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 0.6419359451336564, Val metrics: {'auroc': 0.6175353714053404, 'accuracy': 0.584375, 'precision': 0.584375, 'recall': 1.0, 'f1': 0.73767258382643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 0.618132569490919, Val metrics: {'auroc': 0.6466076063599283, 'accuracy': 0.628125, 'precision': 0.6588785046728972, 'recall': 0.7540106951871658, 'f1': 0.7032418952618454}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 0.6067917583305596, Val metrics: {'auroc': 0.655538132318318, 'accuracy': 0.63125, 'precision': 0.6396761133603239, 'recall': 0.8449197860962567, 'f1': 0.728110599078341}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 0.590242871467682, Val metrics: {'auroc': 0.6488815621942556, 'accuracy': 0.63125, 'precision': 0.6308470290771175, 'recall': 0.8894830659536542, 'f1': 0.7381656804733728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 0.5707895852574433, Val metrics: {'auroc': 0.6368371910167576, 'accuracy': 0.5947916666666667, 'precision': 0.6365079365079365, 'recall': 0.714795008912656, 'f1': 0.6733837111670865}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 0.5549517495047356, Val metrics: {'auroc': 0.631954217093536, 'accuracy': 0.6052083333333333, 'precision': 0.6307471264367817, 'recall': 0.7825311942959001, 'f1': 0.6984884645982498}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 0.5455251353292321, Val metrics: {'auroc': 0.641662087482521, 'accuracy': 0.5895833333333333, 'precision': 0.6602687140115163, 'recall': 0.6131907308377896, 'f1': 0.6358595194085028}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 0.526058482362446, Val metrics: {'auroc': 0.6344292102806035, 'accuracy': 0.61875, 'precision': 0.6644182124789207, 'recall': 0.7023172905525846, 'f1': 0.682842287694974}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.5063523848702436, Val metrics: {'auroc': 0.6382087125121182, 'accuracy': 0.6135416666666667, 'precision': 0.6655052264808362, 'recall': 0.6809269162210339, 'f1': 0.6731277533039648}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:04<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 0.48740576590043777, Val metrics: {'auroc': 0.6311321976956652, 'accuracy': 0.609375, 'precision': 0.6709558823529411, 'recall': 0.6506238859180036, 'f1': 0.6606334841628959}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 0.47294296029807925, Val metrics: {'auroc': 0.6230147561416911, 'accuracy': 0.5927083333333333, 'precision': 0.6349206349206349, 'recall': 0.7130124777183601, 'f1': 0.6717044500419815}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 0.4575477092936374, Val metrics: {'auroc': 0.6239886704282989, 'accuracy': 0.6125, 'precision': 0.6666666666666666, 'recall': 0.6737967914438503, 'f1': 0.6702127659574468}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 0.43261125400023837, Val metrics: {'auroc': 0.6264457936284562, 'accuracy': 0.60625, 'precision': 0.6414219474497682, 'recall': 0.7397504456327986, 'f1': 0.6870860927152318}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 0.42252855523439253, Val metrics: {'auroc': 0.6376458079244457, 'accuracy': 0.6145833333333334, 'precision': 0.6547811993517018, 'recall': 0.7201426024955436, 'f1': 0.6859083191850595}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = None\n",
    "best_val_metric = -math.inf if higher_is_better else math.inf\n",
    "task_a_model.to(device)\n",
    "best_epoch = 0\n",
    "task_a_model.reset_parameters()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    task_a_model.train()\n",
    "    loss_accum = count_accum = 0\n",
    "    for batch in tqdm(taska_loader_dict[\"train\"]):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = task_a_model(\n",
    "            batch,\n",
    "            task_a.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        loss = loss_fn(pred, batch[task_a.entity_table].y.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_accum += loss.detach().item() * pred.size(0)\n",
    "        count_accum += pred.size(0)\n",
    "    \n",
    "    train_loss = loss_accum / count_accum\n",
    "    val_logits = test(taska_loader_dict[\"val\"], task_a_model, task_a)\n",
    "    val_logits = torch.sigmoid(val_logits).numpy()\n",
    "    \n",
    "    val_pred = (val_logits > 0.5).astype(int)\n",
    "    val_pred_hat = task_a.get_table(\"val\").df[task_a.target_col].to_numpy()\n",
    "    val_metrics = {\n",
    "            \"auroc\": roc_auc_score(val_pred_hat, val_logits),\n",
    "        \"accuracy\": accuracy_score(val_pred_hat, val_pred),\n",
    "        \"precision\": precision_score(val_pred_hat, val_pred),\n",
    "        \"recall\": recall_score(val_pred_hat, val_pred),\n",
    "        \"f1\": f1_score(val_pred_hat, val_pred),\n",
    "    }\n",
    "    \n",
    "    print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
    "        not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
    "    ):\n",
    "        best_epoch = epoch\n",
    "        best_val_metric = val_metrics[tune_metric]\n",
    "        state_dict = copy.deepcopy(task_a_model.state_dict())\n",
    "\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': 0.7006465438959718,\n",
       " 'accuracy': 0.6472727272727272,\n",
       " 'precision': 0.6486068111455109,\n",
       " 'recall': 0.8674948240165632,\n",
       " 'f1score': 0.7422497785651019}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test task A\n",
    "task_a_model.load_state_dict(state_dict)\n",
    "test_logits = test(taska_loader_dict[\"test\"], task_a_model, task_a)\n",
    "test_logits =  torch.sigmoid(test_logits).numpy()\n",
    "\n",
    "test_pred = (test_logits > 0.5).astype(int)\n",
    "test_pred_hat = task_a.get_table(\"test\", mask_input_cols = False).df[task_a.target_col].to_numpy()\n",
    "test_metrics = {\n",
    "    \"auroc\": roc_auc_score(test_pred_hat, test_logits),\n",
    "    \"accuracy\": accuracy_score(test_pred_hat, test_pred),\n",
    "    \"precision\": precision_score(test_pred_hat, test_pred),\n",
    "    \"recall\": recall_score(test_pred_hat, test_pred),\n",
    "    \"f1score\": f1_score(test_pred_hat, test_pred),\n",
    "}\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_b_temporal_encoder = HeteroTemporalEncoder(\n",
    "    node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "            channels=channels,\n",
    ")\n",
    "\n",
    "task_b_feat_encoder = FeatureEncodingPart(\n",
    "    data=data,\n",
    "    node_to_col_stats=col_stats_dict,\n",
    "    channels=channels\n",
    ")\n",
    "\n",
    "\n",
    "task_b_node_encoder = NodeRepresentationPart(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    num_layers=1,\n",
    "    normalization=\"layer_norm\",\n",
    "    dropout_prob=0.3\n",
    ")\n",
    "\n",
    "task_b_model =  CompositeModel(\n",
    "    data=data,\n",
    "    channels=channels,\n",
    "    out_channels=1,\n",
    "    aggr=\"mean\",\n",
    "    norm=\"layer_norm\",\n",
    "    num_layer=2,\n",
    "    feature_encoder=task_b_feat_encoder,\n",
    "    node_encoder=task_b_node_encoder,\n",
    "    temporal_encoder=task_b_temporal_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(task_b_model.parameters(), lr=0.002)\n",
    "epochs = 20\n",
    "loss_fn = L1Loss()\n",
    "tune_metric = \"mae\"\n",
    "higher_is_better = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train loss: 0.41037669628858564, Val metrics: {'mae': 0.4475658469513964, 'r2': -0.4558490914968616, 'rmse': 0.5763787357807583}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:29,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train loss: 0.3218929409980774, Val metrics: {'mae': 0.43447137506095423, 'r2': -0.4276799351072853, 'rmse': 0.5707753357201678}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:31,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train loss: 0.28878341168165206, Val metrics: {'mae': 0.4268082009795004, 'r2': -0.3815311949320759, 'rmse': 0.5614746047235288}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:33,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train loss: 0.2742943085730076, Val metrics: {'mae': 0.42401254670573446, 'r2': -0.4522040582154665, 'rmse': 0.5756567391251746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train loss: 0.2792063236236572, Val metrics: {'mae': 0.4285064555773019, 'r2': -0.4072439802881451, 'rmse': 0.5666755437221486}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train loss: 0.25812099650502207, Val metrics: {'mae': 0.4335978127386829, 'r2': -0.47608721061830117, 'rmse': 0.5803711007978084}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:37,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train loss: 0.2523935079574585, Val metrics: {'mae': 0.42417674058638466, 'r2': -0.482422613477671, 'rmse': 0.581615250814505}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:33,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train loss: 0.24488811194896698, Val metrics: {'mae': 0.43002036074424815, 'r2': -0.5023935676136224, 'rmse': 0.5855198568549347}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:29,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Train loss: 0.25629867538809775, Val metrics: {'mae': 0.4144787055295133, 'r2': -0.4362478551514435, 'rmse': 0.5724854677116512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:31,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.24291283264756203, Val metrics: {'mae': 0.40582686573382387, 'r2': -0.41682869668256406, 'rmse': 0.5686020779534545}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:29,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 0.24397690892219542, Val metrics: {'mae': 0.41573333441555393, 'r2': -0.40494048115120607, 'rmse': 0.5662115619059531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:29,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 0.23071654587984086, Val metrics: {'mae': 0.40817946900933894, 'r2': -0.4379565315229117, 'rmse': 0.5728259039486684}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:29,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 0.22962623834609985, Val metrics: {'mae': 0.40849792874504287, 'r2': -0.3449821935057402, 'rmse': 0.5539977967526727}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:30,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 0.2282417193055153, Val metrics: {'mae': 0.4050899251179319, 'r2': -0.40803072336446844, 'rmse': 0.5668339262671477}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:31,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 0.22397611513733864, Val metrics: {'mae': 0.3990583078126809, 'r2': -0.3794551417348908, 'rmse': 0.5610525768339572}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train loss: 0.2209170401096344, Val metrics: {'mae': 0.4019329465990645, 'r2': -0.3392639645544897, 'rmse': 0.5528188738935781}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 0.21357216611504554, Val metrics: {'mae': 0.4003906602231768, 'r2': -0.36502269153562517, 'rmse': 0.5581098735253793}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 0.22007163912057875, Val metrics: {'mae': 0.41247513787363294, 'r2': -0.43537786880026563, 'rmse': 0.5723120540537806}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 0.2103615455329418, Val metrics: {'mae': 0.40700702387336735, 'r2': -0.4448564994177324, 'rmse': 0.574198598516727}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/296 [00:02<00:32,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train loss: 0.20522889345884324, Val metrics: {'mae': 0.4036564826480426, 'r2': -0.3692540406407825, 'rmse': 0.558974229278604}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = None\n",
    "best_val_metric = -math.inf if higher_is_better else math.inf\n",
    "task_b_model.to(device)\n",
    "best_epoch = 0\n",
    "early_stop = 20\n",
    "# train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    task_b_model.train()\n",
    "    \n",
    "    cnt = 0\n",
    "    loss_accum = count_accum = 0\n",
    "    for batch in tqdm(taskb_loader_dict[\"train\"]):\n",
    "        cnt += 1\n",
    "        if cnt > early_stop:\n",
    "            break\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = task_b_model(\n",
    "            batch,\n",
    "            task_b.entity_table,\n",
    "        )\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        loss = loss_fn(pred, batch[task_b.entity_table].y.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_accum += loss.detach().item() * pred.size(0)\n",
    "        count_accum += pred.size(0)\n",
    "\n",
    "    train_loss = loss_accum / count_accum\n",
    "    val_pred_hat = task_b.get_table(\"val\").df[task_b.target_col].to_numpy()\n",
    "    val_logits = test(taskb_loader_dict[\"val\"], task_b_model, task_b)\n",
    "    val_logits = val_logits.numpy()\n",
    "    \n",
    "    val_metrics = {\n",
    "        \"mae\": mean_absolute_error(val_pred_hat, val_logits),\n",
    "        \"r2\": r2_score(val_pred_hat, val_logits),\n",
    "        \"rmse\": root_mean_squared_error(val_pred_hat, val_logits),\n",
    "    }\n",
    "    \n",
    "    print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
    "\n",
    "    \n",
    "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
    "        not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
    "    ):\n",
    "        best_epoch = epoch\n",
    "        best_val_metric = val_metrics[tune_metric]\n",
    "        state_dict = copy.deepcopy(task_b_model.state_dict())\n",
    "\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 0.4027192653319429,\n",
       " 'r2': -0.43432623996859543,\n",
       " 'rmse': 0.5763256675733064}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# test\n",
    "task_b_model.load_state_dict(state_dict)\n",
    "logits = test(taskb_loader_dict[\"test\"], task_b_model, task_b)\n",
    "logits = logits.numpy()\n",
    "pred_hat = task_b.get_table(\"test\", mask_input_cols=False).df[task_b.target_col].to_numpy()\n",
    "test_metrics = {\n",
    "        \"mae\": mean_absolute_error(pred_hat, logits),\n",
    "        \"r2\": r2_score(pred_hat, logits),\n",
    "        \"rmse\": root_mean_squared_error(pred_hat, logits),\n",
    "}\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
