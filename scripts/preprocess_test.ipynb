{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from relbench.datasets import get_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-trial/db...\n",
      "Done in 7.65 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(name=\"rel-trial\", download = True)\n",
    "db = dataset.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table interventions_studies -> table studies has 171771 edges\n",
      "table interventions_studies -> table interventions has 171771 edges\n",
      "table facilities_studies -> table studies has 1798765 edges\n",
      "table facilities_studies -> table facilities has 1798765 edges\n",
      "table eligibilities -> table studies has 249730 edges\n",
      "table reported_event_totals -> table studies has 383064 edges\n",
      "table designs -> table studies has 249093 edges\n",
      "table conditions_studies -> table studies has 408422 edges\n",
      "table conditions_studies -> table conditions has 408422 edges\n",
      "table drop_withdrawals -> table studies has 381199 edges\n",
      "table outcome_analyses -> table studies has 225846 edges\n",
      "table outcome_analyses -> table outcomes has 225846 edges\n",
      "table sponsors_studies -> table studies has 391462 edges\n",
      "table sponsors_studies -> table sponsors has 391462 edges\n",
      "table outcomes -> table studies has 411933 edges\n"
     ]
    }
   ],
   "source": [
    "# homoGraph\n",
    "from utils.builder import HomoGraph, make_homograph_from_db\n",
    "homoGraph = make_homograph_from_db(db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import infer_type_in_db\n",
    "from utils.tokenize import tokenize_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rule 0]: interventionsInferred intervention_id from numerical as categorical\n",
      "[rule 0]: interventions_studiesInferred id from numerical as categorical\n",
      "[rule 0]: interventions_studiesInferred nct_id from numerical as categorical\n",
      "[rule 0]: interventions_studiesInferred intervention_id from numerical as categorical\n",
      "[rule 0]: facilities_studiesInferred id from numerical as categorical\n",
      "[rule 0]: facilities_studiesInferred nct_id from numerical as categorical\n",
      "[rule 0]: facilities_studiesInferred facility_id from numerical as categorical\n",
      "[rule 0]: sponsorsInferred sponsor_id from numerical as categorical\n",
      "[rule 0]: eligibilitiesInferred id from numerical as categorical\n",
      "[rule 0]: eligibilitiesInferred nct_id from numerical as categorical\n",
      "[rule 0]: reported_event_totalsInferred id from numerical as categorical\n",
      "[rule 0]: reported_event_totalsInferred nct_id from numerical as categorical\n",
      "[rule 0]: designsInferred id from numerical as categorical\n",
      "[rule 0]: designsInferred nct_id from numerical as categorical\n",
      "[rule 0]: conditions_studiesInferred id from numerical as categorical\n",
      "[rule 0]: conditions_studiesInferred nct_id from numerical as categorical\n",
      "[rule 0]: conditions_studiesInferred condition_id from numerical as categorical\n",
      "[rule 0]: drop_withdrawalsInferred id from numerical as categorical\n",
      "[rule 0]: drop_withdrawalsInferred nct_id from numerical as categorical\n",
      "[rule 0]: studiesInferred nct_id from numerical as categorical\n",
      "[rule 0]: studiesInferred study_type from text_embedded as categorical\n",
      "[rule 1]: studiesInferred number_of_arms from numerical as categorical\n",
      "[rule 1]: studiesInferred number_of_groups from numerical as categorical\n",
      "[rule 0]: studiesInferred baseline_type_units_analyzed from text_embedded as categorical\n",
      "[rule 0]: outcome_analysesInferred id from numerical as categorical\n",
      "[rule 0]: outcome_analysesInferred nct_id from numerical as categorical\n",
      "[rule 0]: outcome_analysesInferred outcome_id from numerical as categorical\n",
      "[rule 0]: outcome_analysesInferred param_type from text_embedded as categorical\n",
      "[rule 1]: outcome_analysesInferred p_value_modifier from text_embedded as categorical\n",
      "[rule 1]: outcome_analysesInferred ci_percent from numerical as categorical\n",
      "[rule 0]: sponsors_studiesInferred id from numerical as categorical\n",
      "[rule 0]: sponsors_studiesInferred nct_id from numerical as categorical\n",
      "[rule 0]: sponsors_studiesInferred sponsor_id from numerical as categorical\n",
      "[rule 0]: outcomesInferred id from numerical as categorical\n",
      "[rule 0]: outcomesInferred nct_id from numerical as categorical\n",
      "[rule 0]: outcomesInferred dispersion_type from text_embedded as categorical\n",
      "[rule 0]: conditionsInferred condition_id from numerical as categorical\n",
      "[rule 0]: facilitiesInferred facility_id from numerical as categorical\n"
     ]
    }
   ],
   "source": [
    "col_type_dict = infer_type_in_db(db, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------> Tokenizing interventions each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/interventions.npy\n",
      "----------------> Tokenizing interventions_studies each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/interventions_studies.npy\n",
      "----------------> Tokenizing facilities_studies each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/facilities_studies.npy\n",
      "----------------> Tokenizing sponsors each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/sponsors.npy\n",
      "----------------> Tokenizing eligibilities each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/eligibilities.npy\n",
      "----------------> Tokenizing reported_event_totals each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/reported_event_totals.npy\n",
      "----------------> Tokenizing designs each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/designs.npy\n",
      "----------------> Tokenizing conditions_studies each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/conditions_studies.npy\n",
      "----------------> Tokenizing drop_withdrawals each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/drop_withdrawals.npy\n",
      "----------------> Tokenizing studies each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/studies.npy\n",
      "----------------> Tokenizing outcome_analyses each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/outcome_analyses.npy\n",
      "----------------> Tokenizing sponsors_studies each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/sponsors_studies.npy\n",
      "----------------> Tokenizing outcomes each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/outcomes.npy\n",
      "----------------> Tokenizing conditions each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/conditions.npy\n",
      "----------------> Tokenizing facilities each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-trails/facilities.npy\n"
     ]
    }
   ],
   "source": [
    "tk_db = tokenize_database(db, col_type_dict, './tmp_docs/rel-trails', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.document import generate_document_given_table\n",
    "from utils.builder import identify_entity_table\n",
    "from utils.builder import generate_hop_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interventions',\n",
       " 'sponsors',\n",
       " 'eligibilities',\n",
       " 'designs',\n",
       " 'studies',\n",
       " 'conditions']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_tables = identify_entity_table(db)\n",
    "entity_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table interventions - shape torch.Size([3462, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table sponsors - shape torch.Size([53241, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table eligibilities - shape torch.Size([249730, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table designs - shape torch.Size([249093, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table studies - shape torch.Size([249730, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table conditions - shape torch.Size([3973, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "# generated the documents and build the retrieval index\n",
    "entity_to_docs = {}\n",
    "walk_length = 10\n",
    "round = 8\n",
    "for entity in entity_tables:\n",
    "   _, entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round, \n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    }
   ],
   "source": [
    "# temporarily save the index\n",
    "import bm25s\n",
    "entity_to_retriver = {}\n",
    "for entity, docs in entity_to_docs.items():\n",
    "    retriever = bm25s.BM25(backend=\"numba\")\n",
    "    retriever.index(docs)\n",
    "    retriever.activate_numba_scorer()\n",
    "    entity_to_retriver[entity] = retriever\n",
    "\n",
    "# save the retriever\n",
    "for entity, retriever in entity_to_retriver.items():\n",
    "    retriever.save(f\"./tmp/{entity}_retriever_bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ./tmp/interventions_retriever_bm25\n",
      "load ./tmp/sponsors_retriever_bm25\n",
      "load ./tmp/eligibilities_retriever_bm25\n",
      "load ./tmp/designs_retriever_bm25\n",
      "load ./tmp/studies_retriever_bm25\n",
      "load ./tmp/conditions_retriever_bm25\n"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "entity_to_retriver = {}\n",
    "\n",
    "# save the retriever\n",
    "entity_to_retriver = {}\n",
    "for entity in entity_tables:\n",
    "    path = f\"./tmp/{entity}_retriever_bm25\"\n",
    "    retriever = bm25s.BM25.load(path)\n",
    "    retriever.activate_numba_scorer()\n",
    "    entity_to_retriver[entity] = retriever\n",
    "    print(f\"load {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the candidate docs, and retrieve the related docs in the bm25 retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table interventions - shape torch.Size([3462, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table sponsors - shape torch.Size([5324, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table eligibilities - shape torch.Size([24973, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table designs - shape torch.Size([24909, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table studies - shape torch.Size([24973, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table conditions - shape torch.Size([3973, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "# generated the documents and build the retrieval index\n",
    "entity_to_docs = {}\n",
    "walk_length = 10\n",
    "round = 8\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n // 10\n",
    "    sample_size = max(sample_size, 4096)\n",
    "    entity_candidate_pkys[entity], entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interventions', 'sponsors'),\n",
       " ('interventions', 'eligibilities'),\n",
       " ('interventions', 'designs'),\n",
       " ('interventions', 'studies'),\n",
       " ('interventions', 'conditions'),\n",
       " ('sponsors', 'interventions'),\n",
       " ('sponsors', 'eligibilities'),\n",
       " ('sponsors', 'designs'),\n",
       " ('sponsors', 'studies'),\n",
       " ('sponsors', 'conditions'),\n",
       " ('eligibilities', 'interventions'),\n",
       " ('eligibilities', 'sponsors'),\n",
       " ('eligibilities', 'designs'),\n",
       " ('eligibilities', 'conditions'),\n",
       " ('designs', 'interventions'),\n",
       " ('designs', 'sponsors'),\n",
       " ('designs', 'eligibilities'),\n",
       " ('designs', 'conditions'),\n",
       " ('studies', 'interventions'),\n",
       " ('studies', 'sponsors'),\n",
       " ('studies', 'conditions'),\n",
       " ('conditions', 'interventions'),\n",
       " ('conditions', 'sponsors'),\n",
       " ('conditions', 'eligibilities'),\n",
       " ('conditions', 'designs'),\n",
       " ('conditions', 'studies')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the cross-table edges,\n",
    "# first we want to find the multi-hop entity pairs\n",
    "hop_matrix = generate_hop_matrix(db)\n",
    "edge_candidates_pairs = []\n",
    "for entity in entity_tables:\n",
    "    for entity2 in entity_tables:\n",
    "        if entity == entity2:\n",
    "            continue\n",
    "        \n",
    "        if entity2 not in hop_matrix.graph[entity]:\n",
    "            # not one hop\n",
    "            edge_candidates_pairs.append((entity, entity2))\n",
    "edge_candidates_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('interventions', 'sponsors')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_candidates_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add cross table edges #2378 between interventions and sponsors\n",
      "Add cross table edges #2764 between interventions and eligibilities\n",
      "Add cross table edges #2792 between interventions and designs\n",
      "Add cross table edges #2810 between interventions and studies\n",
      "Add cross table edges #2445 between interventions and conditions\n",
      "Add cross table edges #2736 between sponsors and interventions\n",
      "Add cross table edges #5050 between sponsors and eligibilities\n",
      "Add cross table edges #5098 between sponsors and designs\n",
      "Add cross table edges #5029 between sponsors and studies\n",
      "Add cross table edges #2947 between sponsors and conditions\n",
      "Add cross table edges #17213 between eligibilities and interventions\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "topn = 20\n",
    "edge_dict = {}\n",
    "# (src_table, des_table) -> edge 2-D array\n",
    "for entity, retrieve_entity in edge_candidates_pairs:\n",
    "\n",
    "    # retrieve the related docs\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    retriever = entity_to_retriver[retrieve_entity]\n",
    "    \n",
    "    related_pkys, scores = retriever.retrieve(entity_query_docs, k = topn, n_threads = 24)\n",
    "    \n",
    "    score_np = np.array(scores)\n",
    "    related_pkys_np = np.array(related_pkys)\n",
    "    threshold = score_np.mean() + 2*scores.std()\n",
    "    \n",
    "    # Get indices where the score is above the threshold\n",
    "    mask = score_np > threshold\n",
    "\n",
    "    # Apply the mask\n",
    "    filtered_cols = related_pkys_np[mask]\n",
    "\n",
    "    # Generate the corresponding query entities\n",
    "    entity_query_pkys = np.array(entity_query_pkys)  # shape [n]\n",
    "\n",
    "    # Repeat each query item the number of True values per row in the mask\n",
    "    row_repeats = mask.sum(axis=1)  # how many times to repeat each query\n",
    "    filtered_rows = np.repeat(entity_query_pkys, row_repeats)\n",
    "    \n",
    "    \n",
    "    filtered_edge = np.stack([filtered_rows, filtered_cols], axis=1)\n",
    "    # added edge\n",
    "    num_edges = filtered_rows.shape[0]\n",
    "    edge_dict[(entity, retrieve_entity)] = filtered_edge\n",
    "    print(f\"Add cross table edges #{num_edges} between {entity} and {retrieve_entity}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {}\n",
    "# (src_table, des_table) -> edge 2-D array\n",
    "npz_data = {\n",
    "    f\"{src}-{dst}\": edge_array\n",
    "    for (src, dst), edge_array in edge_dict.items()\n",
    "}\n",
    "\n",
    "path = f\"./edges/rel-trail-edges.npz\"\n",
    "np.savez(path, **npz_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self entity correlation, which generate positive pairs in the constrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table interventions - shape torch.Size([3462, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table sponsors - shape torch.Size([10648, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table eligibilities - shape torch.Size([49946, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table designs - shape torch.Size([49818, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table studies - shape torch.Size([49946, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table conditions - shape torch.Size([3973, 8, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "# self-entity correlation\n",
    "# which can generate the positive pairs in the contrastive learning\n",
    "# generated the documents and build the retrieval index\n",
    "entity_to_docs = {}\n",
    "walk_length = 10\n",
    "round = 8\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n // 5\n",
    "    sample_size = max(sample_size, 4096)\n",
    "    pkys , docs = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )\n",
    "    entity_candidate_pkys[entity] = pkys\n",
    "    entity_to_docs[entity] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> interventions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #787, original candidate 3462 in interventions table\n",
      "--------> sponsors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #3942, original candidate 10648 in sponsors table\n",
      "--------> eligibilities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [07:29<00:00,  9.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #2491, original candidate 49946 in eligibilities table\n",
      "--------> designs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [06:36<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #2980, original candidate 49818 in designs table\n",
      "--------> studies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [06:54<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #6019, original candidate 49946 in studies table\n",
      "--------> conditions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #1299, original candidate 3973 in conditions table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topn = 21\n",
    "# the most related doc should be itself, so we need to retrieve topn + 1\n",
    "positive_pool_dict = {}\n",
    "# entity -> positive candidate, padding the non-value\n",
    "threshold = 0.7\n",
    "batch_size = 1024\n",
    "for entity, retriever in entity_to_retriver.items():\n",
    "    # retrieve the related docs\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    score_np = []\n",
    "    related_pkys_np = []\n",
    "    print(f\"--------> {entity}\")\n",
    "    for batch_idx in tqdm(range(0, len(entity_query_docs), batch_size)):\n",
    "        batch_query_docs = entity_query_docs[batch_idx:batch_idx + batch_size]\n",
    "        related_pkys, scores = retriever.retrieve(batch_query_docs, k = topn, n_threads=-1)\n",
    "        score_np.append(np.array(scores))\n",
    "        related_pkys_np.append(np.array(related_pkys))\n",
    "    \n",
    "    score_np = np.concatenate(score_np, axis = 0)\n",
    "    related_pkys_np = np.concatenate(related_pkys_np, axis = 0)\n",
    "    # Get indices where the score is above the threshold\n",
    "    # the first one is the most related one, should be itself\n",
    "    mask = score_np > (score_np[:,[0]] * threshold)\n",
    "    # add padding for those non-related docs which is filtered out.\n",
    "    related_pkys_np[~mask] = -1\n",
    "    rows_num = np.sum(mask, axis = 1)\n",
    "    # except itself, still has similar docs\n",
    "    rows_mask = rows_num > 1\n",
    "    positive_pool = related_pkys_np[rows_mask]\n",
    "    \n",
    "    positive_pool_dict[entity] = positive_pool\n",
    "    print(f\"Generate positive pools #{len(positive_pool)}, original candidate {len(entity_query_docs)} in {entity} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(path, **positive_pool_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
