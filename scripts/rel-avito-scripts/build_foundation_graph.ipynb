{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.base import Table\n",
    "from tqdm import tqdm\n",
    "from typing import Any,Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import Tensor\n",
    "from torch_frame import stype\n",
    "from torch_frame.config import TextEmbedderConfig\n",
    "from torch_frame.data import Dataset\n",
    "from torch_frame.data.stats import StatType\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_geometric.utils import sort_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-avito/db...\n",
      "Done in 4.74 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(name = \"rel-avito\", download = True)\n",
    "db = dataset.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rule 0]: PhoneRequestsStream Inferred UserID from numerical as categorical\n",
      "[rule 0]: PhoneRequestsStream Inferred IPID from numerical as categorical\n",
      "[rule 0]: PhoneRequestsStream Inferred AdID from numerical as categorical\n",
      "[rule 0]: Location Inferred LocationID from numerical as categorical\n",
      "[rule 1]: Location Inferred Level from numerical as categorical\n",
      "[rule 0]: Location Inferred RegionID from numerical as categorical\n",
      "[rule 0]: Location Inferred CityID from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred UserID from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred SearchID from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred IPID from numerical as categorical\n",
      "[rule 1]: SearchInfo Inferred IsUserLoggedOn from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred LocationID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserAgentID from numerical as categorical\n",
      "[rule 1]: UserInfo Inferred UserAgentOSID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserDeviceID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserAgentFamilyID from numerical as categorical\n",
      "[rule 0]: SearchStream Inferred SearchID from numerical as categorical\n",
      "[rule 0]: SearchStream Inferred AdID from numerical as categorical\n",
      "[rule 1]: SearchStream Inferred Position from numerical as categorical\n",
      "[rule 0]: SearchStream Inferred ObjectType from numerical as categorical\n",
      "[rule 0]: VisitStream Inferred UserID from numerical as categorical\n",
      "[rule 0]: VisitStream Inferred IPID from numerical as categorical\n",
      "[rule 0]: VisitStream Inferred AdID from numerical as categorical\n",
      "[rule 0]: AdsInfo Inferred AdID from numerical as categorical\n",
      "[rule 0]: AdsInfo Inferred LocationID from numerical as categorical\n",
      "[rule 1]: AdsInfo Inferred IsContext from numerical as categorical\n",
      "[rule 0]: Category Inferred CategoryID from numerical as categorical\n",
      "[rule 0]: Category Inferred ParentCategoryID from numerical as categorical\n",
      "[rule 0]: Category Inferred SubcategoryID from numerical as categorical\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import infer_type_in_db\n",
    "col_type_dict = infer_type_in_db(db, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_name: PhoneRequestsStream, pkey:None\n",
      "tablename: PhoneRequestsStream, fk: UserID\n",
      "tablename: PhoneRequestsStream, fk: AdID\n",
      "table_name: Location, pkey:LocationID\n",
      "table_name: SearchInfo, pkey:SearchID\n",
      "tablename: SearchInfo, fk: UserID\n",
      "tablename: SearchInfo, fk: LocationID\n",
      "tablename: SearchInfo, fk: CategoryID\n",
      "table_name: UserInfo, pkey:UserID\n",
      "table_name: SearchStream, pkey:None\n",
      "tablename: SearchStream, fk: SearchID\n",
      "tablename: SearchStream, fk: AdID\n",
      "table_name: VisitStream, pkey:None\n",
      "tablename: VisitStream, fk: UserID\n",
      "tablename: VisitStream, fk: AdID\n",
      "table_name: AdsInfo, pkey:AdID\n",
      "tablename: AdsInfo, fk: LocationID\n",
      "tablename: AdsInfo, fk: CategoryID\n",
      "table_name: Category, pkey:CategoryID\n"
     ]
    }
   ],
   "source": [
    "for table_name, table in db.table_dict.items():\n",
    "    print(f'table_name: {table_name}, pkey:{table.pkey_col}')\n",
    "    for fk in table.fkey_col_to_pkey_table.keys():\n",
    "        print(f'tablename: {table_name}, fk: {fk}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PhoneRequestsStream': {'UserID': <stype.categorical: 'categorical'>,\n",
       "  'IPID': <stype.categorical: 'categorical'>,\n",
       "  'AdID': <stype.categorical: 'categorical'>,\n",
       "  'PhoneRequestDate': <stype.timestamp: 'timestamp'>},\n",
       " 'Location': {'LocationID': <stype.categorical: 'categorical'>,\n",
       "  'Level': <stype.categorical: 'categorical'>,\n",
       "  'RegionID': <stype.categorical: 'categorical'>,\n",
       "  'CityID': <stype.categorical: 'categorical'>},\n",
       " 'SearchInfo': {'UserID': <stype.categorical: 'categorical'>,\n",
       "  'SearchID': <stype.categorical: 'categorical'>,\n",
       "  'SearchDate': <stype.timestamp: 'timestamp'>,\n",
       "  'IPID': <stype.categorical: 'categorical'>,\n",
       "  'IsUserLoggedOn': <stype.categorical: 'categorical'>,\n",
       "  'SearchQuery': <stype.text_embedded: 'text_embedded'>,\n",
       "  'LocationID': <stype.categorical: 'categorical'>,\n",
       "  'CategoryID': <stype.categorical: 'categorical'>},\n",
       " 'UserInfo': {'UserID': <stype.categorical: 'categorical'>,\n",
       "  'UserAgentID': <stype.categorical: 'categorical'>,\n",
       "  'UserAgentOSID': <stype.categorical: 'categorical'>,\n",
       "  'UserDeviceID': <stype.categorical: 'categorical'>,\n",
       "  'UserAgentFamilyID': <stype.categorical: 'categorical'>},\n",
       " 'SearchStream': {'SearchID': <stype.categorical: 'categorical'>,\n",
       "  'AdID': <stype.categorical: 'categorical'>,\n",
       "  'Position': <stype.categorical: 'categorical'>,\n",
       "  'ObjectType': <stype.categorical: 'categorical'>,\n",
       "  'HistCTR': <stype.numerical: 'numerical'>,\n",
       "  'IsClick': <stype.categorical: 'categorical'>,\n",
       "  'SearchDate': <stype.timestamp: 'timestamp'>},\n",
       " 'VisitStream': {'UserID': <stype.categorical: 'categorical'>,\n",
       "  'IPID': <stype.categorical: 'categorical'>,\n",
       "  'AdID': <stype.categorical: 'categorical'>,\n",
       "  'ViewDate': <stype.timestamp: 'timestamp'>},\n",
       " 'AdsInfo': {'AdID': <stype.categorical: 'categorical'>,\n",
       "  'LocationID': <stype.categorical: 'categorical'>,\n",
       "  'CategoryID': <stype.categorical: 'categorical'>,\n",
       "  'Price': <stype.numerical: 'numerical'>,\n",
       "  'Title': <stype.text_embedded: 'text_embedded'>,\n",
       "  'IsContext': <stype.categorical: 'categorical'>},\n",
       " 'Category': {'CategoryID': <stype.categorical: 'categorical'>,\n",
       "  'Level': <stype.numerical: 'numerical'>,\n",
       "  'ParentCategoryID': <stype.categorical: 'categorical'>,\n",
       "  'SubcategoryID': <stype.categorical: 'categorical'>}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchInfo.SearchQuery: text_embedded, unique values: 13210/1987156 Nan Value: 1682279/1987156\n",
      "AdsInfo.Title: text_embedded, unique values: 3246389/5960558 Nan Value: 0/5960558\n"
     ]
    }
   ],
   "source": [
    "# print text embedding type, profile\n",
    "for table, type_dict in col_type_dict.items():\n",
    "    for col_name, stype in type_dict.items():\n",
    "        if stype == stype.text_embedded:\n",
    "            unique_value = db.table_dict[table].df[col_name].unique()\n",
    "            n = len(unique_value)\n",
    "            nm = len(db.table_dict[table].df)\n",
    "            nan_num = db.table_dict[table].df[col_name].isnull().sum()\n",
    "            print(f\"{table}.{col_name}: {stype}, unique values: {n}/{nm} Nan Value: {nan_num}/{nm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device\n",
    "                                       ] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"all-MiniLM-L12-v2\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))\n",
    "\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=device), batch_size=512\n",
    ")\n",
    "\n",
    "def remove_pkey_fkey(col_to_stype: Dict[str, Any], table:Table) -> dict:\n",
    "    r\"\"\"Remove pkey, fkey columns since they will not be used as input feature.\"\"\"\n",
    "    if table.pkey_col is not None:\n",
    "        if table.pkey_col in col_to_stype:\n",
    "            col_to_stype.pop(table.pkey_col)\n",
    "    for fkey in table.fkey_col_to_pkey_table.keys():\n",
    "        if fkey in col_to_stype:\n",
    "            col_to_stype.pop(fkey)\n",
    "\n",
    "def to_unix_time(ser: pd.Series) -> np.ndarray:\n",
    "    r\"\"\"Converts a :class:`pandas.Timestamp` series to UNIX timestamp (in seconds).\"\"\"\n",
    "    assert ser.dtype in [np.dtype(\"datetime64[s]\"), np.dtype(\"datetime64[ns]\")]\n",
    "    unix_time = ser.astype(\"int64\").values\n",
    "    if ser.dtype == np.dtype(\"datetime64[ns]\"):\n",
    "        unix_time //= 10**9\n",
    "    return unix_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the table, concatenate the columns which is text type\n",
    "#        /--- text_col_1 ---/ --- text_col_2 --- / --- text_col_3 --- / \n",
    "# row 1  /------- A   -----/ ------- B   -----  / -----   C  ------- /\n",
    "# -------> Generate a new TexT column\n",
    "# \"text_col_1 is A, text_col_2 is B, text_col_3 is C\"\n",
    "\n",
    "# Therefore, we only need to convert this text column to vector \n",
    "# and drop the original text columns\n",
    "# for saving memory and computation \n",
    "\n",
    "for table_name, type_dict in col_type_dict.items():\n",
    "    # collect the text columns\n",
    "    text_cols = [ col for col, stype in type_dict.items() if stype == stype.text_embedded]\n",
    "    compress_cols = []\n",
    "    # for long text, we still keep it as one column\n",
    "    for col in text_cols:\n",
    "        avg_word_count = db.table_dict[table_name].df[col].dropna().apply(lambda x: len(str(x).split())).mean()\n",
    "        if avg_word_count < 128: # a half of default max length of BERT Max length （256）\n",
    "            # remove the long text cols\n",
    "            compress_cols.append(col)\n",
    "          \n",
    "    \n",
    "    if len(compress_cols) <= 1:\n",
    "        # if only one text column, we do not need to compress\n",
    "        continue\n",
    "    \n",
    "    print(f\"----> Compressing {table_name} text columns: {compress_cols}\")\n",
    "    \n",
    "    df = db.table_dict[table_name].df\n",
    "    compress_text_df = df[compress_cols]\n",
    "    \n",
    "    def row_to_text(row):\n",
    "        if row.isna().all():\n",
    "            return None\n",
    "        tokens = [f\"{key} is {value}\" for key, value in row.dropna().items()]\n",
    "        return \", \".join(tokens)\n",
    "\n",
    "    text_list = compress_text_df.apply(row_to_text, axis=1).tolist()\n",
    "    \n",
    "    # drop the compressed columns\n",
    "    df.drop(columns=compress_cols, inplace=True)\n",
    "    df[\"text_compress\"] = text_list\n",
    "    \n",
    "    # update the type dict\n",
    "    for col in compress_cols:\n",
    "        type_dict.pop(col)\n",
    "    type_dict[\"text_compress\"] = stype.text_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no multi-column text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start build graph\n",
    "cache_dir = \"./data/rel-avito-tensor-frame\"\n",
    "if cache_dir is not None:\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "data = HeteroData()\n",
    "col_stats_dict = {}\n",
    "for table_name, table in db.table_dict.items():\n",
    "    df = table.df\n",
    "    # (important for foreignKey value) Ensure the pkey is consecutive\n",
    "    if table.pkey_col is not None:\n",
    "        assert (df[table.pkey_col].values == np.arange(len(df))).all()\n",
    "    \n",
    "    col_to_stype = col_type_dict[table_name]\n",
    "    \n",
    "    # remove pkey, fkey\n",
    "    remove_pkey_fkey(col_to_stype, table)\n",
    "    \n",
    "    if len(col_to_stype) == 0:\n",
    "        # for example, relationship table which only contains pkey and fkey\n",
    "        raise KeyError(f\"{table_name} has no column to build graph\")\n",
    "    \n",
    "    path = (\n",
    "            None if cache_dir is None else os.path.join(cache_dir, f\"{table_name}.pt\")\n",
    "    )\n",
    "    \n",
    "    print(f\"-----> Materialize {table_name} Tensor Frame\")\n",
    "    dataset = Dataset(\n",
    "        df = df,\n",
    "        col_to_stype=col_to_stype,\n",
    "        col_to_text_embedder_cfg=text_embedder_cfg,\n",
    "    ).materialize(path=path)\n",
    "    \n",
    "    data[table_name].tf = dataset.tensor_frame\n",
    "    col_stats_dict[table_name] = dataset.col_stats\n",
    "    \n",
    "    # Add time attribute\n",
    "    if table.time_col is not None:\n",
    "        data[table_name].time = torch.from_numpy(\n",
    "            to_unix_time(df[table.time_col])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchInfo.SearchQuery: text_embedded --> unique values: 13210/1987156 || Nan Value: 1682279/1987156\n",
      "AdsInfo.Title: text_embedded --> unique values: 3246389/5960558 || Nan Value: 0/5960558\n"
     ]
    }
   ],
   "source": [
    "for table, type_dict in col_type_dict.items():\n",
    "    for col_name, stype in type_dict.items():\n",
    "        if stype == stype.text_embedded:\n",
    "            unique_value = db.table_dict[table].df[col_name].unique()\n",
    "            n = len(unique_value)\n",
    "            nm = len(db.table_dict[table].df)\n",
    "            nan_num = db.table_dict[table].df[col_name].isnull().sum()\n",
    "            print(f\"{table}.{col_name}: {stype} --> unique values: {n}/{nm} || Nan Value: {nan_num}/{nm}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the col_type_dict\n",
    "import pickle\n",
    "with open(f\"{cache_dir}/col_type_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(col_type_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
