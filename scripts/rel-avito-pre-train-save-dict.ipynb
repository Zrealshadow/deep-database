{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.base import Table\n",
    "from tqdm import tqdm\n",
    "from typing import Any,Dict\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from torch import Tensor\n",
    "from torch_frame import stype\n",
    "from torch_frame.config import TextEmbedderConfig\n",
    "from torch_frame.data import Dataset\n",
    "from torch_frame.data.stats import StatType\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-avito/db...\n",
      "Done in 4.60 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(name = \"rel-avito\", download = True)\n",
    "db = dataset.get_db()\n",
    "cache_path = \"data/rel-avito-tensor-frame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NOTE]: the dataset has been materialized\n",
    "\n",
    "# get infer_type in cache\n",
    "type_path = os.path.join(cache_path,\"col_type_dict.pkl\")\n",
    "col_type_dict = pickle.load(open(type_path, \"rb\"))\n",
    "len(col_type_dict)\n",
    "\n",
    "# add \"compress_text\" in each table in case \n",
    "for table_name, table in db.table_dict.items():\n",
    "    table.df[\"text_compress\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device\n",
    "                                       ] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            # \"all-MiniLM-L12-v2\",\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))\n",
    "\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=device), batch_size=512\n",
    ")\n",
    "\n",
    "def remove_pkey_fkey(col_to_stype: Dict[str, Any], table:Table) -> dict:\n",
    "    r\"\"\"Remove pkey, fkey columns since they will not be used as input feature.\"\"\"\n",
    "    if table.pkey_col is not None:\n",
    "        if table.pkey_col in col_to_stype:\n",
    "            col_to_stype.pop(table.pkey_col)\n",
    "    for fkey in table.fkey_col_to_pkey_table.keys():\n",
    "        if fkey in col_to_stype:\n",
    "            col_to_stype.pop(fkey)\n",
    "\n",
    "def to_unix_time(ser: pd.Series) -> np.ndarray:\n",
    "    r\"\"\"Converts a :class:`pandas.Timestamp` series to UNIX timestamp (in seconds).\"\"\"\n",
    "    assert ser.dtype in [np.dtype(\"datetime64[s]\"), np.dtype(\"datetime64[ns]\")]\n",
    "    unix_time = ser.astype(\"int64\").values\n",
    "    if ser.dtype == np.dtype(\"datetime64[ns]\"):\n",
    "        unix_time //= 10**9\n",
    "    return unix_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Materialize PhoneRequestsStream Tensor Frame\n",
      "-----> Materialize Location Tensor Frame\n",
      "-----> Materialize SearchInfo Tensor Frame\n",
      "-----> Materialize UserInfo Tensor Frame\n",
      "-----> Materialize SearchStream Tensor Frame\n",
      "-----> Materialize VisitStream Tensor Frame\n",
      "-----> Materialize AdsInfo Tensor Frame\n",
      "-----> Materialize Category Tensor Frame\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build graph\n",
    "\n",
    "# start build graph\n",
    "cache_dir = cache_path\n",
    "if cache_dir is not None:\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "data = HeteroData()\n",
    "col_stats_dict = {}\n",
    "for table_name, table in db.table_dict.items():\n",
    "    df = table.df\n",
    "    # (important for foreignKey value) Ensure the pkey is consecutive\n",
    "    if table.pkey_col is not None:\n",
    "        assert (df[table.pkey_col].values == np.arange(len(df))).all()\n",
    "    \n",
    "    col_to_stype = col_type_dict[table_name]\n",
    "    \n",
    "    # remove pkey, fkey\n",
    "    remove_pkey_fkey(col_to_stype, table)\n",
    "    \n",
    "    if len(col_to_stype) == 0:\n",
    "        # for example, relationship table which only contains pkey and fkey\n",
    "        raise KeyError(f\"{table_name} has no column to build graph\")\n",
    "    \n",
    "    path = (\n",
    "            None if cache_dir is None else os.path.join(cache_dir, f\"{table_name}.pt\")\n",
    "    )\n",
    "    \n",
    "    print(f\"-----> Materialize {table_name} Tensor Frame\")\n",
    "    dataset = Dataset(\n",
    "        df = df,\n",
    "        col_to_stype=col_to_stype,\n",
    "        col_to_text_embedder_cfg=text_embedder_cfg,\n",
    "    ).materialize(path=path)\n",
    "    \n",
    "    data[table_name].tf = dataset.tensor_frame\n",
    "    col_stats_dict[table_name] = dataset.col_stats\n",
    "    \n",
    "    # Add time attribute\n",
    "    if table.time_col is not None:\n",
    "        data[table_name].time = torch.from_numpy(\n",
    "            to_unix_time(df[table.time_col])\n",
    "        )\n",
    "    \n",
    "    # Add edges normal edges\n",
    "    for fkey_col_name, pkey_table_name in table.fkey_col_to_pkey_table.items():\n",
    "        pkey_index = df[fkey_col_name]\n",
    "        # Filter out dangling foreign keys\n",
    "        mask = ~pkey_index.isna()\n",
    "        fkey_index = torch.arange(len(pkey_index))\n",
    "        \n",
    "        # filter dangling foreign keys:\n",
    "        pkey_index = torch.from_numpy(pkey_index[mask].astype(int).values)\n",
    "        fkey_index = fkey_index[torch.from_numpy(mask.values)]\n",
    "        \n",
    "        # fkey -> pkey edges\n",
    "        edge_index = torch.stack([fkey_index, pkey_index], dim=0)\n",
    "        edge_type = (table_name, f\"f2p_{fkey_col_name}\", pkey_table_name)\n",
    "        data[edge_type].edge_index = sort_edge_index(edge_index)\n",
    "\n",
    "        # pkey -> fkey edges.\n",
    "        # \"rev_\" is added so that PyG loader recognizes the reverse edges\n",
    "        edge_index = torch.stack([pkey_index, fkey_index], dim=0)\n",
    "        edge_type = (pkey_table_name, f\"rev_f2p_{fkey_col_name}\", table_name)\n",
    "        data[edge_type].edge_index = sort_edge_index(edge_index)\n",
    "    \n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relbench.tasks import get_task\n",
    "from relbench.modeling.graph import get_node_train_table_input\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from relbench.base import BaseTask\n",
    "from model.base import CompositeModel, FeatureEncodingPart, NodeRepresentationPart\n",
    "from relbench.modeling.nn import HeteroTemporalEncoder\n",
    "# start to fine-train on the task a\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import math\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the additional edges\n",
    "from utils.util import load_np_dict\n",
    "edge_dict = load_np_dict(\"./edges/rel-avito-edges.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for edge_name, edge_np in edge_dict.items():\n",
    "    src_table, dst_table = edge_name.split('-')[0], edge_name.split('-')[1]\n",
    "    edge_index = torch.from_numpy(edge_np.astype(int)).t()\n",
    "    # [2, edge_num]\n",
    "    edge_type = (src_table, f\"appendix\", dst_table)\n",
    "    data[edge_type].edge_index = sort_edge_index(edge_index)\n",
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Location', 'UserInfo', 'Category'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the pre-extracted sample\n",
    "from utils.util import load_np_dict\n",
    "sample_dict = load_np_dict(\"./samples/rel-avito-samples.npz\")\n",
    "sample_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relbench.base import Database\n",
    "def neighborsample_batch(\n",
    "    db: Database,\n",
    "    entity_table: str,\n",
    "    node_idxs: np.ndarray,\n",
    "    num_neighbors: List[int] = [128,128],\n",
    "):\n",
    "    # node_idxs: [n]\n",
    "    nodes = (entity_table, torch.from_numpy(node_idxs))\n",
    "    n = node_idxs.shape[0]\n",
    "    input_time = torch.from_numpy(\n",
    "        to_unix_time(pd.Series([db.max_timestamp] * n)))\n",
    "\n",
    "    if db.table_dict[entity_table].time_col:\n",
    "        time_col = db.table_dict[entity_table].time_col\n",
    "        time_values = db.table_dict[entity_table].df[time_col].loc[node_idxs.tolist(\n",
    "        )]\n",
    "        input_time = torch.from_numpy(to_unix_time(time_values))\n",
    "\n",
    "    loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        input_nodes=nodes,\n",
    "        time_attr = \"time\",\n",
    "        input_time=input_time,\n",
    "        batch_size=n,\n",
    "        temporal_strategy=\"uniform\",\n",
    "        shuffle=False,\n",
    "        disjoint=True,\n",
    "        num_workers=0,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    return next(iter(loader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct bottom model\n",
    "channels = 128\n",
    "args = {\n",
    "    \"channels\": channels,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout_prob\": 0.2,\n",
    "}\n",
    "\n",
    "temporal_encoder = HeteroTemporalEncoder(\n",
    "    node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "    channels=args[\"channels\"],\n",
    ")\n",
    "\n",
    "feat_encoder = FeatureEncodingPart(\n",
    "    data=data,\n",
    "    node_to_col_stats=col_stats_dict,\n",
    "    channels=args[\"channels\"],\n",
    ")\n",
    "\n",
    "node_encoder = NodeRepresentationPart(\n",
    "    data=data,\n",
    "    channels=args[\"channels\"],\n",
    "    num_layers=1,\n",
    "    normalization=\"layer_norm\",\n",
    "    dropout_prob=0.3\n",
    ")\n",
    "\n",
    "net = CompositeModel(\n",
    "    data=data,\n",
    "    channels=args[\"channels\"],\n",
    "    out_channels=1,\n",
    "    dropout=0.3,\n",
    "    aggr=\"mean\",\n",
    "    norm=\"batch_norm\",\n",
    "    num_layer=2,\n",
    "    feature_encoder=feat_encoder,\n",
    "    node_encoder=node_encoder,\n",
    "    temporal_encoder=temporal_encoder\n",
    ")\n",
    "# net.reset_parameters()\n",
    "\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#   # net = torch.nn.DataParallel(net)\n",
    "#     net = DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train on Location\n",
      "shape of sample_np: (162, 21)\n",
      "Start to train on UserInfo\n",
      "shape of sample_np: (17615, 21)\n",
      "Start to train on Category\n",
      "shape of sample_np: (6, 21)\n"
     ]
    }
   ],
   "source": [
    "for sample_table, sample_np in sample_dict.items():\n",
    "    print(f\"Start to train on {sample_table}\")\n",
    "    print(f\"shape of sample_np: {sample_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_num_dict = {\n",
    "    \"Location\" : 10,\n",
    "    \"UserInfo\": 20,\n",
    "    \"Category\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import InfoNCE\n",
    "lr = 5e-4\n",
    "negative_sample_pool_size = 512\n",
    "temprature = 0.01\n",
    "net.reset_parameters()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "epoches = 20\n",
    "early_restart_steps = 20\n",
    "batch_size = 256\n",
    "max_steps_in_epoch = 30\n",
    "loss_fn = InfoNCE(temperature=temprature, negative_mode='paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************<Epoch: 01>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.08918241806972174, Cost Time 0m 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.024141385747740666, Cost Time 5m 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 2.634373505910238, Cost Time 0m 11s\n",
      "******************************<Epoch: 02>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.07407037122749988, Cost Time 0m 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.009554343406731884, Cost Time 5m 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.6830333073933919, Cost Time 0m 11s\n",
      "******************************<Epoch: 03>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.06527979579972631, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.006813572781781355, Cost Time 5m 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.0626503626505535, Cost Time 0m 10s\n",
      "******************************<Epoch: 04>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.060166376608389395, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.005733799515292048, Cost Time 5m 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.5939575831095378, Cost Time 0m 11s\n",
      "******************************<Epoch: 05>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.05451224762716411, Cost Time 0m 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0049683120256910724, Cost Time 5m 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.6488850911458333, Cost Time 0m 11s\n",
      "******************************<Epoch: 06>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.04804271827509374, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.004513030460414787, Cost Time 5m 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 2.2902706464131675, Cost Time 0m 12s\n",
      "******************************<Epoch: 07>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.04503394056249548, Cost Time 0m 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.004201838583685458, Cost Time 5m 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 2.034142812093099, Cost Time 0m 12s\n",
      "******************************<Epoch: 08>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.03845165393970631, Cost Time 0m 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.004059394417951504, Cost Time 5m 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.7021479606628418, Cost Time 0m 11s\n",
      "******************************<Epoch: 09>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.038684059072423865, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0038486236628765863, Cost Time 4m 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.6874440511067708, Cost Time 0m 10s\n",
      "******************************<Epoch: 10>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.034639611656283154, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.003654822101816535, Cost Time 5m 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.0534934997558594, Cost Time 0m 10s\n",
      "******************************<Epoch: 11>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.038157015670964745, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0034491363214328883, Cost Time 5m 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.524783452351888, Cost Time 0m 11s\n",
      "******************************<Epoch: 12>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.031006783614923924, Cost Time 0m 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0034276931391408047, Cost Time 5m 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.5290369987487793, Cost Time 0m 12s\n",
      "******************************<Epoch: 13>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.030287792653213314, Cost Time 0m 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.003338953270576894, Cost Time 5m 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 0.9579149087270101, Cost Time 0m 13s\n",
      "******************************<Epoch: 14>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.03347723572342484, Cost Time 0m 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.003075275290757418, Cost Time 5m 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.1142091751098633, Cost Time 0m 11s\n",
      "******************************<Epoch: 15>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.032191041075153116, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0030806957200790446, Cost Time 5m 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.6216039657592773, Cost Time 0m 11s\n",
      "******************************<Epoch: 16>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.03176107818697706, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0030247592522452274, Cost Time 5m 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 0.8297781149546305, Cost Time 0m 10s\n",
      "******************************<Epoch: 17>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.026165635497481736, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0028386797367905576, Cost Time 5m 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 0.39744122823079425, Cost Time 0m 10s\n",
      "******************************<Epoch: 18>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.027041576526783132, Cost Time 0m 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0028791360634689528, Cost Time 5m 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.1440845330556233, Cost Time 0m 11s\n",
      "******************************<Epoch: 19>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.02837402143596131, Cost Time 0m 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0027808332039664188, Cost Time 5m 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 1.058437665303548, Cost Time 0m 11s\n",
      "******************************<Epoch: 20>******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Location, Train loss: 0.027290547335589374, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In UserInfo, Train loss: 0.0026335918887828787, Cost Time 5m 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Category, Train loss: 0.8737351099650065, Cost Time 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "net.to(device)\n",
    "for epoch in range(1, epoches + 1):\n",
    "    net.train()\n",
    "    print(\"*\"*30 + f\"<Epoch: {epoch:02d}>\" + \"*\"*30)\n",
    "    for sample_table, sample_np in sample_dict.items():\n",
    "        loss_accum = count_accum = 0\n",
    "        shuffle_sample_np = sample_np[np.random.permutation(len(sample_np))]\n",
    "        anchor_nodes_np = shuffle_sample_np[:, 0]\n",
    "        positive_pool_np = shuffle_sample_np[:, 1:]\n",
    "        # choose the positive samples\n",
    "        n = sample_np.shape[0]\n",
    "        negative_num = negative_num_dict[sample_table]\n",
    "        m = len(db.table_dict[sample_table].df)\n",
    "        now = time.time()\n",
    "        cnt = 0\n",
    "        for batch_idx in tqdm(range(0, n, batch_size), leave=False):\n",
    "            cnt += 1\n",
    "            if cnt > max_steps_in_epoch:\n",
    "                break\n",
    "            anchor_nodes = anchor_nodes_np[batch_idx:batch_idx+batch_size]\n",
    "            positive_pool_batch_np = positive_pool_np[batch_idx:batch_idx+batch_size]\n",
    "            positive_nodes = []\n",
    "            # random select the positive samples\n",
    "            for row in positive_pool_batch_np:\n",
    "                valid = row[row != -1]\n",
    "                random_choice = np.random.choice(valid, 1)[0]\n",
    "                positive_nodes.append(random_choice)\n",
    "\n",
    "            positive_nodes = np.array(positive_nodes)\n",
    "            B = positive_nodes.size\n",
    "            # random select the negative sample, negative ratio is 1:20\n",
    "            # for one batch, we still extract batch_size negative samples\n",
    "            # for each positive-negative pair, we extract 20 from this 256 batch as negative samples\n",
    "            excluded = set(positive_nodes.tolist()).union(\n",
    "                set(anchor_nodes.tolist()))\n",
    "            negative_candidates = list(set(range(m)) - excluded)\n",
    "            # print(negative_candidates)\n",
    "            \n",
    "            sample_size = min(negative_sample_pool_size, len(negative_candidates))\n",
    "            \n",
    "            # if sample_size < positive_nodes.size:\n",
    "            #     # special case, for those number of positive pairs is too small, \n",
    "            #     # we employ the pure random to select the negative samples\n",
    "            #     negative_candidates = list(range(m))\n",
    "            #     sample_size = positive_nodes.size\n",
    "                \n",
    "            #     print(\"==> Candidate:\" + str(negative_candidates))\n",
    "            \n",
    "            negative_nodes = np.random.choice(\n",
    "                negative_candidates, size=sample_size, replace=True)\n",
    "            # [batch_size]\n",
    "            # print(negative_nodes.shape)\n",
    "            # print(B)\n",
    "            # neighbor hood loader\n",
    "            anchor_nodes_batch = neighborsample_batch(\n",
    "                db, sample_table, anchor_nodes)\n",
    "            positive_nodes_batch = neighborsample_batch(\n",
    "                db, sample_table, positive_nodes)\n",
    "            negative_nodes_batch = neighborsample_batch(\n",
    "                db, sample_table, negative_nodes)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            anchor_nodes_batch, positive_nodes_batch, negative_nodes_batch = \\\n",
    "                anchor_nodes_batch.to(device), positive_nodes_batch.to(\n",
    "                    device), negative_nodes_batch.to(device)\n",
    "\n",
    "            anchor_nodes_embedding = net.get_node_embedding(\n",
    "                anchor_nodes_batch, sample_table)[sample_table][:B]\n",
    "            positive_nodes_embedding = net.get_node_embedding(\n",
    "                positive_nodes_batch, sample_table)[sample_table][:B]\n",
    "            negative_nodes_embedding = net.get_node_embedding(\n",
    "                negative_nodes_batch, sample_table)[sample_table][:sample_size]\n",
    "\n",
    "            # negative_nodes_embedding = net.get_node_embedding(negative_nodes_batch, sample_table)[sample_table][:B]\n",
    "            # [B, D]\n",
    "            \n",
    "            negative_indices = torch.stack([torch.randperm(sample_size)[\n",
    "                                           :negative_num] for _ in range(B)]).to(device)\n",
    "            negative_nodes_embedding = negative_nodes_embedding[negative_indices]\n",
    "            # [B, negative_num, D]\n",
    "\n",
    "            loss = loss_fn(anchor_nodes_embedding,\n",
    "                           positive_nodes_embedding, negative_nodes_embedding)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss.detach().item()\n",
    "            count_accum += B\n",
    "        end = time.time()\n",
    "        mins, secs = divmod(end - now, 60)\n",
    "        train_loss = loss_accum / count_accum\n",
    "        \n",
    "        print(f\"====> In {sample_table}, Train loss: {train_loss}, Cost Time {mins:.0f}m {secs:.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_state = copy.deepcopy(net.state_dict())\n",
    "import torch\n",
    "import json\n",
    "torch.save(pre_trained_state, \"./static/rel-avito-pre-trained-channel128.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
