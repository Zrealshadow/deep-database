{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.base import Table\n",
    "from tqdm import tqdm\n",
    "from typing import Any,Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import Tensor\n",
    "from torch_frame import stype\n",
    "from torch_frame.config import TextEmbedderConfig\n",
    "from torch_frame.data import Dataset\n",
    "from torch_frame.data.stats import StatType\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.typing import NodeType\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "from utils.data import StackDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/stack/db...\n",
      "Done in 9.84 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset('rel-trial', download=True)\n",
    "db = dataset.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rule 0]: tags Inferred Id from numerical as categorical\n",
      "[rule 0]: postHistory Inferred Id from numerical as categorical\n",
      "[rule 0]: postHistory Inferred PostId from numerical as categorical\n",
      "[rule 0]: postHistory Inferred UserId from numerical as categorical\n",
      "[rule 0]: postHistory Inferred PostHistoryTypeId from numerical as categorical\n",
      "[rule 0]: postHistory Inferred ContentLicense from categorical as text_embedded\n",
      "[rule 1]: postHistory Inferred ContentLicense from text_embedded as categorical\n",
      "[rule 0]: postHistory Inferred RevisionGUID from text_embedded as categorical\n",
      "[rule 0]: comments Inferred Id from numerical as categorical\n",
      "[rule 0]: comments Inferred PostId from numerical as categorical\n",
      "[rule 0]: comments Inferred UserId from numerical as categorical\n",
      "[rule 1]: comments Inferred Score from numerical as categorical\n",
      "[rule 0]: comments Inferred ContentLicense from categorical as text_embedded\n",
      "[rule 1]: comments Inferred ContentLicense from text_embedded as categorical\n",
      "[rule 0]: badges Inferred Id from numerical as categorical\n",
      "[rule 0]: badges Inferred UserId from numerical as categorical\n",
      "[rule 0]: postTag Inferred Id from numerical as categorical\n",
      "[rule 0]: postTag Inferred TagId from numerical as categorical\n",
      "[rule 0]: postTag Inferred PostId from numerical as categorical\n",
      "[rule 0]: users Inferred Id from numerical as categorical\n",
      "[rule 0]: users Inferred AccountId from numerical as categorical\n",
      "[rule 0]: users Inferred WebsiteUrl from text_embedded as categorical\n",
      "[rule 0]: postLinks Inferred Id from numerical as categorical\n",
      "[rule 0]: postLinks Inferred RelatedPostId from numerical as categorical\n",
      "[rule 0]: postLinks Inferred PostId from numerical as categorical\n",
      "[rule 0]: votes Inferred Id from numerical as categorical\n",
      "[rule 0]: votes Inferred UserId from numerical as categorical\n",
      "[rule 0]: votes Inferred PostId from numerical as categorical\n",
      "[rule 0]: votes Inferred VoteTypeId from numerical as categorical\n",
      "[rule 0]: posts Inferred Id from numerical as categorical\n",
      "[rule 0]: posts Inferred OwnerUserId from numerical as categorical\n",
      "[rule 0]: posts Inferred PostTypeId from numerical as categorical\n",
      "[rule 0]: posts Inferred AcceptedAnswerId from numerical as categorical\n",
      "[rule 0]: posts Inferred ParentId from numerical as categorical\n",
      "[rule 0]: posts Inferred ContentLicense from categorical as text_embedded\n",
      "[rule 1]: posts Inferred ContentLicense from text_embedded as categorical\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import infer_type_in_db\n",
    "col_type_dict = infer_type_in_db(db, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags.TagName: text_embedded, unique values: 1597/1597 Nan Value: 0/1597\n",
      "postHistory.UserDisplayName: text_embedded, unique values: 3061/1175368 Nan Value: 1150131/1175368\n",
      "postHistory.Text: text_embedded, unique values: 960746/1175368 Nan Value: 114938/1175368\n",
      "postHistory.Comment: text_embedded, unique values: 125619/1175368 Nan Value: 714309/1175368\n",
      "comments.UserDisplayName: text_embedded, unique values: 2222/623967 Nan Value: 612281/623967\n",
      "comments.Text: text_embedded, unique values: 621044/623967 Nan Value: 0/623967\n",
      "badges.Name: text_embedded, unique values: 327/463463 Nan Value: 0/463463\n",
      "users.DisplayName: text_embedded, unique values: 218050/255360 Nan Value: 19/255360\n",
      "users.Location: text_embedded, unique values: 11514/255360 Nan Value: 184185/255360\n",
      "users.AboutMe: text_embedded, unique values: 47461/255360 Nan Value: 205676/255360\n",
      "posts.OwnerDisplayName: text_embedded, unique values: 4531/333893 Nan Value: 325346/333893\n",
      "posts.Title: text_embedded, unique values: 163648/333893 Nan Value: 170145/333893\n",
      "posts.Body: text_embedded, unique values: 333357/333893 Nan Value: 493/333893\n"
     ]
    }
   ],
   "source": [
    "# print text embedding type, profile\n",
    "for table, type_dict in col_type_dict.items():\n",
    "    for col_name, stype in type_dict.items():\n",
    "        if stype == stype.text_embedded:\n",
    "            unique_value = db.table_dict[table].df[col_name].unique()\n",
    "            n = len(unique_value)\n",
    "            nm = len(db.table_dict[table].df)\n",
    "            nan_num = db.table_dict[table].df[col_name].isnull().sum()\n",
    "            print(f\"{table}.{col_name}: {stype}, unique values: {n}/{nm} Nan Value: {nan_num}/{nm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pkey_fkey(col_to_stype: Dict[str, Any], table:Table) -> dict:\n",
    "    r\"\"\"Remove pkey, fkey columns since they will not be used as input feature.\"\"\"\n",
    "    if table.pkey_col is not None:\n",
    "        if table.pkey_col in col_to_stype:\n",
    "            col_to_stype.pop(table.pkey_col)\n",
    "    for fkey in table.fkey_col_to_pkey_table.keys():\n",
    "        if fkey in col_to_stype:\n",
    "            col_to_stype.pop(fkey)\n",
    "\n",
    "def to_unix_time(ser: pd.Series) -> np.ndarray:\n",
    "    r\"\"\"Converts a :class:`pandas.Timestamp` series to UNIX timestamp (in seconds).\"\"\"\n",
    "    assert ser.dtype in [np.dtype(\"datetime64[s]\"), np.dtype(\"datetime64[ns]\")]\n",
    "    unix_time = ser.astype(\"int64\").values\n",
    "    if ser.dtype == np.dtype(\"datetime64[ns]\"):\n",
    "        unix_time //= 10**9\n",
    "    return unix_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device\n",
    "                                       ] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"all-MiniLM-L12-v2\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))\n",
    "\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=device), batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Compressing postHistory text columns: ['UserDisplayName', 'Text', 'Comment']\n",
      "----> Compressing comments text columns: ['UserDisplayName', 'Text']\n",
      "----> Compressing users text columns: ['DisplayName', 'Location', 'AboutMe']\n",
      "----> Compressing posts text columns: ['OwnerDisplayName', 'Title']\n"
     ]
    }
   ],
   "source": [
    "# preprocess the table, concatenate the columns which is text type\n",
    "#        /--- text_col_1 ---/ --- text_col_2 --- / --- text_col_3 --- / \n",
    "# row 1  /------- A   -----/ ------- B   -----  / -----   C  ------- /\n",
    "# -------> Generate a new TexT column\n",
    "# \"text_col_1 is A, text_col_2 is B, text_col_3 is C\"\n",
    "\n",
    "# Therefore, we only need to convert this text column to vector \n",
    "# and drop the original text columns\n",
    "# for saving memory and computation \n",
    "\n",
    "\n",
    "for table_name, type_dict in col_type_dict.items():\n",
    "    # collect the text columns\n",
    "    text_cols = [ col for col, stype in type_dict.items() if stype == stype.text_embedded]\n",
    "    compress_cols = []\n",
    "    # for long text, we still keep it as one column\n",
    "    for col in text_cols:\n",
    "        avg_word_count = db.table_dict[table_name].df[col].dropna().apply(lambda x: len(str(x).split())).mean()\n",
    "        if avg_word_count < 128: # a half of default max length of BERT Max length （256）\n",
    "            # remove the long text cols\n",
    "            compress_cols.append(col)\n",
    "          \n",
    "    \n",
    "    if len(compress_cols) <= 1:\n",
    "        # if only one text column, we do not need to compress\n",
    "        continue\n",
    "    \n",
    "    print(f\"----> Compressing {table_name} text columns: {compress_cols}\")\n",
    "    \n",
    "    df = db.table_dict[table_name].df\n",
    "    compress_text_df = df[compress_cols]\n",
    "    \n",
    "    def row_to_text(row):\n",
    "        if row.isna().all():\n",
    "            return None\n",
    "        tokens = [f\"{key} is {value}\" for key, value in row.dropna().items()]\n",
    "        return \", \".join(tokens)\n",
    "\n",
    "    text_list = compress_text_df.apply(row_to_text, axis=1).tolist()\n",
    "    \n",
    "    # drop the compressed columns\n",
    "    df.drop(columns=compress_cols, inplace=True)\n",
    "    df[\"text_compress\"] = text_list\n",
    "    \n",
    "    # update the type dict\n",
    "    for col in compress_cols:\n",
    "        type_dict.pop(col)\n",
    "    type_dict[\"text_compress\"] = stype.text_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags.TagName: text_embedded --> unique values: 1597/1597 || Nan Value: 0/1597\n",
      "postHistory.text_compress: text_embedded --> unique values: 1040950/1175368 || Nan Value: 48747/1175368\n",
      "comments.text_compress: text_embedded --> unique values: 621070/623967 || Nan Value: 0/623967\n",
      "badges.Name: text_embedded --> unique values: 327/463463 || Nan Value: 0/463463\n",
      "users.text_compress: text_embedded --> unique values: 230262/255360 || Nan Value: 9/255360\n",
      "posts.Body: text_embedded --> unique values: 333357/333893 || Nan Value: 493/333893\n",
      "posts.text_compress: text_embedded --> unique values: 164938/333893 || Nan Value: 166739/333893\n"
     ]
    }
   ],
   "source": [
    "# print text embedding type\n",
    "for table, type_dict in col_type_dict.items():\n",
    "    for col_name, stype in type_dict.items():\n",
    "        if stype == stype.text_embedded:\n",
    "            unique_value = db.table_dict[table].df[col_name].unique()\n",
    "            n = len(unique_value)\n",
    "            nm = len(db.table_dict[table].df)\n",
    "            nan_num = db.table_dict[table].df[col_name].isnull().sum()\n",
    "            print(f\"{table}.{col_name}: {stype} --> unique values: {n}/{nm} || Nan Value: {nan_num}/{nm}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Materialize interventions Tensor Frame\n",
      "-----> Materialize interventions_studies Tensor Frame\n",
      "-----> Materialize facilities_studies Tensor Frame\n",
      "-----> Materialize sponsors Tensor Frame\n",
      "-----> Materialize eligibilities Tensor Frame\n",
      "-----> Materialize reported_event_totals Tensor Frame\n",
      "-----> Materialize designs Tensor Frame\n",
      "-----> Materialize conditions_studies Tensor Frame\n",
      "-----> Materialize drop_withdrawals Tensor Frame\n",
      "-----> Materialize studies Tensor Frame\n",
      "-----> Materialize outcome_analyses Tensor Frame\n",
      "-----> Materialize sponsors_studies Tensor Frame\n",
      "-----> Materialize outcomes Tensor Frame\n",
      "-----> Materialize conditions Tensor Frame\n",
      "-----> Materialize facilities Tensor Frame\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start build graph\n",
    "cache_dir = \"./data/rel-trial-tensor-frame\"\n",
    "# cache_dir = \"./data/stack-tensor-frame\"\n",
    "if cache_dir is not None:\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "data = HeteroData()\n",
    "col_stats_dict = {}\n",
    "for table_name, table in db.table_dict.items():\n",
    "    df = table.df\n",
    "    # (important for foreignKey value) Ensure the pkey is consecutive\n",
    "    if table.pkey_col is not None:\n",
    "        assert (df[table.pkey_col].values == np.arange(len(df))).all()\n",
    "    \n",
    "    col_to_stype = col_type_dict[table_name]\n",
    "    \n",
    "    # remove pkey, fkey\n",
    "    remove_pkey_fkey(col_to_stype, table)\n",
    "    \n",
    "    if len(col_to_stype) == 0:\n",
    "        # for example, relationship table which only contains pkey and fkey\n",
    "        raise KeyError(f\"{table_name} has no column to build graph\")\n",
    "    \n",
    "    path = (\n",
    "            None if cache_dir is None else os.path.join(cache_dir, f\"{table_name}.pt\")\n",
    "    )\n",
    "    \n",
    "    print(f\"-----> Materialize {table_name} Tensor Frame\")\n",
    "    dataset = Dataset(\n",
    "        df = df,\n",
    "        col_to_stype=col_to_stype,\n",
    "        col_to_text_embedder_cfg=text_embedder_cfg,\n",
    "    ).materialize(path=path)\n",
    "    \n",
    "    data[table_name].tf = dataset.tensor_frame\n",
    "    col_stats_dict[table_name] = dataset.col_stats\n",
    "    \n",
    "    # Add time attribute\n",
    "    if table.time_col is not None:\n",
    "        data[table_name].time = torch.from_numpy(\n",
    "            to_unix_time(df[table.time_col])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the col_type_dict\n",
    "import pickle\n",
    "with open(f\"{cache_dir}/col_type_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(col_type_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
