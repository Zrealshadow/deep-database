{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from relbench.datasets import get_dataset\n",
    "\n",
    "from utils.data import preprocess_event_database\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-event/db...\n",
      "Done in 2.97 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion/utils/data.py:231: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_df[\"event_id\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  event_df[\"event_id\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:233: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_interest_df[\"event\"].replace(event_id2index, inplace=True)\n",
      "/home/lingze/embedding_fusion/utils/data.py:234: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  event_attendees_flattened_df[\"event\"].replace(event_id2index, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset('rel-event')\n",
    "db = dataset.get_db()\n",
    "preprocess_event_database(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table event_attendees\n",
      "id Nonan 49822 - 49822  unique values, 0 NaN values\n",
      "event Nonan 49822 - 6823  unique values, 0 NaN values\n",
      "status Nonan 49822 - 4  unique values, 0 NaN values\n",
      "user_id Nonan 49822 - 9257  unique values, 0 NaN values\n",
      "start_time Nonan 49822 - 2576  unique values, 0 NaN values\n",
      "****************************************\n",
      "Table user_friends\n",
      "id Nonan 213703 - 213703  unique values, 0 NaN values\n",
      "user Nonan 213703 - 28164  unique values, 0 NaN values\n",
      "friend Nonan 213703 - 28184  unique values, 0 NaN values\n",
      "****************************************\n",
      "Table events\n",
      "event_id Nonan 11465 - 11465  unique values, 0 NaN values\n",
      "user_id Nonan 86 - 61  unique values, 11379 NaN values\n",
      "start_time Nonan 11465 - 3667  unique values, 0 NaN values\n",
      "city Nonan 6024 - 998  unique values, 5441 NaN values\n",
      "state Nonan 4661 - 89  unique values, 6804 NaN values\n",
      "zip Nonan 973 - 643  unique values, 10492 NaN values\n",
      "country Nonan 6047 - 82  unique values, 5418 NaN values\n",
      "lat Nonan 7396 - 3418  unique values, 4069 NaN values\n",
      "lng Nonan 7396 - 3613  unique values, 4069 NaN values\n",
      "c_1 Nonan 11465 - 54  unique values, 0 NaN values\n",
      "c_2 Nonan 11465 - 39  unique values, 0 NaN values\n",
      "c_3 Nonan 11465 - 34  unique values, 0 NaN values\n",
      "c_4 Nonan 11465 - 30  unique values, 0 NaN values\n",
      "c_5 Nonan 11465 - 34  unique values, 0 NaN values\n",
      "c_6 Nonan 11465 - 83  unique values, 0 NaN values\n",
      "c_7 Nonan 11465 - 23  unique values, 0 NaN values\n",
      "c_8 Nonan 11465 - 19  unique values, 0 NaN values\n",
      "c_9 Nonan 11465 - 28  unique values, 0 NaN values\n",
      "c_10 Nonan 11465 - 18  unique values, 0 NaN values\n",
      "c_11 Nonan 11465 - 18  unique values, 0 NaN values\n",
      "c_12 Nonan 11465 - 16  unique values, 0 NaN values\n",
      "c_13 Nonan 11465 - 17  unique values, 0 NaN values\n",
      "c_14 Nonan 11465 - 19  unique values, 0 NaN values\n",
      "c_15 Nonan 11465 - 18  unique values, 0 NaN values\n",
      "c_16 Nonan 11465 - 16  unique values, 0 NaN values\n",
      "c_17 Nonan 11465 - 17  unique values, 0 NaN values\n",
      "c_18 Nonan 11465 - 20  unique values, 0 NaN values\n",
      "c_19 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_20 Nonan 11465 - 15  unique values, 0 NaN values\n",
      "c_21 Nonan 11465 - 21  unique values, 0 NaN values\n",
      "c_22 Nonan 11465 - 15  unique values, 0 NaN values\n",
      "c_23 Nonan 11465 - 15  unique values, 0 NaN values\n",
      "c_24 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_25 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_26 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_27 Nonan 11465 - 16  unique values, 0 NaN values\n",
      "c_28 Nonan 11465 - 14  unique values, 0 NaN values\n",
      "c_29 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_30 Nonan 11465 - 16  unique values, 0 NaN values\n",
      "c_31 Nonan 11465 - 15  unique values, 0 NaN values\n",
      "c_32 Nonan 11465 - 14  unique values, 0 NaN values\n",
      "c_33 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_34 Nonan 11465 - 15  unique values, 0 NaN values\n",
      "c_35 Nonan 11465 - 11  unique values, 0 NaN values\n",
      "c_36 Nonan 11465 - 15  unique values, 0 NaN values\n",
      "c_37 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_38 Nonan 11465 - 17  unique values, 0 NaN values\n",
      "c_39 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_40 Nonan 11465 - 16  unique values, 0 NaN values\n",
      "c_41 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_42 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_43 Nonan 11465 - 14  unique values, 0 NaN values\n",
      "c_44 Nonan 11465 - 19  unique values, 0 NaN values\n",
      "c_45 Nonan 11465 - 14  unique values, 0 NaN values\n",
      "c_46 Nonan 11465 - 8  unique values, 0 NaN values\n",
      "c_47 Nonan 11465 - 11  unique values, 0 NaN values\n",
      "c_48 Nonan 11465 - 19  unique values, 0 NaN values\n",
      "c_49 Nonan 11465 - 11  unique values, 0 NaN values\n",
      "c_50 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_51 Nonan 11465 - 11  unique values, 0 NaN values\n",
      "c_52 Nonan 11465 - 16  unique values, 0 NaN values\n",
      "c_53 Nonan 11465 - 11  unique values, 0 NaN values\n",
      "c_54 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_55 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_56 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_57 Nonan 11465 - 8  unique values, 0 NaN values\n",
      "c_58 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_59 Nonan 11465 - 8  unique values, 0 NaN values\n",
      "c_60 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_61 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_62 Nonan 11465 - 8  unique values, 0 NaN values\n",
      "c_63 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_64 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_65 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_66 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_67 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_68 Nonan 11465 - 8  unique values, 0 NaN values\n",
      "c_69 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_70 Nonan 11465 - 6  unique values, 0 NaN values\n",
      "c_71 Nonan 11465 - 13  unique values, 0 NaN values\n",
      "c_72 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_73 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_74 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_75 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_76 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_77 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_78 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_79 Nonan 11465 - 14  unique values, 0 NaN values\n",
      "c_80 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_81 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_82 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_83 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_84 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_85 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_86 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_87 Nonan 11465 - 8  unique values, 0 NaN values\n",
      "c_88 Nonan 11465 - 11  unique values, 0 NaN values\n",
      "c_89 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_90 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_91 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_92 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_93 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_94 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_95 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_96 Nonan 11465 - 12  unique values, 0 NaN values\n",
      "c_97 Nonan 11465 - 10  unique values, 0 NaN values\n",
      "c_98 Nonan 11465 - 7  unique values, 0 NaN values\n",
      "c_99 Nonan 11465 - 9  unique values, 0 NaN values\n",
      "c_100 Nonan 11465 - 8  unique values, 0 NaN values\n",
      "c_other Nonan 11465 - 418  unique values, 0 NaN values\n",
      "****************************************\n",
      "Table event_interest\n",
      "user Nonan 14135 - 1970  unique values, 0 NaN values\n",
      "event Nonan 14135 - 8119  unique values, 0 NaN values\n",
      "invited Nonan 14135 - 2  unique values, 0 NaN values\n",
      "timestamp Nonan 14135 - 4400  unique values, 0 NaN values\n",
      "interested Nonan 14135 - 2  unique values, 0 NaN values\n",
      "not_interested Nonan 14135 - 2  unique values, 0 NaN values\n",
      "id Nonan 14135 - 14135  unique values, 0 NaN values\n",
      "****************************************\n",
      "Table users\n",
      "user_id Nonan 37143 - 37143  unique values, 0 NaN values\n",
      "locale Nonan 37143 - 64  unique values, 0 NaN values\n",
      "birthyear Nonan 35668 - 94  unique values, 1475 NaN values\n",
      "gender Nonan 37037 - 2  unique values, 106 NaN values\n",
      "joinedAt Nonan 37143 - 37143  unique values, 0 NaN values\n",
      "location Nonan 31747 - 2705  unique values, 5396 NaN values\n",
      "timezone Nonan 36749 - 52  unique values, 394 NaN values\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# describe data\n",
    "def describe_df(df: pd.DataFrame):\n",
    "    n = len(df)\n",
    "    for col in df.columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        nan_count = df[col].isnull().sum()\n",
    "        print(f'{col} Nonan {n-nan_count} - {unique_count}  unique values, {nan_count} NaN values')\n",
    "        \n",
    "for table_name, table in db.table_dict.items():\n",
    "    print(f\"Table {table_name}\")\n",
    "    describe_df(table.df)\n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table event_attendees has 49822 rows\n",
      "Table user_friends has 213703 rows\n",
      "Table events has 11465 rows\n",
      "Table event_interest has 14135 rows\n",
      "Table users has 37143 rows\n"
     ]
    }
   ],
   "source": [
    "for table_name, table in db.table_dict.items():\n",
    "    n = len(table.df)\n",
    "    print(f\"Table {table_name} has {n} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rule 0]: event_attendees Inferred id from numerical as categorical\n",
      "[rule 0]: event_attendees Inferred user_id from numerical as categorical\n",
      "[rule 0]: user_friends Inferred id from numerical as categorical\n",
      "[rule 0]: events Inferred event_id from numerical as categorical\n",
      "[rule 0]: events Inferred user_id from numerical as categorical\n",
      "[rule 1]: events Inferred c_1 from numerical as categorical\n",
      "[rule 1]: events Inferred c_2 from numerical as categorical\n",
      "[rule 1]: events Inferred c_3 from numerical as categorical\n",
      "[rule 1]: events Inferred c_4 from numerical as categorical\n",
      "[rule 1]: events Inferred c_5 from numerical as categorical\n",
      "[rule 1]: events Inferred c_6 from numerical as categorical\n",
      "[rule 1]: events Inferred c_7 from numerical as categorical\n",
      "[rule 1]: events Inferred c_8 from numerical as categorical\n",
      "[rule 1]: events Inferred c_9 from numerical as categorical\n",
      "[rule 1]: events Inferred c_10 from numerical as categorical\n",
      "[rule 1]: events Inferred c_11 from numerical as categorical\n",
      "[rule 1]: events Inferred c_12 from numerical as categorical\n",
      "[rule 1]: events Inferred c_13 from numerical as categorical\n",
      "[rule 1]: events Inferred c_14 from numerical as categorical\n",
      "[rule 1]: events Inferred c_15 from numerical as categorical\n",
      "[rule 1]: events Inferred c_16 from numerical as categorical\n",
      "[rule 1]: events Inferred c_17 from numerical as categorical\n",
      "[rule 1]: events Inferred c_18 from numerical as categorical\n",
      "[rule 1]: events Inferred c_19 from numerical as categorical\n",
      "[rule 1]: events Inferred c_20 from numerical as categorical\n",
      "[rule 1]: events Inferred c_21 from numerical as categorical\n",
      "[rule 1]: events Inferred c_22 from numerical as categorical\n",
      "[rule 1]: events Inferred c_23 from numerical as categorical\n",
      "[rule 1]: events Inferred c_24 from numerical as categorical\n",
      "[rule 1]: events Inferred c_25 from numerical as categorical\n",
      "[rule 1]: events Inferred c_26 from numerical as categorical\n",
      "[rule 1]: events Inferred c_27 from numerical as categorical\n",
      "[rule 1]: events Inferred c_28 from numerical as categorical\n",
      "[rule 1]: events Inferred c_29 from numerical as categorical\n",
      "[rule 1]: events Inferred c_30 from numerical as categorical\n",
      "[rule 1]: events Inferred c_31 from numerical as categorical\n",
      "[rule 1]: events Inferred c_32 from numerical as categorical\n",
      "[rule 1]: events Inferred c_33 from numerical as categorical\n",
      "[rule 1]: events Inferred c_34 from numerical as categorical\n",
      "[rule 1]: events Inferred c_35 from numerical as categorical\n",
      "[rule 1]: events Inferred c_36 from numerical as categorical\n",
      "[rule 1]: events Inferred c_37 from numerical as categorical\n",
      "[rule 1]: events Inferred c_38 from numerical as categorical\n",
      "[rule 1]: events Inferred c_39 from numerical as categorical\n",
      "[rule 1]: events Inferred c_40 from numerical as categorical\n",
      "[rule 1]: events Inferred c_41 from numerical as categorical\n",
      "[rule 1]: events Inferred c_42 from numerical as categorical\n",
      "[rule 1]: events Inferred c_43 from numerical as categorical\n",
      "[rule 1]: events Inferred c_44 from numerical as categorical\n",
      "[rule 1]: events Inferred c_45 from numerical as categorical\n",
      "[rule 1]: events Inferred c_46 from numerical as categorical\n",
      "[rule 1]: events Inferred c_47 from numerical as categorical\n",
      "[rule 1]: events Inferred c_48 from numerical as categorical\n",
      "[rule 1]: events Inferred c_49 from numerical as categorical\n",
      "[rule 1]: events Inferred c_50 from numerical as categorical\n",
      "[rule 1]: events Inferred c_51 from numerical as categorical\n",
      "[rule 1]: events Inferred c_52 from numerical as categorical\n",
      "[rule 1]: events Inferred c_53 from numerical as categorical\n",
      "[rule 1]: events Inferred c_54 from numerical as categorical\n",
      "[rule 1]: events Inferred c_55 from numerical as categorical\n",
      "[rule 1]: events Inferred c_56 from numerical as categorical\n",
      "[rule 1]: events Inferred c_57 from numerical as categorical\n",
      "[rule 1]: events Inferred c_58 from numerical as categorical\n",
      "[rule 1]: events Inferred c_59 from numerical as categorical\n",
      "[rule 1]: events Inferred c_60 from numerical as categorical\n",
      "[rule 1]: events Inferred c_61 from numerical as categorical\n",
      "[rule 1]: events Inferred c_62 from numerical as categorical\n",
      "[rule 1]: events Inferred c_63 from numerical as categorical\n",
      "[rule 1]: events Inferred c_64 from numerical as categorical\n",
      "[rule 1]: events Inferred c_65 from numerical as categorical\n",
      "[rule 1]: events Inferred c_66 from numerical as categorical\n",
      "[rule 1]: events Inferred c_67 from numerical as categorical\n",
      "[rule 1]: events Inferred c_68 from numerical as categorical\n",
      "[rule 1]: events Inferred c_69 from numerical as categorical\n",
      "[rule 1]: events Inferred c_70 from numerical as categorical\n",
      "[rule 1]: events Inferred c_71 from numerical as categorical\n",
      "[rule 1]: events Inferred c_72 from numerical as categorical\n",
      "[rule 1]: events Inferred c_73 from numerical as categorical\n",
      "[rule 1]: events Inferred c_74 from numerical as categorical\n",
      "[rule 1]: events Inferred c_75 from numerical as categorical\n",
      "[rule 1]: events Inferred c_76 from numerical as categorical\n",
      "[rule 1]: events Inferred c_77 from numerical as categorical\n",
      "[rule 1]: events Inferred c_78 from numerical as categorical\n",
      "[rule 1]: events Inferred c_79 from numerical as categorical\n",
      "[rule 1]: events Inferred c_80 from numerical as categorical\n",
      "[rule 1]: events Inferred c_81 from numerical as categorical\n",
      "[rule 1]: events Inferred c_82 from numerical as categorical\n",
      "[rule 1]: events Inferred c_83 from numerical as categorical\n",
      "[rule 1]: events Inferred c_84 from numerical as categorical\n",
      "[rule 1]: events Inferred c_85 from numerical as categorical\n",
      "[rule 1]: events Inferred c_86 from numerical as categorical\n",
      "[rule 1]: events Inferred c_87 from numerical as categorical\n",
      "[rule 1]: events Inferred c_88 from numerical as categorical\n",
      "[rule 1]: events Inferred c_89 from numerical as categorical\n",
      "[rule 1]: events Inferred c_90 from numerical as categorical\n",
      "[rule 1]: events Inferred c_91 from numerical as categorical\n",
      "[rule 1]: events Inferred c_92 from numerical as categorical\n",
      "[rule 1]: events Inferred c_93 from numerical as categorical\n",
      "[rule 1]: events Inferred c_94 from numerical as categorical\n",
      "[rule 1]: events Inferred c_95 from numerical as categorical\n",
      "[rule 1]: events Inferred c_96 from numerical as categorical\n",
      "[rule 1]: events Inferred c_97 from numerical as categorical\n",
      "[rule 1]: events Inferred c_98 from numerical as categorical\n",
      "[rule 1]: events Inferred c_99 from numerical as categorical\n",
      "[rule 1]: events Inferred c_100 from numerical as categorical\n",
      "[rule 0]: event_interest Inferred id from numerical as categorical\n",
      "[rule 0]: users Inferred user_id from numerical as categorical\n",
      "[rule 1]: users Inferred locale from text_embedded as categorical\n",
      "[rule 1]: users Inferred birthyear from numerical as categorical\n",
      "[rule 1]: users Inferred timezone from numerical as categorical\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import infer_type_in_db\n",
    "from utils.tokenize import tokenize_database\n",
    "col_type_dict = infer_type_in_db(db, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table event_attendees\n",
      "id: categorical\n",
      "event: categorical\n",
      "status: categorical\n",
      "user_id: categorical\n",
      "start_time: timestamp\n",
      "****************************************\n",
      "Table user_friends\n",
      "id: categorical\n",
      "user: categorical\n",
      "friend: categorical\n",
      "****************************************\n",
      "Table events\n",
      "event_id: categorical\n",
      "user_id: categorical\n",
      "start_time: timestamp\n",
      "city: text_embedded\n",
      "state: text_embedded\n",
      "zip: text_embedded\n",
      "country: text_embedded\n",
      "lat: numerical\n",
      "lng: numerical\n",
      "c_1: categorical\n",
      "c_2: categorical\n",
      "c_3: categorical\n",
      "c_4: categorical\n",
      "c_5: categorical\n",
      "c_6: categorical\n",
      "c_7: categorical\n",
      "c_8: categorical\n",
      "c_9: categorical\n",
      "c_10: categorical\n",
      "c_11: categorical\n",
      "c_12: categorical\n",
      "c_13: categorical\n",
      "c_14: categorical\n",
      "c_15: categorical\n",
      "c_16: categorical\n",
      "c_17: categorical\n",
      "c_18: categorical\n",
      "c_19: categorical\n",
      "c_20: categorical\n",
      "c_21: categorical\n",
      "c_22: categorical\n",
      "c_23: categorical\n",
      "c_24: categorical\n",
      "c_25: categorical\n",
      "c_26: categorical\n",
      "c_27: categorical\n",
      "c_28: categorical\n",
      "c_29: categorical\n",
      "c_30: categorical\n",
      "c_31: categorical\n",
      "c_32: categorical\n",
      "c_33: categorical\n",
      "c_34: categorical\n",
      "c_35: categorical\n",
      "c_36: categorical\n",
      "c_37: categorical\n",
      "c_38: categorical\n",
      "c_39: categorical\n",
      "c_40: categorical\n",
      "c_41: categorical\n",
      "c_42: categorical\n",
      "c_43: categorical\n",
      "c_44: categorical\n",
      "c_45: categorical\n",
      "c_46: categorical\n",
      "c_47: categorical\n",
      "c_48: categorical\n",
      "c_49: categorical\n",
      "c_50: categorical\n",
      "c_51: categorical\n",
      "c_52: categorical\n",
      "c_53: categorical\n",
      "c_54: categorical\n",
      "c_55: categorical\n",
      "c_56: categorical\n",
      "c_57: categorical\n",
      "c_58: categorical\n",
      "c_59: categorical\n",
      "c_60: categorical\n",
      "c_61: categorical\n",
      "c_62: categorical\n",
      "c_63: categorical\n",
      "c_64: categorical\n",
      "c_65: categorical\n",
      "c_66: categorical\n",
      "c_67: categorical\n",
      "c_68: categorical\n",
      "c_69: categorical\n",
      "c_70: categorical\n",
      "c_71: categorical\n",
      "c_72: categorical\n",
      "c_73: categorical\n",
      "c_74: categorical\n",
      "c_75: categorical\n",
      "c_76: categorical\n",
      "c_77: categorical\n",
      "c_78: categorical\n",
      "c_79: categorical\n",
      "c_80: categorical\n",
      "c_81: categorical\n",
      "c_82: categorical\n",
      "c_83: categorical\n",
      "c_84: categorical\n",
      "c_85: categorical\n",
      "c_86: categorical\n",
      "c_87: categorical\n",
      "c_88: categorical\n",
      "c_89: categorical\n",
      "c_90: categorical\n",
      "c_91: categorical\n",
      "c_92: categorical\n",
      "c_93: categorical\n",
      "c_94: categorical\n",
      "c_95: categorical\n",
      "c_96: categorical\n",
      "c_97: categorical\n",
      "c_98: categorical\n",
      "c_99: categorical\n",
      "c_100: categorical\n",
      "c_other: numerical\n",
      "****************************************\n",
      "Table event_interest\n",
      "user: categorical\n",
      "event: categorical\n",
      "invited: categorical\n",
      "timestamp: timestamp\n",
      "interested: categorical\n",
      "not_interested: categorical\n",
      "id: categorical\n",
      "****************************************\n",
      "Table users\n",
      "user_id: categorical\n",
      "locale: categorical\n",
      "birthyear: categorical\n",
      "gender: categorical\n",
      "joinedAt: timestamp\n",
      "location: text_embedded\n",
      "timezone: categorical\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# check all col types\n",
    "for table_name, col_types in col_type_dict.items():\n",
    "    print(f\"Table {table_name}\")\n",
    "    for col, type_ in col_types.items():\n",
    "        print(f\"{col}: {type_}\")\n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Compressing events text columns: ['city', 'state', 'zip', 'country']\n"
     ]
    }
   ],
   "source": [
    "# preprocess the table, concatenate the columns which is text type\n",
    "#        /--- text_col_1 ---/ --- text_col_2 --- / --- text_col_3 --- / \n",
    "# row 1  /------- A   -----/ ------- B   -----  / -----   C  ------- /\n",
    "# -------> Generate a new TexT column\n",
    "# \"text_col_1 is A, text_col_2 is B, text_col_3 is C\"\n",
    "\n",
    "# Therefore, we only need to convert this text column to vector \n",
    "# and drop the original text columns\n",
    "# for saving memory and computation \n",
    "from torch_frame import stype\n",
    "\n",
    "for table_name, type_dict in col_type_dict.items():\n",
    "    # collect the text columns\n",
    "    text_cols = [ col for col, stype in type_dict.items() if stype == stype.text_embedded]\n",
    "    compress_cols = []\n",
    "    # for long text, we still keep it as one column\n",
    "    for col in text_cols:\n",
    "        avg_word_count = db.table_dict[table_name].df[col].dropna().apply(lambda x: len(str(x).split())).mean()\n",
    "        if avg_word_count < 128: # a half of default max length of BERT Max length （256）\n",
    "            # remove the long text cols\n",
    "            compress_cols.append(col)\n",
    "          \n",
    "    \n",
    "    if len(compress_cols) <= 1:\n",
    "        # if only one text column, we do not need to compress\n",
    "        continue\n",
    "    \n",
    "    print(f\"----> Compressing {table_name} text columns: {compress_cols}\")\n",
    "    \n",
    "    df = db.table_dict[table_name].df\n",
    "    compress_text_df = df[compress_cols]\n",
    "    \n",
    "    def row_to_text(row):\n",
    "        if row.isna().all():\n",
    "            return None\n",
    "        tokens = [f\"{key} is {value}\" for key, value in row.dropna().items()]\n",
    "        return \", \".join(tokens)\n",
    "\n",
    "    text_list = compress_text_df.apply(row_to_text, axis=1).tolist()\n",
    "    \n",
    "    # drop the compressed columns\n",
    "    df.drop(columns=compress_cols, inplace=True)\n",
    "    df[\"text_compress\"] = text_list\n",
    "    \n",
    "    # update the type dict\n",
    "    for col in compress_cols:\n",
    "        type_dict.pop(col)\n",
    "    type_dict[\"text_compress\"] = stype.text_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.resource import get_text_embedder_cfg\n",
    "text_embedder_cfg = get_text_embedder_cfg(\n",
    "    # model_name = \"sentence-transformers/average_word_embeddings_glove.6B.300d\", \n",
    "    model_name = \"all-MiniLM-L12-v2\",\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Materialize event_attendees Tensor Frame\n",
      "-----> Build edge between users and users\n",
      "-----> Materialize events Tensor Frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding raw data in mini-batch: 100%|██████████| 23/23 [00:09<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Materialize event_interest Tensor Frame\n",
      "-----> Materialize users Tensor Frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding raw data in mini-batch: 100%|██████████| 73/73 [00:24<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.builder import build_pyg_hetero_graph\n",
    "cache_dir = \"./data/rel-event-tensor-frame\"\n",
    "data, col_stats_dict = build_pyg_hetero_graph(\n",
    "    db,\n",
    "    col_type_dict,\n",
    "    text_embedder_cfg,\n",
    "    cache_dir,\n",
    "    True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the col_type_dict\n",
    "with open(os.path.join(cache_dir, \"col_type_dict.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(col_type_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
