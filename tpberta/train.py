"""
TP-BERTa Training Module

Trains prediction head on preprocessed embedding data.
"""

import sys
from pathlib import Path
from typing import Optional, List
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error


class EmbeddingDataset(Dataset):
    """Dataset for loading embedding strings and labels."""
    
    def __init__(self, embedding_strings: List[str], labels: np.ndarray, 
                 embedding_dim: Optional[int] = None):
        """
        Args:
            embedding_strings: List of comma-separated embedding strings
            labels: Array of labels
            embedding_dim: Dimension of embeddings (auto-detect if None)
        """
        self.embedding_strings = embedding_strings
        self.labels = labels
        
        # Auto-detect embedding dimension from first embedding
        if embedding_dim is None and len(embedding_strings) > 0:
            self.embedding_dim = len(embedding_strings[0].split(","))
        else:
            self.embedding_dim = embedding_dim
    
    def __len__(self):
        return len(self.embedding_strings)
    
    def __getitem__(self, idx):
        # Parse comma-separated embedding string
        embedding = np.array([float(x) for x in self.embedding_strings[idx].split(",")])
        
        # Ensure correct dimension
        if self.embedding_dim is not None and len(embedding) != self.embedding_dim:
            raise ValueError(f"Embedding dimension mismatch: expected {self.embedding_dim}, got {len(embedding)}")
        
        return {
            'embedding': torch.FloatTensor(embedding),
            'label': torch.FloatTensor([self.labels[idx]])
        }


class PredictionHead(nn.Module):
    """Simple MLP prediction head."""
    
    def __init__(self, input_dim: int, hidden_dims: List[int] = [256, 128], 
                 output_dim: int = 1, dropout: float = 0.2):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


def train_prediction_head(
    data_dir: str,
    output_dir: str,
    target_col_txt_path: Optional[str] = None,
    hidden_dims: List[int] = [256, 128],
    dropout: float = 0.2,
    batch_size: int = 256,
    learning_rate: float = 0.001,
    num_epochs: int = 200,
    early_stop: int = 10,
    device: Optional[str] = None,
) -> dict:
    """
    Train prediction head on preprocessed embedding data.
    
    This function loads embeddings from separate train/val/test CSV files (generated by preprocess.py) 
    and trains a simple MLP prediction head. No TP-BERTa model is loaded - only the embeddings.
    
    Args:
        data_dir: Directory containing train.csv, val.csv, test.csv (each with columns: embedding, target)
        output_dir: Directory to save model and results
        target_col_txt_path: Path to target_col.txt (if None, auto-detect from parent dir)
        hidden_dims: Hidden layer dimensions for prediction head
        dropout: Dropout rate
        batch_size: Batch size
        learning_rate: Learning rate
        num_epochs: Maximum number of epochs
        early_stop: Early stopping patience
        device: Device to use (default: "cuda" if available)
    
    Returns:
        Dictionary with training results and metrics
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    data_dir = Path(data_dir)
    
    # Auto-detect target_col.txt path
    if target_col_txt_path is None:
        target_col_txt_path = data_dir.parent / "target_col.txt"  # Go up one level to find original data dir
    
    # Load task type from target_col.txt (always exists)
    with open(target_col_txt_path, 'r') as f:
        lines = f.readlines()
        task_type_str = lines[1].strip()
    task_type_map = {
        "BINARY_CLASSIFICATION": "binclass",
        "REGRESSION": "regression",
        "MULTICLASS_CLASSIFICATION": "multiclass"
    }
    task_type = task_type_map.get(task_type_str, "binclass")
    print(f"Detected task type from target_col.txt: {task_type}")
    
    # Load data from separate CSV files
    print(f"Loading embedding data from {data_dir}...")
    
    train_csv = data_dir / "train.csv"
    val_csv = data_dir / "val.csv"
    test_csv = data_dir / "test.csv"
    
    if not train_csv.exists():
        raise FileNotFoundError(f"train.csv not found in {data_dir}")
    if not val_csv.exists():
        raise FileNotFoundError(f"val.csv not found in {data_dir}")
    if not test_csv.exists():
        raise FileNotFoundError(f"test.csv not found in {data_dir}")
    
    # Load each split
    train_df = pd.read_csv(train_csv)
    train_strings = train_df['embedding'].tolist()
    train_labels = train_df['target'].values
    print(f"  Train: {len(train_strings)} rows")
    
    val_df = pd.read_csv(val_csv)
    val_strings = val_df['embedding'].tolist()
    val_labels = val_df['target'].values
    print(f"  Val: {len(val_strings)} rows")
    
    test_df = pd.read_csv(test_csv)
    test_strings = test_df['embedding'].tolist()
    test_labels = test_df['target'].values
    print(f"  Test: {len(test_strings)} rows")
    
    print(f"Data split: Train={len(train_strings)}, Val={len(val_strings)}, Test={len(test_strings)}")
    
    # Create datasets (auto-detect embedding_dim)
    train_dataset = EmbeddingDataset(train_strings, train_labels, None)
    val_dataset = EmbeddingDataset(val_strings, val_labels, train_dataset.embedding_dim)
    test_dataset = EmbeddingDataset(test_strings, test_labels, train_dataset.embedding_dim)
    
    # Use detected embedding dimension
    embedding_dim = train_dataset.embedding_dim
    print(f"Detected embedding dimension: {embedding_dim}")
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # Create model
    model = PredictionHead(
        input_dim=embedding_dim,
        hidden_dims=hidden_dims,
        output_dim=1,
        dropout=dropout
    ).to(device)
    
    print(f"Model architecture: Input={embedding_dim}, Hidden={hidden_dims}, Output=1")
    
    # Setup loss and optimizer
    if task_type == "binclass":
        criterion = nn.BCEWithLogitsLoss()
        metric_fn = roc_auc_score
        higher_is_better = True
    else:  # regression
        criterion = nn.MSELoss()
        metric_fn = mean_squared_error
        higher_is_better = False
    
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    # Training loop
    best_val_metric = -np.inf if higher_is_better else np.inf
    best_model_state = None
    no_improvement = 0
    train_losses = []
    val_metrics = []
    
    print(f"\nStarting training for {num_epochs} epochs...")
    print(f"Task: {task_type}, Device: {device}")
    print("=" * 60)
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        epoch_loss = 0.0
        for batch in train_loader:
            embeddings = batch['embedding'].to(device)
            labels = batch['label'].squeeze().to(device)
            
            optimizer.zero_grad()
            logits = model(embeddings).squeeze()
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)
        
        # Validation
        model.eval()
        val_preds = []
        val_targets = []
        
        with torch.no_grad():
            for batch in val_loader:
                embeddings = batch['embedding'].to(device)
                labels = batch['label'].squeeze().cpu().numpy()
                
                logits = model(embeddings).squeeze().cpu().numpy()
                
                if task_type == "binclass":
                    probs = 1 / (1 + np.exp(-logits))  # sigmoid
                    val_preds.extend(probs)
                else:
                    val_preds.extend(logits)
                
                val_targets.extend(labels)
        
        val_metric = metric_fn(val_targets, val_preds)
        val_metrics.append(val_metric)
        
        # Check for improvement
        is_better = (higher_is_better and val_metric > best_val_metric) or \
                   (not higher_is_better and val_metric < best_val_metric)
        
        if is_better:
            best_val_metric = val_metric
            best_model_state = model.state_dict().copy()
            no_improvement = 0
            print(f"Epoch {epoch+1:3d} | Loss: {avg_loss:.6f} | Val {metric_fn.__name__}: {val_metric:.6f} *")
        else:
            no_improvement += 1
            print(f"Epoch {epoch+1:3d} | Loss: {avg_loss:.6f} | Val {metric_fn.__name__}: {val_metric:.6f}")
        
        # Early stopping
        if no_improvement >= early_stop:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break
    
    # Load best model and evaluate on test set
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    model.eval()
    test_preds = []
    test_targets = []
    
    with torch.no_grad():
        for batch in test_loader:
            embeddings = batch['embedding'].to(device)
            labels = batch['label'].squeeze().cpu().numpy()
            
            logits = model(embeddings).squeeze().cpu().numpy()
            
            if task_type == "binclass":
                probs = 1 / (1 + np.exp(-logits))  # sigmoid
                test_preds.extend(probs)
            else:
                test_preds.extend(logits)
            
            test_targets.extend(labels)
    
    test_metric = metric_fn(test_targets, test_preds)
    
    # Save results
    results = {
        'best_val_metric': float(best_val_metric),
        'test_metric': float(test_metric),
        'task_type': task_type,
        'embedding_dim': embedding_dim,
        'num_epochs_trained': epoch + 1,
        'train_losses': train_losses,
        'val_metrics': val_metrics,
    }
    
    with open(output_dir / "results.json", 'w') as f:
        json.dump(results, f, indent=2)
    
    # Save model
    torch.save({
        'model_state_dict': best_model_state,
        'embedding_dim': embedding_dim,
        'hidden_dims': hidden_dims,
        'task_type': task_type,
    }, output_dir / "prediction_head.pth")
    
    # Save predictions
    np.save(output_dir / "test_predictions.npy", np.array(test_preds))
    np.save(output_dir / "test_targets.npy", np.array(test_targets))
    
    print("\n" + "=" * 60)
    print("TRAINING COMPLETE")
    print("=" * 60)
    print(f"Best Val {metric_fn.__name__}: {best_val_metric:.6f}")
    print(f"Test {metric_fn.__name__}: {test_metric:.6f}")
    print(f"Model saved to: {output_dir / 'prediction_head.pth'}")
    
    return results


def main():
    """Main function to train prediction head from command line."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Train prediction head on TP-BERTa embeddings"
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        required=True,
        help="Directory containing train.csv, val.csv, test.csv (each with columns: embedding, target)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="Output directory for model and results"
    )
    parser.add_argument(
        "--target_col_txt",
        type=str,
        default=None,
        help="Path to target_col.txt (auto-detect if None)"
    )
    parser.add_argument(
        "--hidden_dims",
        type=int,
        nargs="+",
        default=[256, 128],
        help="Hidden layer dimensions (default: 256 128)"
    )
    parser.add_argument(
        "--dropout",
        type=float,
        default=0.2,
        help="Dropout rate (default: 0.2)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=256,
        help="Batch size (default: 256)"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=0.001,
        help="Learning rate (default: 0.001)"
    )
    parser.add_argument(
        "--max_epochs",
        type=int,
        default=200,
        help="Maximum number of epochs (default: 200)"
    )
    parser.add_argument(
        "--early_stop",
        type=int,
        default=10,
        help="Early stopping patience (default: 10)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device to use (cuda/cpu, default: auto-detect)"
    )
    
    args = parser.parse_args()
    
    try:
        results = train_prediction_head(
            data_dir=args.data_dir,
            output_dir=args.output_dir,
            target_col_txt_path=args.target_col_txt,
            hidden_dims=args.hidden_dims,
            dropout=args.dropout,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            num_epochs=args.max_epochs,
            early_stop=args.early_stop,
            device=args.device,
        )
        print(f"\n✅ Training completed successfully!")
        return 0
    except Exception as e:
        print(f"\n❌ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())

