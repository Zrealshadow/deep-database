{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from relbench.datasets import get_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils.data import StackDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/stack/db...\n",
      "Done in 9.84 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = StackDataset(cache_dir=\"/home/lingze/.cache/relbench/stack\")\n",
    "db = dataset.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table tags has 1597 rows\n",
      "Table postHistory has 1175368 rows\n",
      "Table comments has 623967 rows\n",
      "Table badges has 463463 rows\n",
      "Table postTag has 648577 rows\n",
      "Table users has 255360 rows\n",
      "Table postLinks has 77337 rows\n",
      "Table votes has 1317876 rows\n",
      "Table posts has 333893 rows\n"
     ]
    }
   ],
   "source": [
    "for table_name, table in db.table_dict.items():\n",
    "    n = len(table.df)\n",
    "    print(f\"Table {table_name} has {n} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tags', 'badges', 'users', 'posts']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.document import generate_document_given_table\n",
    "from utils.builder import identify_entity_table\n",
    "from utils.builder import generate_hop_matrix\n",
    "entity_tables = identify_entity_table(db)\n",
    "entity_tables.append(\"posts\")\n",
    "entity_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tags', 'badges'),\n",
       " ('tags', 'users'),\n",
       " ('tags', 'posts'),\n",
       " ('badges', 'tags'),\n",
       " ('badges', 'posts'),\n",
       " ('users', 'tags'),\n",
       " ('posts', 'tags'),\n",
       " ('posts', 'badges')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hop_matrix = generate_hop_matrix(db)\n",
    "edge_candidates_pairs = []\n",
    "for entity in entity_tables:\n",
    "    for entity2 in entity_tables:\n",
    "        if entity == entity2:\n",
    "            continue\n",
    "        \n",
    "        if entity2 not in hop_matrix.graph[entity]:\n",
    "            # not one hop\n",
    "            edge_candidates_pairs.append((entity, entity2))\n",
    "edge_candidates_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tags', 'badges'),\n",
       " ('tags', 'users'),\n",
       " ('badges', 'tags'),\n",
       " ('badges', 'posts'),\n",
       " ('users', 'tags'),\n",
       " ('posts', 'badges')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove (tags, posts) and (posts, tags), since they are one hop\n",
    "# we transfer the relationship table to edge\n",
    "edge_candidates_pairs.remove((\"tags\", \"posts\"))\n",
    "edge_candidates_pairs.remove((\"posts\", \"tags\"))\n",
    "edge_candidates_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table postHistory -> table posts has 1175368 edges\n",
      "table postHistory -> table users has 1100031 edges\n",
      "table comments -> table users has 612288 edges\n",
      "table comments -> table posts has 623962 edges\n",
      "table badges -> table users has 463463 edges\n",
      "table postTag -> table posts has 499164 edges\n",
      "table postTag -> table tags has 648577 edges\n",
      "table postLinks -> table posts has 61171 edges\n",
      "table postLinks -> table posts has 75588 edges\n",
      "table votes -> table posts has 1199831 edges\n",
      "table votes -> table users has 5182 edges\n",
      "table posts -> table users has 328648 edges\n",
      "table posts -> table posts has 167355 edges\n",
      "table posts -> table posts has 57714 edges\n"
     ]
    }
   ],
   "source": [
    "# homoGraph\n",
    "from utils.builder import HomoGraph, make_homograph_from_db\n",
    "homoGraph = make_homograph_from_db(db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rule 0]: tags Inferred Id from numerical as categorical\n",
      "[rule 0]: postHistory Inferred Id from numerical as categorical\n",
      "[rule 0]: postHistory Inferred PostId from numerical as categorical\n",
      "[rule 0]: postHistory Inferred UserId from numerical as categorical\n",
      "[rule 0]: postHistory Inferred PostHistoryTypeId from numerical as categorical\n",
      "[rule 0]: postHistory Inferred ContentLicense from categorical as text_embedded\n",
      "[rule 1]: postHistory Inferred ContentLicense from text_embedded as categorical\n",
      "[rule 0]: postHistory Inferred RevisionGUID from text_embedded as categorical\n",
      "[rule 0]: comments Inferred Id from numerical as categorical\n",
      "[rule 0]: comments Inferred PostId from numerical as categorical\n",
      "[rule 0]: comments Inferred UserId from numerical as categorical\n",
      "[rule 1]: comments Inferred Score from numerical as categorical\n",
      "[rule 0]: comments Inferred ContentLicense from categorical as text_embedded\n",
      "[rule 1]: comments Inferred ContentLicense from text_embedded as categorical\n",
      "[rule 0]: badges Inferred Id from numerical as categorical\n",
      "[rule 0]: badges Inferred UserId from numerical as categorical\n",
      "[rule 1]: badges Inferred Name from text_embedded as categorical\n",
      "[rule 0]: postTag Inferred Id from numerical as categorical\n",
      "[rule 0]: postTag Inferred TagId from numerical as categorical\n",
      "[rule 0]: postTag Inferred PostId from numerical as categorical\n",
      "[rule 0]: users Inferred Id from numerical as categorical\n",
      "[rule 0]: users Inferred AccountId from numerical as categorical\n",
      "[rule 0]: users Inferred WebsiteUrl from text_embedded as categorical\n",
      "[rule 0]: postLinks Inferred Id from numerical as categorical\n",
      "[rule 0]: postLinks Inferred RelatedPostId from numerical as categorical\n",
      "[rule 0]: postLinks Inferred PostId from numerical as categorical\n",
      "[rule 0]: votes Inferred Id from numerical as categorical\n",
      "[rule 0]: votes Inferred UserId from numerical as categorical\n",
      "[rule 0]: votes Inferred PostId from numerical as categorical\n",
      "[rule 0]: votes Inferred VoteTypeId from numerical as categorical\n",
      "[rule 0]: posts Inferred Id from numerical as categorical\n",
      "[rule 0]: posts Inferred OwnerUserId from numerical as categorical\n",
      "[rule 0]: posts Inferred AcceptedAnswerId from numerical as categorical\n",
      "[rule 0]: posts Inferred ParentId from numerical as categorical\n",
      "[rule 0]: posts Inferred ContentLicense from categorical as text_embedded\n",
      "[rule 1]: posts Inferred ContentLicense from text_embedded as categorical\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import infer_type_in_db\n",
    "from utils.tokenize import tokenize_database\n",
    "col_type_dict = infer_type_in_db(db, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------> Tokenizing tags each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/tags.npy\n",
      "----------------> Tokenizing postHistory each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/postHistory.npy\n",
      "----------------> Tokenizing comments each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/comments.npy\n",
      "----------------> Tokenizing badges each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/badges.npy\n",
      "----------------> Tokenizing postTag each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/postTag.npy\n",
      "----------------> Tokenizing users each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/users.npy\n",
      "----------------> Tokenizing postLinks each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/postLinks.npy\n",
      "----------------> Tokenizing votes each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/votes.npy\n",
      "----------------> Tokenizing posts each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-stack/posts.npy\n"
     ]
    }
   ],
   "source": [
    "tk_db = tokenize_database(db, col_type_dict, './tmp_docs/rel-stack', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated the documents and build the retrieval index\n",
    "# entity_to_docs = {}\n",
    "# walk_length = 10\n",
    "# round = 10\n",
    "# for entity in entity_tables:\n",
    "#    _, entity_to_docs[entity] = generate_document_given_table(\n",
    "#         homoGraph, \n",
    "#         tk_db, \n",
    "#         entity, \n",
    "#         walk_length=walk_length, \n",
    "#         round = round, \n",
    "#         verbose=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    }
   ],
   "source": [
    "# # temporarily save the index\n",
    "# import bm25s\n",
    "# entity_to_retriver = {}\n",
    "# for entity, docs in entity_to_docs.items():\n",
    "#     retriever = bm25s.BM25(backend=\"numba\")\n",
    "#     retriever.index(docs)\n",
    "#     retriever.activate_numba_scorer()\n",
    "#     entity_to_retriver[entity] = retriever\n",
    "\n",
    "# # save the retriever\n",
    "# for entity, retriever in entity_to_retriver.items():\n",
    "#     retriever.save(f\"./tmp/stack/{entity}_retriever_bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ./tmp/stack/tags_retriever_bm25\n",
      "load ./tmp/stack/badges_retriever_bm25\n",
      "load ./tmp/stack/users_retriever_bm25\n",
      "load ./tmp/stack/posts_retriever_bm25\n"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "entity_to_retriver = {}\n",
    "\n",
    "# save the retriever\n",
    "entity_to_retriver = {}\n",
    "for entity in entity_tables:\n",
    "    path = f\"./tmp/stack/{entity}_retriever_bm25\"\n",
    "    retriever = bm25s.BM25.load(path)\n",
    "    retriever.activate_numba_scorer()\n",
    "    entity_to_retriver[entity] = retriever\n",
    "    print(f\"load {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table tags - shape torch.Size([1597, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table badges - shape torch.Size([92692, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table users - shape torch.Size([51072, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table posts - shape torch.Size([66778, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    }
   ],
   "source": [
    "# resample the candidate docs, and retrieve the related docs in the bm25 retrievers\n",
    "# generated the documents and build the retrieval index\n",
    "walk_length = 10\n",
    "round = 10\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n // 5\n",
    "    sample_size = max(sample_size, 4096)\n",
    "    entity_candidate_pkys[entity], entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add cross table edges #985 between tags and badges\n",
      "Add cross table edges #838 between tags and users\n",
      "Add cross table edges #71696 between badges and tags\n",
      "Add cross table edges #90787 between badges and posts\n",
      "Add cross table edges #33379 between users and tags\n",
      "Add cross table edges #74825 between posts and badges\n"
     ]
    }
   ],
   "source": [
    "# Add the cross-table edges,\n",
    "import numpy as np\n",
    "topn = 20\n",
    "edge_dict = {}\n",
    "# (src_table, des_table) -> edge 2-D array\n",
    "for entity, retrieve_entity in edge_candidates_pairs:\n",
    "\n",
    "    # retrieve the related docs\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    retriever = entity_to_retriver[retrieve_entity]\n",
    "    \n",
    "    related_pkys, scores = retriever.retrieve(entity_query_docs, k = topn, n_threads = 24)\n",
    "    \n",
    "    score_np = np.array(scores)\n",
    "    related_pkys_np = np.array(related_pkys)\n",
    "    threshold = score_np.mean() + 2*scores.std()\n",
    "    \n",
    "    # Get indices where the score is above the threshold\n",
    "    mask = score_np > threshold\n",
    "\n",
    "    # Apply the mask\n",
    "    filtered_cols = related_pkys_np[mask]\n",
    "\n",
    "    # Generate the corresponding query entities\n",
    "    entity_query_pkys = np.array(entity_query_pkys)  # shape [n]\n",
    "\n",
    "    # Repeat each query item the number of True values per row in the mask\n",
    "    row_repeats = mask.sum(axis=1)  # how many times to repeat each query\n",
    "    filtered_rows = np.repeat(entity_query_pkys, row_repeats)\n",
    "    \n",
    "    \n",
    "    filtered_edge = np.stack([filtered_rows, filtered_cols], axis=1)\n",
    "    # added edge\n",
    "    num_edges = filtered_rows.shape[0]\n",
    "    edge_dict[(entity, retrieve_entity)] = filtered_edge\n",
    "    print(f\"Add cross table edges #{num_edges} between {entity} and {retrieve_entity}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table tags - shape torch.Size([1597, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table badges - shape torch.Size([92692, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table users - shape torch.Size([51072, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table posts - shape torch.Size([66778, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    }
   ],
   "source": [
    "# resample the candidate docs, and retrieve the related docs in the bm25 retrievers\n",
    "# generated the documents and build the retrieval index\n",
    "walk_length = 10\n",
    "round = 10\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n // 5\n",
    "    sample_size = max(sample_size, 4096)\n",
    "    entity_candidate_pkys[entity], entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #154, original candidate 1597 in tags table\n",
      "--------> badges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [23:14<00:00, 15.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #79752, original candidate 92692 in badges table\n",
      "--------> users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:59<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #768, original candidate 51072 in users table\n",
      "--------> posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [14:30<00:00, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #44429, original candidate 66778 in posts table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# self-entity correlation\n",
    "# which can generate the positive pairs in the contrastive learning\n",
    "topn = 21\n",
    "# the most related doc should be itself, so we need to retrieve topn + 1\n",
    "positive_pool_dict = {}\n",
    "# entity -> positive candidate, padding the non-value\n",
    "threshold = 0.7\n",
    "batch_size = 1024\n",
    "for entity, retriever in entity_to_retriver.items():\n",
    "    # retrieve the related docs\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    score_np = []\n",
    "    related_pkys_np = []\n",
    "    print(f\"--------> {entity}\")\n",
    "    for batch_idx in tqdm(range(0, len(entity_query_docs), batch_size)):\n",
    "        batch_query_docs = entity_query_docs[batch_idx:batch_idx + batch_size]\n",
    "        related_pkys, scores = retriever.retrieve(batch_query_docs, k = topn, n_threads=-1)\n",
    "        score_np.append(np.array(scores))\n",
    "        related_pkys_np.append(np.array(related_pkys))\n",
    "    \n",
    "    score_np = np.concatenate(score_np, axis = 0)\n",
    "    related_pkys_np = np.concatenate(related_pkys_np, axis = 0)\n",
    "    # Get indices where the score is above the threshold\n",
    "    # the first one is the most related one, should be itself\n",
    "    mask = score_np > (score_np[:,[0]] * threshold)\n",
    "    # add padding for those non-related docs which is filtered out.\n",
    "    related_pkys_np[~mask] = -1\n",
    "    rows_num = np.sum(mask, axis = 1)\n",
    "    # except itself, still has similar docs\n",
    "    rows_mask = rows_num > 1\n",
    "    positive_pool = related_pkys_np[rows_mask]\n",
    "    \n",
    "    positive_pool_dict[entity] = positive_pool\n",
    "    print(f\"Generate positive pools #{len(positive_pool)}, original candidate {len(entity_query_docs)} in {entity} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (src_table, des_table) -> edge 2-D array\n",
    "npz_data = {\n",
    "    f\"{src}-{dst}\": edge_array\n",
    "    for (src, dst), edge_array in edge_dict.items()\n",
    "}\n",
    "\n",
    "path = f\"./edges/rel-stack-edges.npz\"\n",
    "np.savez(path, **npz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('tags', 'badges'), ('tags', 'users'), ('badges', 'tags'), ('badges', 'posts'), ('users', 'tags'), ('posts', 'badges')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./samples/rel-avito-samples.npz\"\n",
    "path = \"./samples/rel-stack-samples.npz\"\n",
    "np.savez(path, **positive_pool_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
