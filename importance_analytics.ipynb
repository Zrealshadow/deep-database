{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa8e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.data import DatabaseFactory\n",
    "from typing import Any, Dict, List, Optional, Type\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d841e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport model.aida\n",
    "%aimport model.layer.fusion_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9296f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-event/db...\n",
      "Done in 2.99 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion/utils/data/event_dataset.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  event_df.replace({\"event_id\": event_id2index}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "db_name = \"event\"\n",
    "db = DatabaseFactory.get_db(db_name,\n",
    "                            with_text_compress=True)\n",
    "dataset = DatabaseFactory.get_dataset(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36fa7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.builder import build_pyg_hetero_graph\n",
    "from utils.util import load_col_types\n",
    "from utils.resource import get_text_embedder_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cbde77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Materialize event_attendees Tensor Frame\n",
      "-----> Build edge between users and users\n",
      "-----> Materialize events Tensor Frame\n",
      "-----> Materialize event_interest Tensor Frame\n",
      "-----> Materialize users Tensor Frame\n",
      "Build pyg hetero graph takes 0.094462 seconds\n"
     ]
    }
   ],
   "source": [
    "cache_dir=\"/home/lingze/embedding_fusion/data/rel-event-tensor-frame\"\n",
    "\n",
    "col_type_dict = load_col_types(\n",
    "    cache_path=cache_dir, file_name=\"col_type_dict.pkl\")\n",
    "\n",
    "data, col_stats_dict = build_pyg_hetero_graph(\n",
    "    db,\n",
    "    col_type_dict,\n",
    "    get_text_embedder_cfg(device=\"cpu\"),\n",
    "    cache_dir=cache_dir,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14b41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"user-repeat\"\n",
    "task = DatabaseFactory.get_task(db_name, task_name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8022119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "from utils.sample import get_node_train_table_input_with_sample\n",
    "num_neighbors = [64, 64]\n",
    "batch_size = 128\n",
    "data_loader_dict: Dict[str, NeighborLoader] = {}\n",
    "for split, sample_ratio, table in [\n",
    "    (\"train\", 1, task.get_table(\"train\")),\n",
    "    (\"valid\", 1, task.get_table(\"val\")),\n",
    "    (\"test\", 1, task.get_table(\"test\", mask_input_cols=False)),\n",
    "]:\n",
    "\n",
    "    _, table_input = get_node_train_table_input_with_sample(\n",
    "        table=table,\n",
    "        task=task,\n",
    "        sample_ratio=sample_ratio,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    data_loader_dict[split] = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        time_attr=\"time\",\n",
    "        input_nodes=table_input.nodes,\n",
    "        input_time=table_input.time,\n",
    "        transform=table_input.transform,\n",
    "        temporal_strategy=\"last\",\n",
    "        batch_size=batch_size,\n",
    "        shuffle=split == \"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7833439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.aida import AIDABaseFeatureEncoder, AIDASharedTableEncoder, AIDAXFormer, AIDATableEncoder\n",
    "from relbench.modeling.nn import HeteroTemporalEncoder\n",
    "from model.graphsage import HeteroGraphSAGE\n",
    "from model.aida import AIDABasicFormer\n",
    "from model.encoder import build_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25e20a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.aida import construct_default_AIDAXFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dfb061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "channels = 128\n",
    "feat_layer_num = 2\n",
    "dropout_prob = 0.1\n",
    "feat_nhead = 1\n",
    "aggr = \"max\"\n",
    "# specific_table_encoder = {\n",
    "#         task.entity_table: build_encoder(\n",
    "#             encoder_type=\"TabM\",\n",
    "#             channels=channels,\n",
    "#             num_layers=2,\n",
    "#             dropout_prob=dropout_prob\n",
    "#         )\n",
    "# }\n",
    "specific_table_encoder = None\n",
    "net = construct_default_AIDAXFormer(\n",
    "    data,\n",
    "    col_stats_dict,\n",
    "    channels=channels,\n",
    "    out_channels=1,\n",
    "    feat_layer_num=feat_layer_num,\n",
    "    dropout_prob=dropout_prob,\n",
    "    feat_nhead=feat_nhead,\n",
    "    relation_aggr=aggr,\n",
    "    deactivate_relation_module=True\n",
    ")\n",
    "net.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19b7c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relbench.base import TaskType\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
    "from torch.nn import L1Loss, BCEWithLogitsLoss\n",
    "\n",
    "is_regression = task.task_type == TaskType.REGRESSION\n",
    "\n",
    "\n",
    "def deactivate_dropout(net: torch.nn.Module):\n",
    "    \"\"\" Deactivate dropout layers in the model. for regression task\n",
    "    \"\"\"\n",
    "    deactive_nn_instances = (\n",
    "        torch.nn.Dropout, torch.nn.Dropout2d, torch.nn.Dropout3d)\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, deactive_nn_instances):\n",
    "            module.eval()\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "    return net\n",
    "\n",
    "\n",
    "net = deactivate_dropout(net) if is_regression else net\n",
    "loss_fn = L1Loss() if is_regression else BCEWithLogitsLoss()\n",
    "evaluate_metric_func = mean_absolute_error if is_regression else roc_auc_score\n",
    "higher_is_better = False if is_regression else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41d77c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(net: torch.nn.Module, loader: torch.utils.data.DataLoader, entity_table: str, early_stop: int = -1, is_regression: bool = False):\n",
    "    pred_list = []\n",
    "    y_list = []\n",
    "    early_stop = early_stop if early_stop > 0 else len(loader.dataset)\n",
    "\n",
    "    if not is_regression:\n",
    "        net.eval()\n",
    "\n",
    "    for idx, batch in tqdm(enumerate(loader), total=len(loader), leave=False, desc=\"Testing\"):\n",
    "        with torch.no_grad():\n",
    "            batch = batch.to(device)\n",
    "            y = batch[entity_table].y.float()\n",
    "            pred = net(batch, entity_table)\n",
    "            pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "            \n",
    "            # apply a sigmoid\n",
    "            if not is_regression:\n",
    "                pred = torch.sigmoid(pred)\n",
    "\n",
    "            pred_list.append(pred.detach().cpu())\n",
    "            y_list.append(y.detach().cpu())\n",
    "        if idx > early_stop:\n",
    "            break\n",
    "\n",
    "    pred_list = pred_logits = torch.cat(pred_list, dim=0)\n",
    "    pred_list = torch.sigmoid(pred_list).numpy()\n",
    "    y_list = torch.cat(y_list, dim=0).numpy()\n",
    "    pred_list = pred_logits.numpy() if is_regression else pred_list\n",
    "    return pred_list,  y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f42d9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training variables\n",
    "import math\n",
    "net.to(device)\n",
    "patience = 0\n",
    "best_epoch = 0\n",
    "best_val_metric = -math.inf if higher_is_better else math.inf\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df8007d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "num_epochs = 500\n",
    "early_stop_threshold = 5\n",
    "max_round_epoch = 50\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, net.parameters()), lr=lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 0 => Train Loss: 0.877890, Val roc_auc_score Metric: 0.670588 \t0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 1 => Train Loss: 0.691973, Val roc_auc_score Metric: 0.657754 \t1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 2 => Train Loss: 0.714578, Val roc_auc_score Metric: 0.677674 \t0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 3 => Train Loss: 0.667862, Val roc_auc_score Metric: 0.747794 \t0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 4 => Train Loss: 0.665311, Val roc_auc_score Metric: 0.787433 \t0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 5 => Train Loss: 0.654173, Val roc_auc_score Metric: 0.748864 \t1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 6 => Train Loss: 0.653727, Val roc_auc_score Metric: 0.774933 \t2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 7 => Train Loss: 0.653966, Val roc_auc_score Metric: 0.767380 \t3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 8 => Train Loss: 0.665608, Val roc_auc_score Metric: 0.814505 \t0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 9 => Train Loss: 0.685539, Val roc_auc_score Metric: 0.779011 \t1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 10 => Train Loss: 0.656779, Val roc_auc_score Metric: 0.793382 \t2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epcoh: 11 => Train Loss: 0.648645, Val roc_auc_score Metric: 0.771858 \t3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 24/31 [00:04<00:01,  5.53it/s]"
     ]
    }
   ],
   "source": [
    "# loss-step\n",
    "import time\n",
    "start_time = time.time()\n",
    "step_loss = []\n",
    "val_metrics_log = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_accum = count_accum = 0\n",
    "    net.train()\n",
    "    for idx, batch in tqdm(enumerate(data_loader_dict[\"train\"]),\n",
    "                           leave=False,\n",
    "                           total=len(data_loader_dict[\"train\"]),\n",
    "                           desc=\"Training\"):\n",
    "        if idx > max_round_epoch:\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        y = batch[task.entity_table].y.float()\n",
    "        pred = net(batch, task.entity_table)\n",
    "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record\n",
    "        step_loss.append(loss.item())\n",
    "        loss_accum += loss.item()\n",
    "        count_accum += 1\n",
    "    train_loss = loss_accum / count_accum\n",
    "    val_logits, val_pred_hat = test(\n",
    "        net, data_loader_dict[\"test\"], task.entity_table, early_stop=-1, is_regression=is_regression\n",
    "    )\n",
    "    val_metric = evaluate_metric_func(val_pred_hat, val_logits)\n",
    "    val_metrics_log.append(val_metric)\n",
    "\n",
    "    # best_val_metric = 0\n",
    "    if (higher_is_better and val_metric > best_val_metric) or \\\n",
    "       (not higher_is_better and val_metric < best_val_metric):\n",
    "        best_val_metric = val_metric\n",
    "        best_epoch = epoch\n",
    "        best_model_state = copy.deepcopy(net.state_dict())\n",
    "        patience = 0\n",
    "\n",
    "        # test_logits,  test_pred_hat = test(\n",
    "        #     net, data_loader_dict[\"test\"], task.entity_table, is_regression=is_regression)\n",
    "        # test_metric = evaluate_metric_func(test_pred_hat, test_logits)\n",
    "\n",
    "        # print(\n",
    "        #     f\"Update the best scores => Test {evaluate_metric_func.__name__} Metric: {test_metric:.6f}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        # test_logits, test_pred_hat = test(\n",
    "        #     net, data_loader_dict[\"test\"], task.entity_table, is_regression=is_regression)\n",
    "        # test_metric = evaluate_metric_func(test_pred_hat, test_logits)\n",
    "        # print(\n",
    "        #     f\"No improvement in epoch {epoch} => Test {evaluate_metric_func.__name__} Metric: {test_metric:.6f}\")\n",
    "        if patience > early_stop_threshold:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    print(\n",
    "        f\"==> Epcoh: {epoch} => Train Loss: {train_loss:.6f}, Val {evaluate_metric_func.__name__} Metric: {val_metric:.6f} \\t{patience}/{early_stop_threshold}\")\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc2a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(best_model_state)\n",
    "loader = data_loader_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c8c4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_list = []\n",
    "attn_weights_list = []\n",
    "pad_mask_list = []\n",
    "for _, batch in enumerate(loader):\n",
    "    batch = batch.to(device)\n",
    "    tokens, seq_types, attn_weights = net.get_attn_weights(batch, task.entity_table)\n",
    "    pad_mask = (attn_weights == 0)\n",
    "    valid_counts = (~pad_mask).sum(dim=1, keepdim=True).clamp_min(1)\n",
    "    \n",
    "\n",
    "    uniform = (1.0 / valid_counts).expand_as(attn_weights)\n",
    "    importance = torch.clamp(attn_weights - uniform, min=0.0)\n",
    "    \n",
    "    # filter out valid counts == 1 rows\n",
    "    valid_counts = valid_counts.squeeze(-1)\n",
    "    attn_weights = attn_weights[valid_counts > 1]\n",
    "    importance = importance[valid_counts > 1]\n",
    "    pad_mask = pad_mask[valid_counts > 1]\n",
    "    \n",
    "    # We applied the filtering to remove rows with valid counts == 1\n",
    "    attn_weights_list.append(attn_weights.cpu())\n",
    "    pad_mask_list.append(pad_mask.cpu())\n",
    "    importance_list.append(importance.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ca342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all importance tensors\n",
    "importances = torch.cat(importance_list, dim=0)\n",
    "importances = importances.numpy()\n",
    "\n",
    "pad_masks = torch.cat(pad_mask_list, dim=0)\n",
    "pad_masks = pad_masks.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the importances Nan corresponding to pad mask\n",
    "importances[pad_masks] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3755954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_self',\n",
       " 'transactions__f2p_customer_id__customer_max',\n",
       " 'transactions__f2p_customer_id__customer_min',\n",
       " 'transactions__f2p_customer_id__customer_sum',\n",
       " 'transactions__f2p_customer_id__customer_mean']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df = pd.DataFrame(importances, columns = seq_types, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23159b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_self                                   5.013261e-01\n",
       "transactions__f2p_customer_id__customer_min     1.508319e-03\n",
       "transactions__f2p_customer_id__customer_mean    1.487556e-05\n",
       "transactions__f2p_customer_id__customer_max     8.915920e-07\n",
       "transactions__f2p_customer_id__customer_sum     0.000000e+00\n",
       "dtype: float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order the columns according to the mean\n",
    "df.mean(axis=0, skipna=True).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfc387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./aida/logs/importances_hm_user_churn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca333c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
