{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingze/embedding_fusion\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from relbench.datasets import get_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /home/lingze/.cache/relbench/rel-avito/db...\n",
      "Done in 5.33 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(name=\"rel-avito\", download = True)\n",
    "db = dataset.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table PhoneRequestsStream -> table UserInfo has 243415 edges\n",
      "table PhoneRequestsStream -> table AdsInfo has 243836 edges\n",
      "table SearchInfo -> table UserInfo has 1961902 edges\n",
      "table SearchInfo -> table Location has 1987154 edges\n",
      "table SearchInfo -> table Category has 1987156 edges\n",
      "table SearchStream -> table SearchInfo has 7107268 edges\n",
      "table SearchStream -> table AdsInfo has 7107274 edges\n",
      "table VisitStream -> table UserInfo has 5254710 edges\n",
      "table VisitStream -> table AdsInfo has 5265422 edges\n",
      "table AdsInfo -> table Location has 5935808 edges\n",
      "table AdsInfo -> table Category has 5960526 edges\n"
     ]
    }
   ],
   "source": [
    "# homoGraph\n",
    "from utils.builder import HomoGraph, make_homograph_from_db\n",
    "homoGraph = make_homograph_from_db(db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingze/anaconda3/envs/deepdb/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import infer_type_in_db\n",
    "from utils.tokenize import tokenize_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rule 0]: PhoneRequestsStream Inferred UserID from numerical as categorical\n",
      "[rule 0]: PhoneRequestsStream Inferred IPID from numerical as categorical\n",
      "[rule 0]: PhoneRequestsStream Inferred AdID from numerical as categorical\n",
      "[rule 0]: Location Inferred LocationID from numerical as categorical\n",
      "[rule 1]: LocationInferred Level from numerical as categorical\n",
      "[rule 0]: Location Inferred RegionID from numerical as categorical\n",
      "[rule 0]: Location Inferred CityID from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred UserID from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred SearchID from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred IPID from numerical as categorical\n",
      "[rule 1]: SearchInfoInferred IsUserLoggedOn from numerical as categorical\n",
      "[rule 0]: SearchInfo Inferred LocationID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserAgentID from numerical as categorical\n",
      "[rule 1]: UserInfoInferred UserAgentOSID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserDeviceID from numerical as categorical\n",
      "[rule 0]: UserInfo Inferred UserAgentFamilyID from numerical as categorical\n",
      "[rule 0]: SearchStream Inferred SearchID from numerical as categorical\n",
      "[rule 0]: SearchStream Inferred AdID from numerical as categorical\n",
      "[rule 1]: SearchStreamInferred Position from numerical as categorical\n",
      "[rule 0]: SearchStream Inferred ObjectType from numerical as categorical\n",
      "[rule 0]: VisitStream Inferred UserID from numerical as categorical\n",
      "[rule 0]: VisitStream Inferred IPID from numerical as categorical\n",
      "[rule 0]: VisitStream Inferred AdID from numerical as categorical\n",
      "[rule 0]: AdsInfo Inferred AdID from numerical as categorical\n",
      "[rule 0]: AdsInfo Inferred LocationID from numerical as categorical\n",
      "[rule 0]: AdsInfo Inferred CategoryID from numerical as categorical\n",
      "[rule 1]: AdsInfoInferred IsContext from numerical as categorical\n",
      "[rule 0]: Category Inferred CategoryID from numerical as categorical\n",
      "[rule 0]: Category Inferred ParentCategoryID from numerical as categorical\n",
      "[rule 0]: Category Inferred SubcategoryID from numerical as categorical\n"
     ]
    }
   ],
   "source": [
    "col_type_dict = infer_type_in_db(db, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------> Tokenizing PhoneRequestsStream each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/PhoneRequestsStream.npy\n",
      "----------------> Tokenizing Location each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/Location.npy\n",
      "----------------> Tokenizing SearchInfo each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/SearchInfo.npy\n",
      "----------------> Tokenizing UserInfo each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/UserInfo.npy\n",
      "----------------> Tokenizing SearchStream each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/SearchStream.npy\n",
      "----------------> Tokenizing VisitStream each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/VisitStream.npy\n",
      "----------------> Tokenizing AdsInfo each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/AdsInfo.npy\n",
      "----------------> Tokenizing Category each column\n",
      "-> Load tokenized data from ./tmp_docs/rel-avito/Category.npy\n"
     ]
    }
   ],
   "source": [
    "tk_db = tokenize_database(db, col_type_dict, './tmp_docs/rel-avito', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.document import generate_document_given_table\n",
    "from utils.builder import identify_entity_table\n",
    "from utils.builder import generate_hop_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Location', 'UserInfo', 'Category']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_tables = identify_entity_table(db)\n",
    "entity_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "# generated the documents and build the retrieval index\n",
    "entity_to_docs = {}\n",
    "walk_length = 10\n",
    "round = 8\n",
    "for entity in entity_tables:\n",
    "   _, entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round, \n",
    "        verbose=True\n",
    "    )\n",
    "   \n",
    "# temporarily save the index\n",
    "import bm25s\n",
    "entity_to_retriver = {}\n",
    "for entity, docs in entity_to_docs.items():\n",
    "    retriever = bm25s.BM25(backend=\"numba\")\n",
    "    retriever.index(docs)\n",
    "    retriever.activate_numba_scorer()\n",
    "    entity_to_retriver[entity] = retriever\n",
    "\n",
    "# save the retriever\n",
    "for entity, retriever in entity_to_retriver.items():\n",
    "    retriever.save(f\"./tmp/rel-avito/{entity}_retriever_bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ./tmp/rel-avito/Location_retriever_bm25\n",
      "load ./tmp/rel-avito/UserInfo_retriever_bm25\n",
      "load ./tmp/rel-avito/Category_retriever_bm25\n"
     ]
    }
   ],
   "source": [
    "# reload this retriever\n",
    "import bm25s\n",
    "\n",
    "entity_to_retriver = {}\n",
    "for entity in entity_tables:\n",
    "    path = f\"./tmp/rel-avito/{entity}_retriever_bm25\"\n",
    "    retriever = bm25s.BM25.load(path)\n",
    "    retriever.activate_numba_scorer()\n",
    "    entity_to_retriver[entity] = retriever\n",
    "    print(f\"load {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Location', 'UserInfo'),\n",
       " ('Location', 'Category'),\n",
       " ('UserInfo', 'Location'),\n",
       " ('UserInfo', 'Category'),\n",
       " ('Category', 'Location'),\n",
       " ('Category', 'UserInfo')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the cross-table edges,\n",
    "# first we want to find the multi-hop entity pairs\n",
    "hop_matrix = generate_hop_matrix(db)\n",
    "edge_candidates_pairs = []\n",
    "for entity in entity_tables:\n",
    "    for entity2 in entity_tables:\n",
    "        if entity == entity2:\n",
    "            continue\n",
    "        \n",
    "        if entity2 not in hop_matrix.graph[entity]:\n",
    "            # not one hop\n",
    "            edge_candidates_pairs.append((entity, entity2))\n",
    "edge_candidates_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table Location - shape torch.Size([3512, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table UserInfo - shape torch.Size([49125, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table Category - shape torch.Size([68, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    }
   ],
   "source": [
    "# generated the documents and build the retrieval index\n",
    "entity_to_docs = {}\n",
    "walk_length = 10\n",
    "round = 8\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n // 2\n",
    "    sample_size = max(sample_size, 4096)\n",
    "    entity_candidate_pkys[entity], entity_to_docs[entity] = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add cross table edges #2607 between Location and UserInfo\n",
      "Add cross table edges #3324 between Location and Category\n",
      "Add cross table edges #22244 between UserInfo and Location\n",
      "Add cross table edges #29904 between UserInfo and Category\n",
      "Add cross table edges #24 between Category and Location\n",
      "Add cross table edges #11 between Category and UserInfo\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "topn = 20\n",
    "edge_dict = {}\n",
    "# (src_table, des_table) -> edge 2-D array\n",
    "for entity, retrieve_entity in edge_candidates_pairs:\n",
    "\n",
    "    # retrieve the related docs\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    retriever = entity_to_retriver[retrieve_entity]\n",
    "    \n",
    "    related_pkys, scores = retriever.retrieve(entity_query_docs, k = topn, n_threads = 24)\n",
    "    \n",
    "    score_np = np.array(scores)\n",
    "    related_pkys_np = np.array(related_pkys)\n",
    "    threshold = score_np.mean() + 2*scores.std()\n",
    "    \n",
    "    # Get indices where the score is above the threshold\n",
    "    mask = score_np > threshold\n",
    "\n",
    "    # Apply the mask\n",
    "    filtered_cols = related_pkys_np[mask]\n",
    "\n",
    "    # Generate the corresponding query entities\n",
    "    entity_query_pkys = np.array(entity_query_pkys)  # shape [n]\n",
    "\n",
    "    # Repeat each query item the number of True values per row in the mask\n",
    "    row_repeats = mask.sum(axis=1)  # how many times to repeat each query\n",
    "    filtered_rows = np.repeat(entity_query_pkys, row_repeats)\n",
    "    \n",
    "    \n",
    "    filtered_edge = np.stack([filtered_rows, filtered_cols], axis=1)\n",
    "    # added edge\n",
    "    num_edges = filtered_rows.shape[0]\n",
    "    edge_dict[(entity, retrieve_entity)] = filtered_edge\n",
    "    print(f\"Add cross table edges #{num_edges} between {entity} and {retrieve_entity}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (src_table, des_table) -> edge 2-D array\n",
    "npz_data = {\n",
    "    f\"{src}-{dst}\": edge_array\n",
    "    for (src, dst), edge_array in edge_dict.items()\n",
    "}\n",
    "\n",
    "path = f\"./edges/rel-avito-Ads-edges.npz\"\n",
    "np.savez(path, **npz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('Location', 'UserInfo'), ('Location', 'Category'), ('UserInfo', 'Location'), ('UserInfo', 'Category'), ('Category', 'Location'), ('Category', 'UserInfo')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table Location - shape torch.Size([3512, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table UserInfo - shape torch.Size([49125, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Walks for table Category - shape torch.Size([68, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    }
   ],
   "source": [
    "# self-entity correlation\n",
    "# which can generate the positive pairs in the contrastive learning\n",
    "# generated the documents and build the retrieval index\n",
    "entity_to_docs = {}\n",
    "walk_length = 10\n",
    "round = 8\n",
    "entity_to_docs = {}\n",
    "entity_candidate_pkys = {}\n",
    "# for each\n",
    "\n",
    "for entity in entity_tables:\n",
    "    n = len(db.table_dict[entity].df)\n",
    "    sample_size = n // 2\n",
    "    sample_size = max(sample_size, 4096)\n",
    "    pkys , docs = generate_document_given_table(\n",
    "        homoGraph, \n",
    "        tk_db, \n",
    "        entity, \n",
    "        walk_length=walk_length, \n",
    "        round = round,\n",
    "        sample_size = sample_size,\n",
    "        verbose=True\n",
    "    )\n",
    "    entity_candidate_pkys[entity] = pkys\n",
    "    entity_to_docs[entity] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> Location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #162, original candidate 3512 in Location table\n",
      "--------> UserInfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:25<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #17615, original candidate 49125 in UserInfo table\n",
      "--------> Category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate positive pools #6, original candidate 68 in Category table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topn = 21\n",
    "# the most related doc should be itself, so we need to retrieve topn + 1\n",
    "positive_pool_dict = {}\n",
    "# entity -> positive candidate, padding the non-value\n",
    "threshold = 0.7\n",
    "batch_size = 1024\n",
    "for entity, retriever in entity_to_retriver.items():\n",
    "    # retrieve the related docs\n",
    "    entity_query_docs = entity_to_docs[entity]\n",
    "    entity_query_pkys = entity_candidate_pkys[entity]\n",
    "    score_np = []\n",
    "    related_pkys_np = []\n",
    "    print(f\"--------> {entity}\")\n",
    "    for batch_idx in tqdm(range(0, len(entity_query_docs), batch_size)):\n",
    "        batch_query_docs = entity_query_docs[batch_idx:batch_idx + batch_size]\n",
    "        related_pkys, scores = retriever.retrieve(batch_query_docs, k = topn, n_threads=-1)\n",
    "        score_np.append(np.array(scores))\n",
    "        related_pkys_np.append(np.array(related_pkys))\n",
    "    \n",
    "    score_np = np.concatenate(score_np, axis = 0)\n",
    "    related_pkys_np = np.concatenate(related_pkys_np, axis = 0)\n",
    "    # Get indices where the score is above the threshold\n",
    "    # the first one is the most related one, should be itself\n",
    "    mask = score_np > (score_np[:,[0]] * threshold)\n",
    "    # add padding for those non-related docs which is filtered out.\n",
    "    related_pkys_np[~mask] = -1\n",
    "    rows_num = np.sum(mask, axis = 1)\n",
    "    # except itself, still has similar docs\n",
    "    rows_mask = rows_num > 1\n",
    "    positive_pool = related_pkys_np[rows_mask]\n",
    "    \n",
    "    positive_pool_dict[entity] = positive_pool\n",
    "    print(f\"Generate positive pools #{len(positive_pool)}, original candidate {len(entity_query_docs)} in {entity} table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./samples/rel-avito-samples.npz\"\n",
    "path = \"./samples/rel-avito-Ads-samples.npz\"\n",
    "np.savez(path, **positive_pool_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
